//! Work scheduler and orchestration primitives for rs-stellar-core.
//!
//! This crate provides a dependency-aware async work scheduler modeled after
//! the work scheduling system in stellar-core. It enables concurrent
//! execution of tasks with explicit dependencies, automatic retry support,
//! and cancellation propagation.
//!
//! # Overview
//!
//! The scheduler manages work items that implement the [`Work`] trait. Each
//! work item can declare dependencies on other work items, and the scheduler
//! ensures prerequisites complete successfully before running dependent work.
//!
//! The design follows a directed acyclic graph (DAG) execution model where:
//! - Work items are nodes in the graph
//! - Dependencies form edges between nodes
//! - Execution proceeds in topological order
//! - Failed nodes block all downstream dependents
//!
//! # Key Components
//!
//! - [`Work`]: The trait that all schedulable work items must implement.
//! - [`WorkScheduler`]: The core scheduler that manages work execution.
//! - [`WorkSequence`]: A helper for building linear chains of dependent work.
//! - [`WorkWithCallback`]: A wrapper that invokes a callback after work completes.
//!
//! # Example
//!
//! ```ignore
//! use henyey_work::{Work, WorkContext, WorkOutcome, WorkScheduler, WorkSchedulerConfig};
//!
//! struct MyWork { name: String }
//!
//! #[async_trait::async_trait]
//! impl Work for MyWork {
//!     fn name(&self) -> &str { &self.name }
//!     async fn run(&mut self, ctx: WorkContext) -> WorkOutcome {
//!         // Perform work, checking for cancellation as needed
//!         if ctx.is_cancelled() {
//!             return WorkOutcome::Cancelled;
//!         }
//!         WorkOutcome::Success
//!     }
//! }
//!
//! let mut scheduler = WorkScheduler::new(WorkSchedulerConfig::default());
//! let id = scheduler.add_work(Box::new(MyWork { name: "task".into() }), vec![], 3);
//! scheduler.run_until_done().await;
//! ```
//!
//! # Work Lifecycle
//!
//! Work items progress through a well-defined state machine:
//!
//! ```text
//!                      +----------+
//!                      | Pending  |
//!                      +----+-----+
//!                           |
//!              deps satisfied & slot available
//!                           |
//!                           v
//!                      +----------+
//!                      | Running  |
//!                      +----+-----+
//!                           |
//!        +--------+---------+---------+---------+
//!        |        |         |         |         |
//!        v        v         v         v         v
//!   +--------+ +------+ +-------+ +--------+ +-------+
//!   | Success| | Retry| | Failed| |Cancelled| |Blocked|
//!   +--------+ +------+ +-------+ +--------+ +-------+
//!                 |
//!          (if retries remain)
//!                 |
//!                 v
//!            +----------+
//!            | Pending  |
//!            +----------+
//! ```
//!
//! 1. Work items start in [`WorkState::Pending`].
//! 2. When all dependencies succeed, the scheduler moves work to [`WorkState::Running`].
//! 3. Work execution returns a [`WorkOutcome`] indicating success, failure, retry, or cancellation.
//! 4. On success, dependent work items become runnable.
//! 5. On failure or cancellation, dependent work items are blocked.
//!
//! # Cancellation
//!
//! The scheduler supports cooperative cancellation. Work items should periodically
//! check [`WorkContext::is_cancelled()`] and return [`WorkOutcome::Cancelled`] if
//! cancellation is requested. The scheduler propagates cancellation to all
//! registered work items when [`WorkScheduler::cancel_all()`] is called.
//!
//! # Thread Safety
//!
//! The scheduler itself is not thread-safe and should be driven from a single
//! async task. However, work items execute on Tokio's thread pool and must be
//! `Send`. Shared state between work items should use appropriate synchronization
//! primitives (e.g., `Arc<Mutex<T>>`, channels).

use std::collections::{HashMap, HashSet, VecDeque};
use std::sync::Arc;
use std::time::{Duration, Instant};

use async_trait::async_trait;
use tokio::sync::mpsc;
use tokio_util::sync::CancellationToken;
use tracing::{debug, info, warn};

/// Capacity of the internal channel used for work completion notifications.
const COMPLETION_CHANNEL_CAPACITY: usize = 128;

/// Unique identifier for a work item within a scheduler.
///
/// Work IDs are assigned sequentially starting from 1 when work items are
/// added to the scheduler. They can be used to track dependencies, query
/// state, and cancel specific work items.
pub type WorkId = u64;

/// Result of a single work execution attempt.
///
/// Work items return this type from their [`Work::run`] method to indicate
/// the outcome of execution. The scheduler uses this to determine whether
/// to mark the work as complete, retry it, or handle failure.
///
/// # Retry Behavior
///
/// When returning [`WorkOutcome::Retry`], the scheduler will wait for the
/// specified delay before re-attempting the work, provided retries remain.
/// If no retries remain, the work transitions to [`WorkState::Failed`].
#[derive(Debug, Clone)]
pub enum WorkOutcome {
    /// Work completed successfully.
    ///
    /// Dependent work items will become runnable after this outcome.
    Success,

    /// Work was cancelled by the caller.
    ///
    /// Work items should return this when they detect cancellation via
    /// [`WorkContext::is_cancelled()`]. Dependent work items will be blocked.
    Cancelled,

    /// Work should be retried after the specified delay.
    ///
    /// If `delay` is zero, the scheduler's configured `retry_delay` is used.
    /// Retries are only attempted if the work item has remaining retry budget.
    Retry {
        /// Time to wait before the next attempt.
        delay: Duration,
    },

    /// Work failed with an error message.
    ///
    /// This is a terminal failure - the work will not be retried regardless
    /// of remaining retry budget. Dependent work items will be blocked.
    Failed(String),
}

/// Current state of a work item in the scheduler.
///
/// Work items transition through states as they are scheduled and executed.
/// The scheduler maintains state for each registered work item.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum WorkState {
    /// Work is waiting to be scheduled.
    ///
    /// The work item is registered but either has unfinished dependencies
    /// or is waiting for a concurrency slot.
    Pending,

    /// Work is currently executing.
    Running,

    /// Work completed successfully.
    Success,

    /// Work failed permanently (either via [`WorkOutcome::Failed`] or
    /// exhausted retries).
    Failed,

    /// Work cannot run because a dependency failed, was cancelled, or was blocked.
    ///
    /// This is a terminal state - blocked work will not be executed.
    Blocked,

    /// Work was explicitly cancelled.
    Cancelled,
}

impl WorkState {
    /// Returns `true` if this is a terminal state.
    ///
    /// Terminal states are those where no further progress will be made:
    /// [`Success`](Self::Success), [`Failed`](Self::Failed),
    /// [`Blocked`](Self::Blocked), and [`Cancelled`](Self::Cancelled).
    ///
    /// Non-terminal states are [`Pending`](Self::Pending) and
    /// [`Running`](Self::Running).
    #[must_use]
    pub fn is_terminal(self) -> bool {
        matches!(
            self,
            Self::Success | Self::Failed | Self::Blocked | Self::Cancelled
        )
    }

    /// Returns `true` if this is a successful terminal state.
    #[must_use]
    pub fn is_success(self) -> bool {
        matches!(self, Self::Success)
    }

    /// Returns `true` if this is a failure state.
    ///
    /// This includes [`Failed`](Self::Failed), [`Blocked`](Self::Blocked),
    /// and [`Cancelled`](Self::Cancelled).
    #[must_use]
    pub fn is_failure(self) -> bool {
        matches!(self, Self::Failed | Self::Blocked | Self::Cancelled)
    }
}

/// Execution context provided to a work item during execution.
///
/// The context provides the work item with its identity, the current attempt
/// number, and a mechanism to check for cancellation requests.
#[derive(Debug, Clone)]
pub struct WorkContext {
    /// The unique identifier of this work item.
    pub id: WorkId,

    /// The current attempt number (1-indexed).
    ///
    /// This is 1 for the first attempt, 2 for the first retry, etc.
    pub attempt: u32,

    /// Cancellation token for cooperative cancellation.
    cancel_token: CancellationToken,
}

impl WorkContext {
    /// Returns `true` if cancellation has been requested.
    ///
    /// Work items should check this periodically during long-running operations
    /// and return [`WorkOutcome::Cancelled`] if cancellation is detected.
    pub fn is_cancelled(&self) -> bool {
        self.cancel_token.is_cancelled()
    }

    /// Returns a reference to the cancellation token.
    ///
    /// This can be used for more advanced cancellation patterns, such as
    /// passing the token to async operations that support it directly.
    pub fn cancel_token(&self) -> &CancellationToken {
        &self.cancel_token
    }
}

/// Event emitted by the scheduler when work state changes.
///
/// Events can be received via the [`WorkSchedulerConfig::event_tx`] channel.
/// This enables external monitoring, logging, or progress tracking.
#[derive(Debug, Clone)]
pub struct WorkEvent {
    /// The work item this event pertains to.
    pub id: WorkId,

    /// Human-readable name of the work item.
    pub name: String,

    /// The new state of the work item.
    pub state: WorkState,

    /// The attempt number when this event occurred.
    pub attempt: u32,
}

/// A unit of schedulable, async work.
///
/// Implement this trait for types that represent work to be executed by
/// the scheduler. Work items are stateful and can maintain state across
/// retry attempts.
///
/// # Implementation Notes
///
/// - The `name` method should return a stable, human-readable identifier
///   for logging and debugging purposes.
/// - The `run` method receives a [`WorkContext`] and should check for
///   cancellation periodically during long operations.
/// - Work items are executed with `&mut self`, allowing them to update
///   internal state between retries.
#[async_trait]
pub trait Work: Send {
    /// Returns the name of this work item for logging and identification.
    fn name(&self) -> &str;

    /// Executes the work and returns an outcome.
    ///
    /// This method is called each time the work item is executed, including
    /// retries. The provided context contains the attempt number and a
    /// cancellation token.
    async fn run(&mut self, ctx: WorkContext) -> WorkOutcome;
}

// ============================================================================
// Internal Types
// ============================================================================

/// Internal representation of a registered work item.
///
/// This struct holds all the state needed to manage a work item's lifecycle,
/// including its dependencies, retry budget, timing information, and the
/// work implementation itself.
struct WorkEntry {
    /// Human-readable name for logging and identification.
    name: String,

    /// Work items that must complete successfully before this one can run.
    deps: Vec<WorkId>,

    /// Number of retry attempts remaining.
    ///
    /// Decremented each time a retry is consumed. When zero, further
    /// retry requests result in failure.
    retries_left: u32,

    /// Total number of execution attempts made (1-indexed).
    ///
    /// Incremented before each execution attempt, including retries.
    attempts: u32,

    /// Error message from the most recent failure, if any.
    last_error: Option<String>,

    /// Timestamp when the current or most recent execution started.
    ///
    /// Used to calculate execution duration.
    started_at: Option<Instant>,

    /// Duration of the most recent execution attempt.
    last_duration: Option<Duration>,

    /// Cumulative execution time across all attempts.
    total_duration: Duration,

    /// Token for cooperative cancellation.
    ///
    /// Shared with the [`WorkContext`] provided to the work item during execution.
    cancel_token: CancellationToken,

    /// The actual work implementation.
    work: Box<dyn Work + Send>,
}

// ============================================================================
// Callback Wrapper
// ============================================================================

/// A work wrapper that invokes a callback after work finishes.
///
/// This is useful for integrating work completion notifications into
/// higher-level orchestration logic, such as catchup or publish workflows.
/// The callback receives both the outcome and execution context, allowing
/// for rich logging, metrics collection, or triggering downstream actions.
///
/// # Example
///
/// ```ignore
/// use henyey_work::{Work, WorkWithCallback, WorkOutcome, WorkContext};
/// use std::sync::Arc;
///
/// let callback = Arc::new(|outcome: WorkOutcome, ctx: WorkContext| {
///     println!("Work {} finished with {:?}", ctx.id, outcome);
/// });
///
/// let wrapped = WorkWithCallback::new(my_work, callback);
/// scheduler.add_work(Box::new(wrapped), vec![], 0);
/// ```
pub struct WorkWithCallback {
    /// The underlying work item being wrapped.
    work: Box<dyn Work + Send>,

    /// Callback invoked after each execution attempt with the outcome and context.
    callback: Arc<dyn Fn(WorkOutcome, WorkContext) + Send + Sync>,
}

impl WorkWithCallback {
    /// Creates a new callback-wrapped work item.
    ///
    /// # Arguments
    ///
    /// * `work` - The underlying work item to execute.
    /// * `callback` - A function called after the work completes, receiving
    ///   the outcome and execution context.
    pub fn new(
        work: Box<dyn Work + Send>,
        callback: Arc<dyn Fn(WorkOutcome, WorkContext) + Send + Sync>,
    ) -> Self {
        Self { work, callback }
    }
}

#[async_trait]
impl Work for WorkWithCallback {
    fn name(&self) -> &str {
        self.work.name()
    }

    async fn run(&mut self, ctx: WorkContext) -> WorkOutcome {
        let outcome = self.work.run(ctx.clone()).await;
        (self.callback)(outcome.clone(), ctx);
        outcome
    }
}

// ============================================================================
// Scheduler Configuration
// ============================================================================

/// Configuration for the work scheduler.
///
/// This struct controls the scheduler's behavior including concurrency limits,
/// retry timing, and event monitoring. Use [`Default::default()`] for sensible
/// defaults or customize as needed.
///
/// # Example
///
/// ```ignore
/// use henyey_work::{WorkScheduler, WorkSchedulerConfig};
/// use std::time::Duration;
///
/// let config = WorkSchedulerConfig {
///     max_concurrency: 8,
///     retry_delay: Duration::from_secs(5),
///     event_tx: None,
/// };
/// let scheduler = WorkScheduler::new(config);
/// ```
#[derive(Debug, Clone)]
pub struct WorkSchedulerConfig {
    /// Maximum number of work items that can execute concurrently.
    ///
    /// Controls the parallelism of work execution. Higher values allow more
    /// work items to run simultaneously, which can improve throughput but
    /// also increases resource usage.
    ///
    /// Defaults to 4. Set to 1 for strictly sequential execution.
    pub max_concurrency: usize,

    /// Default delay between retry attempts.
    ///
    /// Used when a work item returns [`WorkOutcome::Retry`] with a zero duration.
    /// Work items can override this by specifying a non-zero delay in their
    /// retry outcome.
    ///
    /// Defaults to 1 second.
    pub retry_delay: Duration,

    /// Optional channel for receiving work state change events.
    ///
    /// If provided, the scheduler will send [`WorkEvent`] messages as work
    /// items transition between states. This enables external monitoring,
    /// progress tracking, and debugging without modifying work items.
    ///
    /// The channel should have sufficient capacity to avoid blocking the
    /// scheduler; events are sent with `try_send` and dropped if the
    /// channel is full.
    pub event_tx: Option<mpsc::Sender<WorkEvent>>,
}

impl Default for WorkSchedulerConfig {
    fn default() -> Self {
        Self {
            max_concurrency: 4,
            retry_delay: Duration::from_secs(1),
            event_tx: None,
        }
    }
}

// ============================================================================
// Work Scheduler
// ============================================================================

/// A dependency-aware async work scheduler.
///
/// The scheduler manages a collection of work items, tracks their dependencies,
/// and executes them concurrently (up to a configured limit). It handles:
///
/// - **Dependency ordering**: Work items only run after all their dependencies succeed.
/// - **Concurrency control**: Limits the number of simultaneously running work items.
/// - **Automatic retries**: Retries work items that request it, with configurable delays.
/// - **Cancellation propagation**: Cancels all work items when requested.
/// - **Failure isolation**: Blocks dependent work when a dependency fails.
///
/// # Design
///
/// The scheduler is modeled after the work scheduling system in stellar-core.
/// It uses a simple ready-queue approach: work items whose dependencies have all
/// succeeded are eligible for execution. The scheduler spawns Tokio tasks for each
/// work item and collects completion notifications via an internal channel.
///
/// The execution loop follows these steps:
/// 1. Fill available concurrency slots with ready work items
/// 2. Wait for any running work to complete
/// 3. Process the completion and update state accordingly
/// 4. Repeat until no work can make progress
///
/// # Ownership
///
/// Work items are moved into the scheduler and remain there until the scheduler
/// is dropped. The scheduler temporarily moves work items out during execution
/// (replacing them with a placeholder `EmptyWork`) and moves them back after
/// completion. This design satisfies Rust's ownership rules while allowing
/// stateful work items to be retried.
///
/// # Example
///
/// ```ignore
/// use henyey_work::{WorkScheduler, WorkSchedulerConfig, Work, WorkOutcome, WorkContext};
///
/// // Create scheduler with custom configuration
/// let mut scheduler = WorkScheduler::new(WorkSchedulerConfig {
///     max_concurrency: 4,
///     retry_delay: Duration::from_secs(1),
///     event_tx: None,
/// });
///
/// // Add work items with dependencies
/// let id1 = scheduler.add_work(Box::new(work1), vec![], 3);
/// let id2 = scheduler.add_work(Box::new(work2), vec![id1], 0); // depends on id1
///
/// // Run until all work completes or is blocked
/// scheduler.run_until_done().await;
///
/// // Check results
/// assert_eq!(scheduler.state(id1), Some(WorkState::Success));
/// ```
pub struct WorkScheduler {
    /// Scheduler configuration (concurrency, retry delay, events).
    config: WorkSchedulerConfig,

    /// Next work ID to assign (monotonically increasing).
    next_id: WorkId,

    /// All registered work entries, keyed by their ID.
    entries: HashMap<WorkId, WorkEntry>,

    /// Current state of each work item.
    states: HashMap<WorkId, WorkState>,

    /// Reverse dependency map: work ID -> IDs of work items that depend on it.
    ///
    /// Used to efficiently find and block dependents when a work item fails.
    dependents: HashMap<WorkId, Vec<WorkId>>,
}

// ============================================================================
// Metrics and Snapshots
// ============================================================================

/// Aggregate metrics for the work scheduler.
///
/// Provides a point-in-time summary of the scheduler's state, useful for
/// monitoring progress, building dashboards, or debugging scheduling issues.
/// All counts are mutually exclusive - a work item is counted in exactly one
/// state category.
///
/// # Example
///
/// ```ignore
/// let metrics = scheduler.metrics();
/// println!(
///     "Progress: {}/{} complete, {} failed, {} running",
///     metrics.success, metrics.total, metrics.failed, metrics.running
/// );
/// ```
#[derive(Debug, Clone, Default)]
pub struct WorkSchedulerMetrics {
    /// Total number of registered work items.
    pub total: usize,

    /// Number of work items waiting to run (dependencies not satisfied or no slot).
    pub pending: usize,

    /// Number of work items currently executing.
    pub running: usize,

    /// Number of work items that completed successfully.
    pub success: usize,

    /// Number of work items that failed (terminal failure or exhausted retries).
    pub failed: usize,

    /// Number of work items blocked due to failed or cancelled dependencies.
    pub blocked: usize,

    /// Number of work items that were explicitly cancelled.
    pub cancelled: usize,

    /// Total number of execution attempts across all work items.
    ///
    /// This counts each retry as a separate attempt, so it may exceed `total`.
    pub attempts: u64,

    /// Total remaining retry budget across all work items.
    pub retries_left: u64,
}

/// A point-in-time snapshot of a single work item's state.
///
/// Used for debugging, monitoring, and introspection of the scheduler's
/// internal state. Snapshots are independent copies that can be examined
/// without affecting scheduler operation.
///
/// The `deps` and `dependents` fields show the dependency graph structure,
/// which is useful for understanding why work items are blocked or not yet
/// running.
#[derive(Debug, Clone)]
pub struct WorkSnapshot {
    /// Unique identifier of this work item.
    pub id: WorkId,

    /// Human-readable name of the work item.
    pub name: String,

    /// Current state of the work item.
    pub state: WorkState,

    /// IDs of work items this one depends on (prerequisites).
    pub deps: Vec<WorkId>,

    /// IDs of work items that depend on this one (downstream).
    pub dependents: Vec<WorkId>,

    /// Number of execution attempts made so far (includes retries).
    pub attempts: u32,

    /// Remaining retry budget.
    pub retries_left: u32,

    /// Error message from the most recent failure, if any.
    pub last_error: Option<String>,

    /// Duration of the most recent execution attempt.
    pub last_duration: Option<Duration>,

    /// Cumulative execution time across all attempts.
    pub total_duration: Duration,
}

impl WorkScheduler {
    // ========================================================================
    // Construction and Registration
    // ========================================================================

    /// Creates a new work scheduler with the given configuration.
    ///
    /// The scheduler starts empty with no registered work items. Use
    /// [`add_work`](Self::add_work) to register work items before calling
    /// [`run_until_done`](Self::run_until_done).
    pub fn new(config: WorkSchedulerConfig) -> Self {
        Self {
            config,
            next_id: 1,
            entries: HashMap::new(),
            states: HashMap::new(),
            dependents: HashMap::new(),
        }
    }

    /// Registers a work item with the scheduler.
    ///
    /// # Arguments
    ///
    /// * `work` - The work item to schedule.
    /// * `deps` - IDs of work items that must complete successfully before this one runs.
    /// * `retries` - Number of retry attempts allowed if the work returns [`WorkOutcome::Retry`].
    ///
    /// # Returns
    ///
    /// A unique [`WorkId`] that can be used to reference this work item.
    ///
    /// # Panics
    ///
    /// Does not panic. Invalid dependency IDs will cause the work item to be
    /// blocked when its dependencies are checked.
    #[must_use]
    pub fn add_work(
        &mut self,
        work: Box<dyn Work + Send>,
        deps: Vec<WorkId>,
        retries: u32,
    ) -> WorkId {
        let id = self.next_id;
        self.next_id += 1;

        let name = work.name().to_string();
        debug!(work_id = id, name = %name, "registered work item");

        for &dep in &deps {
            self.dependents.entry(dep).or_default().push(id);
        }

        let entry = WorkEntry {
            name,
            deps,
            retries_left: retries,
            attempts: 0,
            last_error: None,
            started_at: None,
            last_duration: None,
            total_duration: Duration::ZERO,
            cancel_token: CancellationToken::new(),
            work,
        };

        self.entries.insert(id, entry);
        self.states.insert(id, WorkState::Pending);

        id
    }

    // ========================================================================
    // State Queries
    // ========================================================================

    /// Returns the current state of a work item, if it exists.
    ///
    /// Returns `None` if no work item with the given ID has been registered.
    #[must_use]
    pub fn state(&self, id: WorkId) -> Option<WorkState> {
        self.states.get(&id).copied()
    }

    // ========================================================================
    // Cancellation
    // ========================================================================

    /// Cancels a specific work item.
    ///
    /// If the work item is pending or running, it will be marked as cancelled
    /// and its dependents will be blocked. Running work items will receive
    /// a cancellation signal via their [`WorkContext`].
    ///
    /// # Returns
    ///
    /// `true` if the work item was successfully cancelled, `false` if it was
    /// already in a terminal state or does not exist.
    pub fn cancel(&mut self, id: WorkId) -> bool {
        let Some(state) = self.states.get(&id).copied() else {
            return false;
        };
        if state.is_terminal() {
            return false;
        }

        if let Some(entry) = self.entries.get_mut(&id) {
            entry.cancel_token.cancel();
            let attempts = entry.attempts;
            self.fail_or_cancel(id, WorkState::Cancelled, attempts);
            return true;
        }
        false
    }

    /// Cancels all registered work items.
    ///
    /// Each work item that is pending or running will be cancelled, and their
    /// dependents will be blocked.
    pub fn cancel_all(&mut self) {
        let ids: Vec<WorkId> = self.entries.keys().copied().collect();
        for id in ids {
            let _ = self.cancel(id);
        }
    }

    // ========================================================================
    // Introspection
    // ========================================================================

    /// Returns a snapshot of all work items in the scheduler.
    ///
    /// The snapshots are sorted by work ID. This is useful for debugging,
    /// logging, and monitoring the scheduler's internal state. The returned
    /// data is a copy and does not affect scheduler operation.
    pub fn snapshot(&self) -> Vec<WorkSnapshot> {
        let mut snapshots: Vec<WorkSnapshot> = self
            .entries
            .iter()
            .map(|(id, entry)| WorkSnapshot {
                id: *id,
                name: entry.name.clone(),
                state: self.states.get(id).copied().unwrap_or(WorkState::Pending),
                deps: entry.deps.clone(),
                dependents: self.dependents.get(id).cloned().unwrap_or_default(),
                attempts: entry.attempts,
                retries_left: entry.retries_left,
                last_error: entry.last_error.clone(),
                last_duration: entry.last_duration,
                total_duration: entry.total_duration,
            })
            .collect();
        snapshots.sort_by_key(|snapshot| snapshot.id);
        snapshots
    }

    /// Returns aggregate metrics for all work items in the scheduler.
    ///
    /// This is a lightweight operation that scans all entries once.
    /// For detailed per-item information, use [`snapshot`](Self::snapshot).
    pub fn metrics(&self) -> WorkSchedulerMetrics {
        let mut metrics = WorkSchedulerMetrics {
            total: self.entries.len(),
            ..Default::default()
        };
        for (id, entry) in &self.entries {
            match self.states.get(id).copied().unwrap_or(WorkState::Pending) {
                WorkState::Pending => metrics.pending += 1,
                WorkState::Running => metrics.running += 1,
                WorkState::Success => metrics.success += 1,
                WorkState::Failed => metrics.failed += 1,
                WorkState::Blocked => metrics.blocked += 1,
                WorkState::Cancelled => metrics.cancelled += 1,
            }
            metrics.attempts += u64::from(entry.attempts);
            metrics.retries_left += u64::from(entry.retries_left);
        }
        metrics
    }

    // ========================================================================
    // Execution
    // ========================================================================

    /// Runs the scheduler until all work items complete or are blocked.
    ///
    /// This is a convenience method that creates a non-triggered cancellation
    /// token. Use [`run_until_done_with_cancel`](Self::run_until_done_with_cancel)
    /// if you need external cancellation control.
    ///
    /// The scheduler will return when:
    /// - All work items have reached a terminal state (Success, Failed, Blocked, Cancelled)
    /// - No pending work can make progress (all blocked on failed dependencies)
    pub async fn run_until_done(&mut self) {
        let cancel = CancellationToken::new();
        self.run_until_done_with_cancel(cancel).await;
    }

    /// Runs the scheduler until all work items complete, are blocked, or cancellation is requested.
    ///
    /// The scheduler will:
    /// 1. Start work items whose dependencies have all succeeded.
    /// 2. Execute up to `max_concurrency` work items in parallel.
    /// 3. Handle retries with the configured delay.
    /// 4. Block dependents of failed or cancelled work items.
    /// 5. Stop when no more work can make progress or cancellation is triggered.
    ///
    /// # Cancellation
    ///
    /// When the provided `cancel` token is triggered, the scheduler will:
    /// - Cancel all pending and running work items.
    /// - Wait for running work items to complete (they should check for cancellation).
    /// - Return once all work has stopped.
    pub async fn run_until_done_with_cancel(&mut self, cancel: CancellationToken) {
        let (tx, mut rx) = mpsc::channel::<WorkCompletion>(COMPLETION_CHANNEL_CAPACITY);
        let mut running: HashSet<WorkId> = HashSet::new();
        let mut queue = self.ready_queue();
        let mut queued: HashSet<WorkId> = queue.iter().copied().collect();

        let mut cancel_requested = false;

        loop {
            if !cancel_requested && cancel.is_cancelled() {
                cancel_requested = true;
                self.cancel_all();
            }

            while running.len() < self.config.max_concurrency {
                let Some(id) = queue.pop_front() else { break };
                queued.remove(&id);
                if running.contains(&id) {
                    continue;
                }

                if !self.can_run(id) {
                    continue;
                }

                let Some(entry) = self.entries.get_mut(&id) else {
                    continue;
                };
                if entry.cancel_token.is_cancelled() {
                    let attempts = entry.attempts;
                    self.fail_or_cancel(id, WorkState::Cancelled, attempts);
                    continue;
                }
                entry.attempts += 1;
                let attempt = entry.attempts;
                let mut work = std::mem::replace(&mut entry.work, Box::new(EmptyWork));
                let name = entry.name.clone();
                let completion_tx = tx.clone();
                let cancel_token = entry.cancel_token.clone();
                entry.started_at = Some(Instant::now());

                self.states.insert(id, WorkState::Running);
                self.emit_event(id, WorkState::Running, attempt);
                running.insert(id);

                tokio::spawn(async move {
                    let outcome = work
                        .run(WorkContext {
                            id,
                            attempt,
                            cancel_token: cancel_token.clone(),
                        })
                        .await;
                    let _ = completion_tx
                        .send(WorkCompletion {
                            id,
                            outcome,
                            work: Some(work),
                            attempt,
                            cancelled: cancel_token.is_cancelled(),
                        })
                        .await;
                    debug!(work_id = id, name = %name, "work completed");
                });
            }

            if running.is_empty() && queue.is_empty() {
                // No runnable work left.
                break;
            }

            let completion = if cancel_requested {
                rx.recv().await
            } else {
                tokio::select! {
                    _ = cancel.cancelled() => {
                        cancel_requested = true;
                        self.cancel_all();
                        continue;
                    }
                    completion = rx.recv() => completion,
                }
            };
            let Some(completion) = completion else { break };
            running.remove(&completion.id);

            let cancelled = completion.cancelled
                || matches!(self.states.get(&completion.id), Some(WorkState::Cancelled));

            match completion.outcome {
                WorkOutcome::Cancelled => {
                    self.fail_or_cancel(
                        completion.id,
                        WorkState::Cancelled,
                        completion.attempt,
                    );
                    self.finalize_entry(completion.id, completion.work);
                }
                WorkOutcome::Success => {
                    if cancelled {
                        self.fail_or_cancel(
                            completion.id,
                            WorkState::Cancelled,
                            completion.attempt,
                        );
                    } else {
                        self.states.insert(completion.id, WorkState::Success);
                        self.emit_event(completion.id, WorkState::Success, completion.attempt);
                    }
                    self.finalize_entry(completion.id, completion.work);
                    self.enqueue_dependents(completion.id, &mut queue, &mut queued, &running);
                }
                WorkOutcome::Retry { delay } => {
                    if cancelled {
                        self.fail_or_cancel(
                            completion.id,
                            WorkState::Cancelled,
                            completion.attempt,
                        );
                        continue;
                    }
                    let no_retries = self
                        .entries
                        .get(&completion.id)
                        .is_some_and(|e| e.retries_left == 0);
                    if no_retries {
                        self.finalize_entry(completion.id, completion.work);
                        self.fail_or_cancel(
                            completion.id,
                            WorkState::Failed,
                            completion.attempt,
                        );
                        continue;
                    }
                    if let Some(entry) = self.entries.get_mut(&completion.id) {
                        entry.retries_left -= 1;
                    }
                    self.finalize_entry(completion.id, completion.work);
                    let retry_delay = if delay == Duration::ZERO {
                        self.config.retry_delay
                    } else {
                        delay
                    };
                    self.emit_event(completion.id, WorkState::Pending, completion.attempt);
                    tokio::time::sleep(retry_delay).await;
                    if queued.insert(completion.id) {
                        queue.push_back(completion.id);
                    }
                }
                WorkOutcome::Failed(err) => {
                    if cancelled {
                        self.fail_or_cancel(
                            completion.id,
                            WorkState::Cancelled,
                            completion.attempt,
                        );
                        continue;
                    }
                    warn!(work_id = completion.id, error = %err, "work failed");
                    if let Some(entry) = self.entries.get_mut(&completion.id) {
                        entry.last_error = Some(err);
                    }
                    self.finalize_entry(completion.id, completion.work);
                    self.fail_or_cancel(
                        completion.id,
                        WorkState::Failed,
                        completion.attempt,
                    );
                }
            }
        }

        info!("work scheduler finished");
    }

    // ========================================================================
    // Internal Helpers
    // ========================================================================

    /// Returns all work IDs that are currently in the Pending state.
    ///
    /// This forms the initial ready queue; actual runnability is determined
    /// by checking dependency satisfaction in [`can_run`](Self::can_run).
    fn ready_queue(&self) -> VecDeque<WorkId> {
        self.entries
            .keys()
            .filter(|id| matches!(self.states.get(id), Some(WorkState::Pending)))
            .copied()
            .collect()
    }

    /// Enqueues dependents of a completed work item that are now ready to run.
    ///
    /// Only checks the direct dependents of `completed_id` rather than
    /// scanning all entries, since those are the only items whose readiness
    /// could have changed.
    fn enqueue_dependents(
        &self,
        completed_id: WorkId,
        queue: &mut VecDeque<WorkId>,
        queued: &mut HashSet<WorkId>,
        running: &HashSet<WorkId>,
    ) {
        let Some(children) = self.dependents.get(&completed_id) else {
            return;
        };
        for &child in children {
            if running.contains(&child) {
                continue;
            }
            if !matches!(self.states.get(&child), Some(WorkState::Pending)) {
                continue;
            }
            if self.can_run(child) && queued.insert(child) {
                queue.push_back(child);
            }
        }
    }

    /// Returns `true` if all dependencies of the given work item have succeeded.
    ///
    /// A work item can only run when every work item in its `deps` list
    /// has reached the [`WorkState::Success`] state.
    fn can_run(&self, id: WorkId) -> bool {
        let Some(entry) = self.entries.get(&id) else {
            return false;
        };
        entry
            .deps
            .iter()
            .all(|dep| matches!(self.states.get(dep), Some(WorkState::Success)))
    }

    /// Marks all direct dependents of a work item as blocked.
    ///
    /// Called when a work item fails, is cancelled, or is blocked itself.
    /// Only affects dependents that are still in the Pending state.
    fn block_dependents(&mut self, id: WorkId) {
        if let Some(children) = self.dependents.get(&id).cloned() {
            for child in children {
                if matches!(self.states.get(&child), Some(WorkState::Pending)) {
                    self.states.insert(child, WorkState::Blocked);
                    self.emit_event(child, WorkState::Blocked, 0);
                }
            }
        }
    }

    /// Transitions a work item to a non-success terminal state, emits the
    /// event, and blocks its dependents.
    fn fail_or_cancel(&mut self, id: WorkId, state: WorkState, attempt: u32) {
        self.states.insert(id, state);
        self.emit_event(id, state, attempt);
        self.block_dependents(id);
    }

    /// Restores a work item after execution and records timing.
    ///
    /// Called after a spawned work task completes, regardless of outcome.
    /// Moves the work implementation back into the entry (replacing the
    /// placeholder) and records the elapsed execution time.
    fn finalize_entry(&mut self, id: WorkId, work: Option<Box<dyn Work + Send>>) {
        if let Some(entry) = self.entries.get_mut(&id) {
            if let Some(work) = work {
                entry.work = work;
            }
            if let Some(started_at) = entry.started_at.take() {
                let elapsed = started_at.elapsed();
                entry.last_duration = Some(elapsed);
                entry.total_duration += elapsed;
            }
        }
    }

    /// Sends a work event to the configured event channel, if any.
    ///
    /// Events are sent with `try_send` to avoid blocking the scheduler.
    /// If the channel is full, the event is dropped silently.
    fn emit_event(&self, id: WorkId, state: WorkState, attempt: u32) {
        let Some(tx) = self.config.event_tx.as_ref() else {
            return;
        };
        let name = self
            .entries
            .get(&id)
            .map_or_else(|| "unknown".into(), |entry| entry.name.clone());
        let _ = tx.try_send(WorkEvent {
            id,
            name,
            state,
            attempt,
        });
    }
}

/// Internal message sent when a spawned work task completes.
///
/// Carries all the information needed to update scheduler state after
/// a work item finishes executing.
struct WorkCompletion {
    /// The ID of the completed work item.
    id: WorkId,

    /// The outcome returned by the work item's `run` method.
    outcome: WorkOutcome,

    /// The work item itself, returned for potential reuse on retry.
    ///
    /// This allows stateful work items to maintain state across retries.
    work: Option<Box<dyn Work + Send>>,

    /// The attempt number for this execution.
    attempt: u32,

    /// Whether cancellation was requested during execution.
    ///
    /// Used to override the outcome if the work reported success but
    /// was actually cancelled.
    cancelled: bool,
}

/// A placeholder work item used during execution.
///
/// When a work item is spawned for execution, the scheduler must move it
/// out of the `entries` map (Rust ownership). This placeholder takes its
/// place temporarily. The real work item is moved back after execution
/// completes.
///
/// This is an internal implementation detail and should never actually
/// be executed.
struct EmptyWork;

#[async_trait]
impl Work for EmptyWork {
    fn name(&self) -> &str {
        "empty"
    }

    async fn run(&mut self, _ctx: WorkContext) -> WorkOutcome {
        WorkOutcome::Success
    }
}

// ============================================================================
// Work Sequence Helper
// ============================================================================

/// A helper for building linear sequences of dependent work items.
///
/// `WorkSequence` simplifies the common pattern of creating a chain of work
/// items where each depends on the previous one. Instead of manually tracking
/// the last work ID and passing it as a dependency, use this helper to
/// automatically chain work items.
///
/// This is particularly useful for multi-step processes like:
/// - Download -> Verify -> Apply workflows
/// - Sequential ledger processing
/// - Build pipelines with ordered stages
///
/// # Example
///
/// ```ignore
/// use henyey_work::{WorkScheduler, WorkSchedulerConfig, WorkSequence};
///
/// let mut scheduler = WorkScheduler::new(WorkSchedulerConfig::default());
/// let mut sequence = WorkSequence::new();
///
/// // Each work item automatically depends on the previous one
/// sequence.push(&mut scheduler, Box::new(step_1), 0);
/// sequence.push(&mut scheduler, Box::new(step_2), 0);
/// sequence.push(&mut scheduler, Box::new(step_3), 0);
///
/// // Run all steps in order
/// scheduler.run_until_done().await;
/// // Execution order: step_1 -> step_2 -> step_3
/// ```
///
/// # Combining with Direct Dependencies
///
/// You can also add work items with additional dependencies beyond the sequence:
///
/// ```ignore
/// let other_id = scheduler.add_work(Box::new(other_work), vec![], 0);
/// // This work depends on both the sequence and other_id
/// let combined = scheduler.add_work(
///     Box::new(final_work),
///     vec![*sequence.ids().last().unwrap(), other_id],
///     0
/// );
/// ```
#[derive(Default)]
pub struct WorkSequence {
    /// All work IDs added to this sequence, in order of addition.
    ids: Vec<WorkId>,
}

impl WorkSequence {
    /// Creates a new empty work sequence.
    pub fn new() -> Self {
        Self::default()
    }

    /// Adds a work item to the sequence.
    ///
    /// The work item will depend on the previously added item (if any).
    /// The first item in a sequence has no dependencies (from this sequence).
    ///
    /// # Arguments
    ///
    /// * `scheduler` - The scheduler to register the work with.
    /// * `work` - The work item to add.
    /// * `retries` - Number of retry attempts for this work item.
    ///
    /// # Returns
    ///
    /// The [`WorkId`] of the newly added work item.
    pub fn push(
        &mut self,
        scheduler: &mut WorkScheduler,
        work: Box<dyn Work + Send>,
        retries: u32,
    ) -> WorkId {
        let deps = self.ids.last().copied().into_iter().collect();
        let id = scheduler.add_work(work, deps, retries);
        self.ids.push(id);
        id
    }

    /// Returns all work IDs in this sequence, in order of addition.
    pub fn ids(&self) -> &[WorkId] {
        &self.ids
    }
}
