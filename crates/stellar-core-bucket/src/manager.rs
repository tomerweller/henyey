//! BucketManager - manages bucket files on disk.
//!
//! The `BucketManager` is responsible for the lifecycle of bucket files,
//! providing a high-level interface for bucket operations.
//!
//! # Responsibilities
//!
//! - **Creating** buckets from entries (sort, serialize, compress, write to disk)
//! - **Loading** buckets by content hash (from cache or disk)
//! - **Caching** frequently accessed buckets in memory
//! - **Merging** buckets and storing the results
//! - **Garbage collecting** unused bucket files
//!
//! # File Layout
//!
//! Bucket files are stored in a configurable directory with names derived
//! from their content hash:
//!
//! ```text
//! <bucket_dir>/
//!   <hash1>.bucket.xdr
//!   <hash2>.bucket.xdr
//!   ...
//! ```
//!
//! Files are uncompressed XDR with record marks (RFC 5531). The hash is
//! computed from these contents. This format supports random-access seeks
//! for efficient disk-backed indexing and streaming iteration.
//!
//! # Caching
//!
//! The manager maintains an in-memory cache of recently accessed buckets.
//! The cache uses a simple eviction policy (random eviction when full).
//! For production use, consider a more sophisticated LRU policy.
//!
//! # Thread Safety
//!
//! The manager uses `RwLock` for the cache, making it safe for concurrent
//! reads with exclusive writes. File operations are atomic (write to temp,
//! then rename).

use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::{Arc, RwLock};

use stellar_xdr::curr::{LedgerEntry, LedgerKey, Limits, WriteXdr};

use stellar_core_common::Hash256;

use crate::bucket::Bucket;
use crate::entry::BucketEntry;
use crate::merge::merge_buckets_to_file;
use crate::{BucketError, Result};

use std::sync::atomic::{AtomicU64, Ordering};

/// Global counter for generating unique temp file names.
static TEMP_FILE_COUNTER: AtomicU64 = AtomicU64::new(0);

/// Generate a unique temporary file path in the given directory.
pub(crate) fn temp_merge_path(bucket_dir: &Path) -> PathBuf {
    let id = TEMP_FILE_COUNTER.fetch_add(1, Ordering::Relaxed);
    bucket_dir.join(format!(
        "merge-tmp-{}-{}.xdr",
        std::process::id(),
        id
    ))
}

/// Manager for bucket files on disk.
///
/// The `BucketManager` provides the main interface for working with buckets
/// on disk. It handles all the complexity of serialization, compression,
/// caching, and file management.
///
/// # Example
///
/// ```ignore
/// use stellar_core_bucket::{BucketManager, BucketEntry};
///
/// // Create a manager
/// let manager = BucketManager::new("/path/to/buckets".into())?;
///
/// // Create a bucket from entries
/// let entries = vec![BucketEntry::Live(some_entry)];
/// let bucket = manager.create_bucket(entries)?;
///
/// // Later, load the bucket by hash
/// let loaded = manager.load_bucket(&bucket.hash())?;
///
/// // Garbage collect unused buckets
/// let active_hashes = bucket_list.all_bucket_hashes();
/// manager.retain_buckets(&active_hashes)?;
/// ```
///
/// # Cache Behavior
///
/// The manager caches buckets in memory up to `max_cache_size`. When the
/// cache is full, a random entry is evicted. All cache operations are
/// thread-safe via `RwLock`.
pub struct BucketManager {
    /// Directory where bucket files are stored.
    bucket_dir: PathBuf,
    /// Cache of loaded buckets, keyed by content hash.
    cache: RwLock<HashMap<Hash256, Arc<Bucket>>>,
    /// Maximum number of buckets to keep in cache.
    max_cache_size: usize,
    /// Whether to persist/load disk indexes alongside bucket files.
    persist_index: bool,
}

impl BucketManager {
    /// Default maximum cache size.
    pub const DEFAULT_MAX_CACHE_SIZE: usize = 100;

    /// File size threshold (in bytes) above which buckets are loaded as DiskBacked
    /// instead of InMemory. Files larger than this threshold are accessed via
    /// random disk I/O with an in-memory index, avoiding loading the full file.
    ///
    /// Default: 10 MB. Files under this size are small enough to load entirely
    /// into memory without significant memory pressure.
    pub const DISK_BACKED_THRESHOLD: u64 = 10 * 1024 * 1024;

    /// Create a new BucketManager with the given directory.
    pub fn new(bucket_dir: PathBuf) -> Result<Self> {
        // Create directory if it doesn't exist
        std::fs::create_dir_all(&bucket_dir)?;

        Ok(Self {
            bucket_dir,
            cache: RwLock::new(HashMap::new()),
            max_cache_size: Self::DEFAULT_MAX_CACHE_SIZE,
            persist_index: false,
        })
    }

    /// Create a new BucketManager with a custom cache size.
    pub fn with_cache_size(bucket_dir: PathBuf, max_cache_size: usize) -> Result<Self> {
        std::fs::create_dir_all(&bucket_dir)?;

        Ok(Self {
            bucket_dir,
            cache: RwLock::new(HashMap::new()),
            max_cache_size,
            persist_index: false,
        })
    }

    /// Create a new BucketManager with index persistence enabled.
    ///
    /// When `persist_index` is true, disk indexes are saved alongside bucket files
    /// and loaded on startup, avoiding expensive index rebuilds.
    pub fn with_persist_index(bucket_dir: PathBuf, persist_index: bool) -> Result<Self> {
        std::fs::create_dir_all(&bucket_dir)?;

        Ok(Self {
            bucket_dir,
            cache: RwLock::new(HashMap::new()),
            max_cache_size: Self::DEFAULT_MAX_CACHE_SIZE,
            persist_index,
        })
    }

    /// Returns whether index persistence is enabled.
    pub fn persist_index(&self) -> bool {
        self.persist_index
    }

    /// Get the bucket directory path.
    pub fn bucket_dir(&self) -> &Path {
        &self.bucket_dir
    }

    /// Get the file path for a bucket with the given hash.
    ///
    /// Returns the path using the canonical `.bucket.xdr` extension
    /// (uncompressed XDR with record marks).
    pub fn bucket_path(&self, hash: &Hash256) -> PathBuf {
        self.bucket_dir.join(format!("{}.bucket.xdr", hash.to_hex()))
    }

    /// Get the legacy gzip file path for a bucket with the given hash.
    ///
    /// Used for migration from the old `.bucket.gz` format.
    fn legacy_bucket_path(&self, hash: &Hash256) -> PathBuf {
        self.bucket_dir.join(format!("{}.bucket.gz", hash.to_hex()))
    }

    /// Create a bucket from entries.
    ///
    /// This will:
    /// 1. Sort entries by key
    /// 2. Create the bucket
    /// 3. Save to disk as uncompressed XDR
    /// 4. Add to cache
    pub fn create_bucket(&self, entries: Vec<BucketEntry>) -> Result<Arc<Bucket>> {
        if entries.is_empty() {
            return Ok(Arc::new(Bucket::empty()));
        }

        // Create bucket (entries will be sorted)
        let bucket = Bucket::from_entries(entries)?;
        let hash = bucket.hash();

        // Check if already cached
        {
            let cache = self.cache.read().unwrap();
            if let Some(cached) = cache.get(&hash) {
                return Ok(Arc::clone(cached));
            }
        }

        // Save to disk as uncompressed XDR
        let path = self.bucket_path(&hash);
        bucket.save_to_xdr_file(&path)?;

        // Add to cache
        let bucket = Arc::new(bucket);
        self.add_to_cache(hash, Arc::clone(&bucket));

        Ok(bucket)
    }

    /// Create a bucket from live and dead entries.
    pub fn create_bucket_from_ledger_entries(
        &self,
        live_entries: Vec<LedgerEntry>,
        dead_entries: Vec<LedgerKey>,
    ) -> Result<Arc<Bucket>> {
        let mut entries: Vec<BucketEntry> =
            live_entries.into_iter().map(BucketEntry::Live).collect();

        entries.extend(dead_entries.into_iter().map(BucketEntry::Dead));

        self.create_bucket(entries)
    }

    /// Load a bucket by its hash.
    ///
    /// First checks the cache, then loads from disk if not cached.
    /// Supports both the canonical `.bucket.xdr` format and the legacy
    /// `.bucket.gz` format (with automatic migration).
    ///
    /// Files larger than [`DISK_BACKED_THRESHOLD`] are loaded as DiskBacked
    /// buckets (only the index is in memory); smaller files are loaded entirely
    /// into memory for faster access.
    pub fn load_bucket(&self, hash: &Hash256) -> Result<Arc<Bucket>> {
        // Check if it's the empty bucket
        if hash.is_zero() {
            return Ok(Arc::new(Bucket::empty()));
        }

        // Check cache first
        {
            let cache = self.cache.read().unwrap();
            if let Some(bucket) = cache.get(hash) {
                return Ok(Arc::clone(bucket));
            }
        }

        // Try canonical .bucket.xdr path first
        let xdr_path = self.bucket_path(hash);
        if !xdr_path.exists() {
            // Fall back to legacy .bucket.gz format with migration
            let gz_path = self.legacy_bucket_path(hash);
            if !gz_path.exists() {
                return Err(BucketError::NotFound(hash.to_hex()));
            }

            // Streaming migration: decompress gz → xdr without loading all entries
            Bucket::migrate_gz_to_xdr(&gz_path, &xdr_path)?;
            std::fs::remove_file(&gz_path)?;
            tracing::info!(
                hash = %hash,
                "Migrated bucket from .bucket.gz to .bucket.xdr"
            );
        }

        // Load from .bucket.xdr based on file size
        let file_size = std::fs::metadata(&xdr_path)?.len();
        let bucket = if file_size > Self::DISK_BACKED_THRESHOLD {
            // Large file: use DiskBacked with streaming index build (O(index_size) memory)
            tracing::debug!(
                hash = %hash,
                file_size,
                "Loading bucket as DiskBacked (file exceeds threshold)"
            );
            Bucket::from_xdr_file_disk_backed(&xdr_path)?
        } else {
            // Small file: load entirely into memory for fast access
            Bucket::load_from_xdr_file(&xdr_path)?
        };

        // Verify hash matches
        if bucket.hash() != *hash {
            return Err(BucketError::HashMismatch {
                expected: hash.to_hex(),
                actual: bucket.hash().to_hex(),
            });
        }

        // Add to cache
        let bucket = Arc::new(bucket);
        self.add_to_cache(*hash, Arc::clone(&bucket));

        Ok(bucket)
    }

    /// Load a hot archive bucket by its hash.
    ///
    /// Hot archive buckets contain `HotArchiveBucketEntry` instead of `BucketEntry`.
    /// This method loads and parses the bucket file with the correct entry type.
    /// Supports both canonical `.bucket.xdr` and legacy `.bucket.gz` formats.
    ///
    /// For files larger than `DISK_BACKED_THRESHOLD`, creates a DiskBacked bucket
    /// that only holds an index in memory (matching C++ behavior for hot archive).
    ///
    /// Note: Hot archive buckets are not cached (they use a different entry type).
    pub fn load_hot_archive_bucket(
        &self,
        hash: &Hash256,
    ) -> Result<crate::hot_archive::HotArchiveBucket> {
        // Check if it's the empty bucket
        if hash.is_zero() {
            return Ok(crate::hot_archive::HotArchiveBucket::empty());
        }

        // Try canonical .bucket.xdr path first
        let xdr_path = self.bucket_path(hash);
        if !xdr_path.exists() {
            // Fall back to legacy .bucket.gz format and migrate
            let gz_path = self.legacy_bucket_path(hash);
            if !gz_path.exists() {
                return Err(BucketError::NotFound(hash.to_hex()));
            }

            // Streaming migration: decompress gz → xdr without loading all entries
            Bucket::migrate_gz_to_xdr(&gz_path, &xdr_path)?;
            std::fs::remove_file(&gz_path)?;
            tracing::info!(
                hash = %hash,
                "Migrated hot archive bucket from .bucket.gz to .bucket.xdr"
            );
        }

        // Load based on file size: DiskBacked for large files, InMemory for small
        let file_size = std::fs::metadata(&xdr_path)?.len();
        let bucket = if file_size > Self::DISK_BACKED_THRESHOLD {
            crate::hot_archive::HotArchiveBucket::from_xdr_file_disk_backed(&xdr_path)?
        } else {
            crate::hot_archive::HotArchiveBucket::load_from_xdr_file(&xdr_path)?
        };

        // Verify hash matches
        if bucket.hash() != *hash {
            return Err(BucketError::HashMismatch {
                expected: hash.to_hex(),
                actual: bucket.hash().to_hex(),
            });
        }

        Ok(bucket)
    }

    /// Check if a bucket exists (in cache or on disk).
    ///
    /// Checks both the canonical `.bucket.xdr` and legacy `.bucket.gz` paths.
    pub fn bucket_exists(&self, hash: &Hash256) -> bool {
        if hash.is_zero() {
            return true; // Empty bucket always "exists"
        }

        // Check cache
        {
            let cache = self.cache.read().unwrap();
            if cache.contains_key(hash) {
                return true;
            }
        }

        // Check disk (canonical path first, then legacy)
        self.bucket_path(hash).exists() || self.legacy_bucket_path(hash).exists()
    }

    /// Merge two buckets and create a new bucket.
    ///
    /// Uses disk-backed streaming merge to avoid loading all entries into memory.
    /// The merge output is written directly to disk and the resulting bucket is
    /// DiskBacked, keeping memory usage O(index_size) instead of O(data_size).
    pub fn merge(
        &self,
        old: &Bucket,
        new: &Bucket,
        max_protocol_version: u32,
    ) -> Result<Arc<Bucket>> {
        if old.is_empty() && new.is_empty() {
            return Ok(Arc::new(Bucket::empty()));
        }

        // Write merge output to a temp file
        let temp_path = temp_merge_path(&self.bucket_dir);
        let (hash, entry_count) = merge_buckets_to_file(
            old,
            new,
            &temp_path,
            true, // keep_dead_entries
            max_protocol_version,
            true, // normalize_init_entries
        )?;

        if hash.is_zero() || entry_count == 0 {
            let _ = std::fs::remove_file(&temp_path);
            return Ok(Arc::new(Bucket::empty()));
        }

        // Check if already cached
        {
            let cache = self.cache.read().unwrap();
            if let Some(cached) = cache.get(&hash) {
                let _ = std::fs::remove_file(&temp_path);
                return Ok(Arc::clone(cached));
            }
        }

        // Move to final canonical path
        let final_path = self.bucket_path(&hash);
        if !final_path.exists() {
            std::fs::rename(&temp_path, &final_path)?;
        } else {
            let _ = std::fs::remove_file(&temp_path);
        }

        // Load as DiskBacked (builds index, O(index_size) memory)
        let bucket = Bucket::from_xdr_file_disk_backed(&final_path)?;
        let bucket = Arc::new(bucket);
        self.add_to_cache(hash, Arc::clone(&bucket));

        Ok(bucket)
    }

    /// Merge two buckets asynchronously.
    pub async fn merge_async(
        &self,
        old: &Bucket,
        new: &Bucket,
        max_protocol_version: u32,
    ) -> Result<Arc<Bucket>> {
        // For now, just call the sync version
        // In the future, this could use tokio::task::spawn_blocking
        self.merge(old, new, max_protocol_version)
    }

    /// Save a disk index for a bucket.
    ///
    /// Only effective when `persist_index` is true. The index is saved as a
    /// `.index` file alongside the bucket file.
    ///
    /// # Arguments
    ///
    /// * `hash` - The bucket hash (used to derive file paths)
    /// * `index` - The DiskIndex to persist
    pub fn save_index_for_bucket(
        &self,
        hash: &Hash256,
        index: &crate::index::DiskIndex,
    ) -> Result<()> {
        if !self.persist_index {
            return Ok(());
        }
        let bucket_path = self.bucket_path(hash);
        crate::index_persistence::save_disk_index(index, &bucket_path)
    }

    /// Try to load a persisted disk index for a bucket.
    ///
    /// Returns `None` if persistence is disabled, the file doesn't exist,
    /// or the stored version/page-size doesn't match.
    ///
    /// # Arguments
    ///
    /// * `hash` - The bucket hash
    /// * `expected_page_size` - Expected page size for validation
    pub fn try_load_index_for_bucket(
        &self,
        hash: &Hash256,
        expected_page_size: u64,
    ) -> Result<Option<crate::index::DiskIndex>> {
        if !self.persist_index {
            return Ok(None);
        }
        let bucket_path = self.bucket_path(hash);
        crate::index_persistence::load_disk_index(&bucket_path, expected_page_size)
    }

    /// Add a bucket to the cache.
    fn add_to_cache(&self, hash: Hash256, bucket: Arc<Bucket>) {
        let mut cache = self.cache.write().unwrap();

        // Evict if cache is full (simple LRU would be better)
        if cache.len() >= self.max_cache_size {
            // Remove a random entry (not ideal, but simple)
            if let Some(key) = cache.keys().next().cloned() {
                cache.remove(&key);
            }
        }

        cache.insert(hash, bucket);
    }

    /// Clear the bucket cache.
    pub fn clear_cache(&self) {
        let mut cache = self.cache.write().unwrap();
        cache.clear();
    }

    /// Get the number of cached buckets.
    pub fn cache_size(&self) -> usize {
        let cache = self.cache.read().unwrap();
        cache.len()
    }

    /// List all bucket files in the directory.
    ///
    /// Finds both canonical `.bucket.xdr` and legacy `.bucket.gz` files.
    pub fn list_buckets(&self) -> Result<Vec<Hash256>> {
        let mut hashes = Vec::new();

        for entry in std::fs::read_dir(&self.bucket_dir)? {
            let entry = entry?;
            let path = entry.path();

            if let Some(name) = path.file_name().and_then(|n| n.to_str()) {
                let hash_str = if name.ends_with(".bucket.xdr") {
                    Some(name.trim_end_matches(".bucket.xdr"))
                } else if name.ends_with(".bucket.gz") {
                    Some(name.trim_end_matches(".bucket.gz"))
                } else {
                    None
                };

                if let Some(hash_str) = hash_str {
                    if let Ok(hash) = Hash256::from_hex(hash_str) {
                        if !hashes.contains(&hash) {
                            hashes.push(hash);
                        }
                    }
                }
            }
        }

        Ok(hashes)
    }

    /// Delete a bucket file.
    ///
    /// This also removes the bucket from the cache and deletes the
    /// associated `.index` file when `persist_index` is true.
    /// Removes both `.bucket.xdr` and legacy `.bucket.gz` files if present.
    pub fn delete_bucket(&self, hash: &Hash256) -> Result<()> {
        // Remove from cache
        {
            let mut cache = self.cache.write().unwrap();
            cache.remove(hash);
        }

        // Delete canonical .bucket.xdr file
        let xdr_path = self.bucket_path(hash);
        if xdr_path.exists() {
            // Delete associated index file
            if self.persist_index {
                let _ = crate::index_persistence::delete_index(&xdr_path);
            }
            std::fs::remove_file(&xdr_path)?;
        }

        // Delete legacy .bucket.gz file if it exists
        let gz_path = self.legacy_bucket_path(hash);
        if gz_path.exists() {
            if self.persist_index {
                let _ = crate::index_persistence::delete_index(&gz_path);
            }
            std::fs::remove_file(&gz_path)?;
        }

        Ok(())
    }

    /// Delete all bucket files not in the given set of hashes.
    ///
    /// This is useful for garbage collection. When `persist_index` is enabled,
    /// also cleans up orphaned `.index` files.
    pub fn retain_buckets(&self, keep: &[Hash256]) -> Result<usize> {
        let keep_set: std::collections::HashSet<_> = keep.iter().collect();
        let all_buckets = self.list_buckets()?;
        let mut deleted = 0;

        for hash in all_buckets {
            if !keep_set.contains(&hash) {
                self.delete_bucket(&hash)?;
                deleted += 1;
            }
        }

        // Clean up any orphaned index files
        if self.persist_index {
            let _ = crate::index_persistence::cleanup_orphaned_indexes(&self.bucket_dir);
        }

        Ok(deleted)
    }

    /// Get statistics about the bucket manager.
    pub fn stats(&self) -> BucketManagerStats {
        let cached = self.cache_size();
        let on_disk = self.list_buckets().map(|v| v.len()).unwrap_or(0);

        BucketManagerStats {
            cached_buckets: cached,
            disk_buckets: on_disk,
            bucket_dir: self.bucket_dir.clone(),
        }
    }

    /// Import a bucket from raw XDR bytes.
    ///
    /// Saves the raw bytes to disk first, then loads via the threshold-aware
    /// path (DiskBacked for files > 10 MB, InMemory for smaller files).
    /// This avoids loading all entries into memory for large buckets.
    pub fn import_bucket(&self, xdr_bytes: &[u8]) -> Result<Arc<Bucket>> {
        // Compute hash directly from raw bytes (no entry parsing needed)
        let hash = Hash256::hash(xdr_bytes);

        // Check cache
        {
            let cache = self.cache.read().unwrap();
            if let Some(cached) = cache.get(&hash) {
                return Ok(Arc::clone(cached));
            }
        }

        // Save raw XDR bytes to disk if not already there
        let path = self.bucket_path(&hash);
        if !path.exists() {
            use std::io::Write;
            if let Some(parent) = path.parent() {
                std::fs::create_dir_all(parent)?;
            }
            let mut file = std::fs::File::create(&path)?;
            file.write_all(xdr_bytes)?;
            file.sync_all()?;
        }

        // Load via threshold-aware path (DiskBacked for large, InMemory for small)
        self.load_bucket(&hash)
    }

    /// Export a bucket to raw XDR bytes.
    pub fn export_bucket(&self, hash: &Hash256) -> Result<Vec<u8>> {
        let bucket = self.load_bucket(hash)?;
        bucket.to_xdr_bytes()
    }

    /// Visits all ledger entries in buckets matching the filter criteria.
    ///
    /// This method iterates through bucket entries, applying a filter function
    /// to determine which entries to process, and an accept function to handle
    /// matching entries.
    ///
    /// # Arguments
    ///
    /// * `buckets` - Iterator over buckets to scan
    /// * `filter_entry` - Function that returns `true` for entries to consider
    /// * `accept_entry` - Function called for each matching entry; return `false` to stop
    /// * `min_ledger` - Optional minimum ledger sequence to filter entries
    ///
    /// # Returns
    ///
    /// `true` if iteration completed, `false` if stopped early by `accept_entry`.
    ///
    /// # Example
    ///
    /// ```ignore
    /// manager.visit_ledger_entries(
    ///     &bucket_hashes,
    ///     |entry| matches!(entry.data, LedgerEntryData::Account(_)),
    ///     |entry| {
    ///         println!("Found account: {:?}", entry);
    ///         true // continue
    ///     },
    ///     Some(1000), // only entries modified after ledger 1000
    /// );
    /// ```
    pub fn visit_ledger_entries<F, A>(
        &self,
        bucket_hashes: &[Hash256],
        filter_entry: F,
        mut accept_entry: A,
        min_ledger: Option<u32>,
    ) -> Result<bool>
    where
        F: Fn(&LedgerEntry) -> bool,
        A: FnMut(&LedgerEntry) -> bool,
    {
        use std::collections::HashSet;

        // Track seen keys to avoid processing duplicates
        let mut seen_keys: HashSet<LedgerKey> = HashSet::new();

        for hash in bucket_hashes {
            if hash.is_zero() {
                continue;
            }

            let bucket = self.load_bucket(hash)?;

            for entry in bucket.iter() {
                match entry {
                    crate::BucketEntry::Live(ref ledger_entry)
                    | crate::BucketEntry::Init(ref ledger_entry) => {
                        // Check min_ledger filter
                        if let Some(min) = min_ledger {
                            if ledger_entry.last_modified_ledger_seq < min {
                                continue;
                            }
                        }

                        // Check filter
                        if !filter_entry(ledger_entry) {
                            continue;
                        }

                        // Check if already seen
                        if let Some(key) = crate::entry::ledger_entry_to_key(ledger_entry) {
                            if seen_keys.contains(&key) {
                                continue;
                            }
                            seen_keys.insert(key);
                        }

                        // Accept the entry
                        if !accept_entry(ledger_entry) {
                            return Ok(false);
                        }
                    }
                    crate::BucketEntry::Dead(ref key) => {
                        // Mark key as seen (it's deleted)
                        seen_keys.insert(key.clone());
                    }
                    crate::BucketEntry::Metadata(_) => {
                        // Skip metadata entries
                    }
                }
            }
        }

        Ok(true)
    }

    /// Visits all ledger entries of a specific type in the given buckets.
    ///
    /// This is a convenience wrapper around [`visit_ledger_entries`] that
    /// filters by entry type.
    pub fn visit_ledger_entries_of_type<A>(
        &self,
        bucket_hashes: &[Hash256],
        entry_type: stellar_xdr::curr::LedgerEntryType,
        accept_entry: A,
        min_ledger: Option<u32>,
    ) -> Result<bool>
    where
        A: FnMut(&LedgerEntry) -> bool,
    {
        self.visit_ledger_entries(
            bucket_hashes,
            |entry| ledger_entry_type(&entry.data) == entry_type,
            accept_entry,
            min_ledger,
        )
    }
}

/// Returns the ledger entry type for a given entry data.
fn ledger_entry_type(
    data: &stellar_xdr::curr::LedgerEntryData,
) -> stellar_xdr::curr::LedgerEntryType {
    use stellar_xdr::curr::{LedgerEntryData, LedgerEntryType};
    match data {
        LedgerEntryData::Account(_) => LedgerEntryType::Account,
        LedgerEntryData::Trustline(_) => LedgerEntryType::Trustline,
        LedgerEntryData::Offer(_) => LedgerEntryType::Offer,
        LedgerEntryData::Data(_) => LedgerEntryType::Data,
        LedgerEntryData::ClaimableBalance(_) => LedgerEntryType::ClaimableBalance,
        LedgerEntryData::LiquidityPool(_) => LedgerEntryType::LiquidityPool,
        LedgerEntryData::ContractData(_) => LedgerEntryType::ContractData,
        LedgerEntryData::ContractCode(_) => LedgerEntryType::ContractCode,
        LedgerEntryData::ConfigSetting(_) => LedgerEntryType::ConfigSetting,
        LedgerEntryData::Ttl(_) => LedgerEntryType::Ttl,
    }
}

impl BucketManager {
    /// Loads the complete ledger state from a list of bucket hashes.
    ///
    /// This iterates through all buckets from oldest to newest (reverse order),
    /// building a map of all live entries. Dead entries shadow older versions.
    /// The result is a complete view of ledger state at the bucket list snapshot.
    ///
    /// This is the Rust equivalent of C++ `loadCompleteLedgerState`.
    ///
    /// # Arguments
    ///
    /// * `bucket_hashes` - List of bucket hashes in order from level 10 snap to level 0 curr
    ///   (oldest to newest is correct order for last-write-wins)
    ///
    /// # Returns
    ///
    /// A vector of all live ledger entries, with newer entries shadowing older ones.
    pub fn load_complete_ledger_state(
        &self,
        bucket_hashes: &[Hash256],
    ) -> Result<Vec<LedgerEntry>> {
        use std::collections::BTreeMap;

        // Use BTreeMap to maintain sorted order by key
        let mut state: BTreeMap<Vec<u8>, LedgerEntry> = BTreeMap::new();

        // Process buckets from oldest to newest (so newer entries overwrite older)
        // bucket_hashes should be in order: level 10 snap, level 10 curr, ..., level 0 snap, level 0 curr
        // We iterate in that order so newest entries win
        for hash in bucket_hashes {
            if hash.is_zero() {
                continue;
            }

            let bucket = self.load_bucket(hash)?;

            for entry in bucket.iter() {
                match entry {
                    crate::BucketEntry::Live(ref ledger_entry)
                    | crate::BucketEntry::Init(ref ledger_entry) => {
                        if let Some(key) = crate::entry::ledger_entry_to_key(ledger_entry) {
                            let key_bytes = key.to_xdr(Limits::none()).map_err(|e| {
                                BucketError::Serialization(format!(
                                    "failed to serialize ledger key: {}",
                                    e
                                ))
                            })?;
                            state.insert(key_bytes, ledger_entry.clone());
                        }
                    }
                    crate::BucketEntry::Dead(ref key) => {
                        let key_bytes = key.to_xdr(Limits::none()).map_err(|e| {
                            BucketError::Serialization(format!(
                                "failed to serialize ledger key: {}",
                                e
                            ))
                        })?;
                        // Remove the entry if it exists (dead shadows live)
                        state.remove(&key_bytes);
                    }
                    crate::BucketEntry::Metadata(_) => {
                        // Skip metadata entries
                    }
                }
            }
        }

        Ok(state.into_values().collect())
    }

    /// Merges all buckets in a bucket list into a single "super bucket".
    ///
    /// This creates a consolidated bucket containing all live entries from
    /// the bucket list. Useful for creating offline archives or testing.
    ///
    /// This is the Rust equivalent of C++ `mergeBuckets` (the standalone function
    /// that merges an entire bucket list).
    ///
    /// # Arguments
    ///
    /// * `bucket_hashes` - List of bucket hashes from the bucket list
    /// * `protocol_version` - Protocol version for the output bucket
    ///
    /// # Returns
    ///
    /// A single bucket containing all live entries.
    pub fn merge_all_buckets(
        &self,
        bucket_hashes: &[Hash256],
        protocol_version: u32,
    ) -> Result<Arc<Bucket>> {
        use stellar_xdr::curr::BucketMetadata;

        // Load complete state
        let entries = self.load_complete_ledger_state(bucket_hashes)?;

        if entries.is_empty() {
            return Ok(Arc::new(Bucket::empty()));
        }

        // Create bucket entries with metadata
        let mut bucket_entries: Vec<crate::BucketEntry> = Vec::with_capacity(entries.len() + 1);

        // Add metadata for protocol 11+
        if protocol_version >= 11 {
            bucket_entries.push(crate::BucketEntry::Metadata(BucketMetadata {
                ledger_version: protocol_version,
                ext: stellar_xdr::curr::BucketMetadataExt::V0,
            }));
        }

        // Add all entries as LIVE (not INIT since these are resolved entries)
        bucket_entries.extend(entries.into_iter().map(crate::BucketEntry::Live));

        // Create and save the merged bucket
        self.create_bucket(bucket_entries)
    }

    /// Verifies that all referenced bucket files exist on disk.
    ///
    /// This is a simple synchronous check. For background verification with
    /// hash validation, use `verify_bucket_hashes`.
    ///
    /// # Arguments
    ///
    /// * `bucket_hashes` - List of bucket hashes to verify
    ///
    /// # Returns
    ///
    /// A list of missing bucket hashes, or empty if all exist.
    pub fn verify_buckets_exist(&self, bucket_hashes: &[Hash256]) -> Vec<Hash256> {
        bucket_hashes
            .iter()
            .filter(|hash| !hash.is_zero() && !self.bucket_exists(hash))
            .copied()
            .collect()
    }

    /// Verifies bucket file contents match their expected hashes.
    ///
    /// This performs a full hash verification by reading and decompressing
    /// each bucket file. This is expensive and should be used sparingly.
    ///
    /// This is a simplified version of C++ `scheduleVerifyReferencedBucketsWork`.
    ///
    /// # Arguments
    ///
    /// * `bucket_hashes` - List of bucket hashes to verify
    ///
    /// # Returns
    ///
    /// A list of (expected_hash, actual_hash) pairs for any mismatched buckets.
    pub fn verify_bucket_hashes(
        &self,
        bucket_hashes: &[Hash256],
    ) -> Result<Vec<(Hash256, Hash256)>> {
        let mut mismatches = Vec::new();

        for expected_hash in bucket_hashes {
            if expected_hash.is_zero() {
                continue;
            }

            // Try canonical path first, then legacy
            let xdr_path = self.bucket_path(expected_hash);
            let gz_path = self.legacy_bucket_path(expected_hash);

            let load_result = if xdr_path.exists() {
                Bucket::load_from_xdr_file(&xdr_path)
            } else if gz_path.exists() {
                Bucket::load_from_file(&gz_path)
            } else {
                continue; // Skip missing files (use verify_buckets_exist for that check)
            };

            match load_result {
                Ok(bucket) => {
                    let actual_hash = bucket.hash();
                    if actual_hash != *expected_hash {
                        mismatches.push((*expected_hash, actual_hash));
                    }
                }
                Err(_) => {
                    // File exists but couldn't be loaded - treat as hash mismatch
                    mismatches.push((*expected_hash, Hash256::ZERO));
                }
            }
        }

        Ok(mismatches)
    }

    /// Ensures all referenced bucket files exist, downloading if needed.
    ///
    /// This method checks if each bucket exists locally. For missing buckets,
    /// it calls the provided fetch function to obtain the bucket data.
    ///
    /// This supports the `assumeState` flow for restoring from HistoryArchiveState.
    ///
    /// # Arguments
    ///
    /// * `bucket_hashes` - List of bucket hashes required
    /// * `fetch_bucket` - Function that fetches bucket XDR bytes by hash
    ///
    /// # Returns
    ///
    /// The number of buckets that were fetched (not already present).
    pub fn ensure_buckets_exist<F>(
        &self,
        bucket_hashes: &[Hash256],
        mut fetch_bucket: F,
    ) -> Result<usize>
    where
        F: FnMut(&Hash256) -> Result<Vec<u8>>,
    {
        let mut fetched = 0;

        for hash in bucket_hashes {
            if hash.is_zero() {
                continue;
            }

            if self.bucket_exists(hash) {
                continue;
            }

            // Fetch and import the bucket
            let xdr_bytes = fetch_bucket(hash)?;
            let bucket = self.import_bucket(&xdr_bytes)?;

            // Verify hash matches
            if bucket.hash() != *hash {
                return Err(BucketError::HashMismatch {
                    expected: hash.to_hex(),
                    actual: bucket.hash().to_hex(),
                });
            }

            fetched += 1;
        }

        Ok(fetched)
    }

    /// Clean up unreferenced merge temp files from the bucket directory.
    ///
    /// This method removes `merge-tmp-*.xdr` files that are not in the provided
    /// set of referenced file paths. This is useful for garbage collection during
    /// long-running operations like verify-execution where many merge temp files
    /// are created but only a subset remain referenced by the bucket list.
    ///
    /// # Arguments
    ///
    /// * `referenced_paths` - Set of file paths that should be kept (from bucket list's
    ///   `referenced_file_paths()` method)
    ///
    /// # Returns
    ///
    /// The number of files deleted.
    pub fn cleanup_unreferenced_files(
        &self,
        referenced_paths: &std::collections::HashSet<PathBuf>,
    ) -> Result<usize> {
        let mut deleted = 0;

        // List all files in bucket directory that match merge-tmp-*.xdr pattern
        for entry in std::fs::read_dir(&self.bucket_dir)? {
            let entry = entry?;
            let path = entry.path();

            if let Some(name) = path.file_name().and_then(|n| n.to_str()) {
                // Only consider merge temp files
                if name.starts_with("merge-tmp-") && name.ends_with(".xdr") {
                    // Delete if not referenced
                    if !referenced_paths.contains(&path) {
                        tracing::debug!(path = %path.display(), "Deleting unreferenced merge temp file");
                        if let Err(e) = std::fs::remove_file(&path) {
                            tracing::warn!(
                                path = %path.display(),
                                error = %e,
                                "Failed to delete unreferenced merge temp file"
                            );
                        } else {
                            deleted += 1;
                        }
                    }
                }
            }
        }

        if deleted > 0 {
            tracing::info!(deleted, "Cleaned up unreferenced merge temp files");
        }

        Ok(deleted)
    }
}

impl std::fmt::Debug for BucketManager {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("BucketManager")
            .field("bucket_dir", &self.bucket_dir)
            .field("cache_size", &self.cache_size())
            .field("max_cache_size", &self.max_cache_size)
            .finish()
    }
}

/// Statistics about a BucketManager.
///
/// Provides insight into the manager's state for monitoring and debugging.
#[derive(Debug, Clone)]
pub struct BucketManagerStats {
    /// Number of buckets currently held in the in-memory cache.
    pub cached_buckets: usize,
    /// Number of bucket files on disk (may include garbage).
    pub disk_buckets: usize,
    /// The directory where bucket files are stored.
    pub bucket_dir: PathBuf,
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::BucketEntry; // Re-import to shadow XDR's BucketEntry
    use stellar_xdr::curr::*;
    use tempfile::TempDir;

    fn make_account_id(bytes: [u8; 32]) -> AccountId {
        AccountId(PublicKey::PublicKeyTypeEd25519(Uint256(bytes)))
    }

    fn make_account_entry(bytes: [u8; 32], balance: i64) -> LedgerEntry {
        LedgerEntry {
            last_modified_ledger_seq: 1,
            data: LedgerEntryData::Account(AccountEntry {
                account_id: make_account_id(bytes),
                balance,
                seq_num: SequenceNumber(1),
                num_sub_entries: 0,
                inflation_dest: None,
                flags: 0,
                home_domain: String32::default(),
                thresholds: Thresholds([1, 0, 0, 0]),
                signers: Vec::new().try_into().unwrap(),
                ext: AccountEntryExt::V0,
            }),
            ext: LedgerEntryExt::V0,
        }
    }

    fn make_account_key(bytes: [u8; 32]) -> LedgerKey {
        LedgerKey::Account(LedgerKeyAccount {
            account_id: make_account_id(bytes),
        })
    }

    fn create_manager() -> (TempDir, BucketManager) {
        let temp_dir = TempDir::new().unwrap();
        let manager = BucketManager::new(temp_dir.path().to_path_buf()).unwrap();
        (temp_dir, manager)
    }

    #[test]
    fn test_create_bucket() {
        let (_temp_dir, manager) = create_manager();

        let entries = vec![
            BucketEntry::Live(make_account_entry([1u8; 32], 100)),
            BucketEntry::Live(make_account_entry([2u8; 32], 200)),
        ];

        let bucket = manager.create_bucket(entries).unwrap();
        assert_eq!(bucket.len(), 2);
        assert!(!bucket.hash().is_zero());
    }

    #[test]
    fn test_load_bucket() {
        let (_temp_dir, manager) = create_manager();

        let entries = vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))];

        let bucket = manager.create_bucket(entries).unwrap();
        let hash = bucket.hash();

        // Clear cache to force disk load
        manager.clear_cache();

        let loaded = manager.load_bucket(&hash).unwrap();
        assert_eq!(loaded.hash(), hash);
        assert_eq!(loaded.len(), 1);
    }

    #[test]
    fn test_bucket_caching() {
        let (_temp_dir, manager) = create_manager();

        let entries = vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))];

        let bucket = manager.create_bucket(entries).unwrap();
        let hash = bucket.hash();

        assert_eq!(manager.cache_size(), 1);

        // Loading again should use cache
        let loaded = manager.load_bucket(&hash).unwrap();
        assert!(Arc::ptr_eq(&bucket, &loaded));
    }

    #[test]
    fn test_bucket_exists() {
        let (_temp_dir, manager) = create_manager();

        let entries = vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))];

        let bucket = manager.create_bucket(entries).unwrap();
        let hash = bucket.hash();

        assert!(manager.bucket_exists(&hash));
        assert!(!manager.bucket_exists(&Hash256::hash(b"nonexistent")));
        assert!(manager.bucket_exists(&Hash256::ZERO)); // Empty bucket
    }

    #[test]
    fn test_merge_buckets() {
        let (_temp_dir, manager) = create_manager();

        let old_entries = vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))];
        let new_entries = vec![BucketEntry::Live(make_account_entry([1u8; 32], 200))];

        let old_bucket = Bucket::from_entries(old_entries).unwrap();
        let new_bucket = Bucket::from_entries(new_entries).unwrap();

        let merged = manager.merge(&old_bucket, &new_bucket, 0).unwrap();
        assert_eq!(merged.len(), 1);

        let key = make_account_key([1u8; 32]);
        let entry = merged.get_entry(&key).unwrap().unwrap();
        if let LedgerEntryData::Account(account) = &entry.data {
            assert_eq!(account.balance, 200);
        }
    }

    #[test]
    fn test_list_and_delete_buckets() {
        let (_temp_dir, manager) = create_manager();

        let entries1 = vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))];
        let entries2 = vec![BucketEntry::Live(make_account_entry([2u8; 32], 200))];

        let bucket1 = manager.create_bucket(entries1).unwrap();
        let bucket2 = manager.create_bucket(entries2).unwrap();

        let buckets = manager.list_buckets().unwrap();
        assert_eq!(buckets.len(), 2);

        manager.delete_bucket(&bucket1.hash()).unwrap();

        let buckets = manager.list_buckets().unwrap();
        assert_eq!(buckets.len(), 1);
        assert!(buckets.contains(&bucket2.hash()));
    }

    #[test]
    fn test_retain_buckets() {
        let (_temp_dir, manager) = create_manager();

        let entries1 = vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))];
        let entries2 = vec![BucketEntry::Live(make_account_entry([2u8; 32], 200))];
        let entries3 = vec![BucketEntry::Live(make_account_entry([3u8; 32], 300))];

        let bucket1 = manager.create_bucket(entries1).unwrap();
        let bucket2 = manager.create_bucket(entries2).unwrap();
        let _bucket3 = manager.create_bucket(entries3).unwrap();

        // Keep only bucket1 and bucket2
        let deleted = manager
            .retain_buckets(&[bucket1.hash(), bucket2.hash()])
            .unwrap();
        assert_eq!(deleted, 1);

        let buckets = manager.list_buckets().unwrap();
        assert_eq!(buckets.len(), 2);
    }

    #[test]
    fn test_import_export_bucket() {
        let (_temp_dir, manager) = create_manager();

        let entries = vec![
            BucketEntry::Live(make_account_entry([1u8; 32], 100)),
            BucketEntry::Live(make_account_entry([2u8; 32], 200)),
        ];

        let original = manager.create_bucket(entries).unwrap();
        let hash = original.hash();

        // Export
        let xdr_bytes = manager.export_bucket(&hash).unwrap();

        // Import (create new manager)
        let temp_dir2 = TempDir::new().unwrap();
        let manager2 = BucketManager::new(temp_dir2.path().to_path_buf()).unwrap();

        let imported = manager2.import_bucket(&xdr_bytes).unwrap();
        assert_eq!(imported.hash(), hash);
        assert_eq!(imported.len(), 2);
    }

    #[test]
    fn test_empty_bucket() {
        let (_temp_dir, manager) = create_manager();

        let bucket = manager.create_bucket(vec![]).unwrap();
        assert!(bucket.is_empty());
        assert_eq!(bucket.hash(), Hash256::ZERO);

        // Loading empty bucket should work
        let loaded = manager.load_bucket(&Hash256::ZERO).unwrap();
        assert!(loaded.is_empty());
    }

    #[test]
    fn test_stats() {
        let (_temp_dir, manager) = create_manager();

        let entries = vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))];
        manager.create_bucket(entries).unwrap();

        let stats = manager.stats();
        assert_eq!(stats.cached_buckets, 1);
        assert_eq!(stats.disk_buckets, 1);
    }

    #[test]
    fn test_load_complete_ledger_state() {
        let (_temp_dir, manager) = create_manager();

        // Create two buckets - older has entry1, newer has entry2
        let bucket1 = manager
            .create_bucket(vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))])
            .unwrap();
        let bucket2 = manager
            .create_bucket(vec![BucketEntry::Live(make_account_entry([2u8; 32], 200))])
            .unwrap();

        // Load complete state (bucket1 is older, bucket2 is newer)
        let entries = manager
            .load_complete_ledger_state(&[bucket1.hash(), bucket2.hash()])
            .unwrap();

        assert_eq!(entries.len(), 2);
    }

    #[test]
    fn test_load_complete_ledger_state_with_updates() {
        let (_temp_dir, manager) = create_manager();

        // Create older bucket with initial entry
        let bucket1 = manager
            .create_bucket(vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))])
            .unwrap();

        // Create newer bucket with updated entry
        let bucket2 = manager
            .create_bucket(vec![BucketEntry::Live(make_account_entry([1u8; 32], 500))])
            .unwrap();

        // Load complete state - newer entry should win
        let entries = manager
            .load_complete_ledger_state(&[bucket1.hash(), bucket2.hash()])
            .unwrap();

        assert_eq!(entries.len(), 1);
        if let LedgerEntryData::Account(account) = &entries[0].data {
            assert_eq!(account.balance, 500);
        } else {
            panic!("Expected Account entry");
        }
    }

    #[test]
    fn test_load_complete_ledger_state_with_deletes() {
        let (_temp_dir, manager) = create_manager();

        // Create older bucket with entry
        let bucket1 = manager
            .create_bucket(vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))])
            .unwrap();

        // Create newer bucket with deletion
        let bucket2 = manager
            .create_bucket(vec![BucketEntry::Dead(make_account_key([1u8; 32]))])
            .unwrap();

        // Load complete state - entry should be deleted
        let entries = manager
            .load_complete_ledger_state(&[bucket1.hash(), bucket2.hash()])
            .unwrap();

        assert!(entries.is_empty());
    }

    #[test]
    fn test_merge_all_buckets() {
        let (_temp_dir, manager) = create_manager();

        // Create buckets with different entries
        let bucket1 = manager
            .create_bucket(vec![
                BucketEntry::Live(make_account_entry([1u8; 32], 100)),
                BucketEntry::Live(make_account_entry([2u8; 32], 200)),
            ])
            .unwrap();
        let bucket2 = manager
            .create_bucket(vec![
                BucketEntry::Live(make_account_entry([3u8; 32], 300)),
                BucketEntry::Live(make_account_entry([1u8; 32], 150)), // Update entry 1
            ])
            .unwrap();

        // Merge all buckets
        let merged = manager
            .merge_all_buckets(&[bucket1.hash(), bucket2.hash()], 25)
            .unwrap();

        // Should have 3 entries (entry 1 updated, entries 2 and 3)
        assert_eq!(merged.len(), 4); // 1 metadata + 3 entries

        // Check that entry 1 has the updated value
        let key1 = make_account_key([1u8; 32]);
        let entry1 = merged.get_entry(&key1).unwrap().unwrap();
        if let LedgerEntryData::Account(account) = &entry1.data {
            assert_eq!(account.balance, 150);
        } else {
            panic!("Expected Account entry");
        }
    }

    #[test]
    fn test_verify_buckets_exist() {
        let (_temp_dir, manager) = create_manager();

        let bucket = manager
            .create_bucket(vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))])
            .unwrap();

        let nonexistent = Hash256::hash(b"does not exist");

        // Should find the nonexistent bucket as missing
        let missing = manager.verify_buckets_exist(&[bucket.hash(), nonexistent, Hash256::ZERO]);

        assert_eq!(missing.len(), 1);
        assert_eq!(missing[0], nonexistent);
    }

    #[test]
    fn test_verify_bucket_hashes() {
        let (_temp_dir, manager) = create_manager();

        let bucket = manager
            .create_bucket(vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))])
            .unwrap();

        // Verify hash matches
        let mismatches = manager.verify_bucket_hashes(&[bucket.hash()]).unwrap();
        assert!(mismatches.is_empty());
    }

    #[test]
    fn test_ensure_buckets_exist() {
        let (_temp_dir, manager) = create_manager();

        // Create a bucket in another manager
        let temp_dir2 = TempDir::new().unwrap();
        let manager2 = BucketManager::new(temp_dir2.path().to_path_buf()).unwrap();
        let bucket = manager2
            .create_bucket(vec![BucketEntry::Live(make_account_entry([1u8; 32], 100))])
            .unwrap();

        let bucket_xdr = manager2.export_bucket(&bucket.hash()).unwrap();

        // Ensure it exists in manager (needs to fetch)
        let fetched = manager
            .ensure_buckets_exist(&[bucket.hash()], |_hash| Ok(bucket_xdr.clone()))
            .unwrap();

        assert_eq!(fetched, 1);

        // Second call should fetch 0 (already exists)
        let fetched = manager
            .ensure_buckets_exist(&[bucket.hash()], |_hash| panic!("Should not be called"))
            .unwrap();
        assert_eq!(fetched, 0);
    }

    #[test]
    fn test_bucket_manager_persist_index() {
        use crate::index::{DiskIndex, DEFAULT_PAGE_SIZE};
        use crate::index_persistence::index_path_for_bucket;

        let temp_dir = TempDir::new().unwrap();
        let manager =
            BucketManager::with_persist_index(temp_dir.path().to_path_buf(), true).unwrap();
        assert!(manager.persist_index());

        // Create a bucket
        let entries: Vec<BucketEntry> = (0..100u8)
            .map(|i| BucketEntry::Live(make_account_entry([i; 32], i as i64 * 100)))
            .collect();
        let bucket = manager.create_bucket(entries.clone()).unwrap();
        let hash = bucket.hash();
        let bucket_path = manager.bucket_path(&hash);

        // Build and save a disk index
        let indexed_entries: Vec<(crate::entry::BucketEntry, u64)> = entries
            .into_iter()
            .enumerate()
            .map(|(i, e)| (e, i as u64 * 100))
            .collect();
        let bloom_seed = [0u8; 16];
        let index =
            DiskIndex::from_entries(indexed_entries.into_iter(), bloom_seed, DEFAULT_PAGE_SIZE);
        manager.save_index_for_bucket(&hash, &index).unwrap();

        // Verify index file exists
        let index_path = index_path_for_bucket(&bucket_path);
        assert!(index_path.exists(), "Index file should be created");

        // Load the index back
        let loaded = manager
            .try_load_index_for_bucket(&hash, DEFAULT_PAGE_SIZE)
            .unwrap();
        assert!(loaded.is_some(), "Should load persisted index");

        let loaded_idx = loaded.unwrap();
        assert_eq!(loaded_idx.page_size(), DEFAULT_PAGE_SIZE);

        // Delete the bucket — index should also be removed
        manager.delete_bucket(&hash).unwrap();
        assert!(!bucket_path.exists(), "Bucket file should be deleted");
        assert!(!index_path.exists(), "Index file should also be deleted");
    }

    #[test]
    fn test_bucket_manager_no_persist_index() {
        let temp_dir = TempDir::new().unwrap();
        let manager = BucketManager::new(temp_dir.path().to_path_buf()).unwrap();
        assert!(!manager.persist_index());

        // save/load should be no-ops when persist_index is false
        let hash = Hash256::hash(b"dummy");
        let entries: Vec<(crate::entry::BucketEntry, u64)> = vec![];
        let index = crate::index::DiskIndex::from_entries(entries.into_iter(), [0u8; 16], 10);

        manager.save_index_for_bucket(&hash, &index).unwrap();
        let loaded = manager.try_load_index_for_bucket(&hash, 10).unwrap();
        assert!(loaded.is_none());
    }
}
