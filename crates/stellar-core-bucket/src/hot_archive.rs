//! Hot Archive Bucket List for recently evicted Soroban entries.
//!
//! The HotArchiveBucketList stores recently evicted persistent Soroban entries.
//! When persistent entries (ContractData with persistent durability or ContractCode)
//! expire, they are moved from the live BucketList to the HotArchiveBucketList.
//!
//! # Entry Types
//!
//! Hot archive buckets contain `HotArchiveBucketEntry` which has three variants:
//!
//! - `Metaentry`: Metadata about the bucket (protocol version)
//! - `Archived`: A recently archived persistent entry (full LedgerEntry)
//! - `Live`: A restored entry marker (LedgerKey only, indicates the entry was restored)
//!
//! # Merge Semantics
//!
//! Hot archive bucket merging follows these rules:
//!
//! - `Archived + Live` = Annihilate (entry was restored, remove from archive)
//! - `Live + Archived` = Keep `Archived` (re-archived after restoration)
//! - `Archived + Archived` = Keep newer `Archived`
//! - At level 10: `Live` entries are dropped (tombstones not needed)
//!
//! # Protocol Support
//!
//! Hot archive is only supported from Protocol 23+ (Soroban state archival).

use sha2::{Digest, Sha256};
use std::collections::{BTreeMap, HashMap};
use std::io::{BufReader, Read as _, Seek, SeekFrom};
use std::path::{Path, PathBuf};
use stellar_xdr::curr::{
    BucketListType, BucketMetadata, BucketMetadataExt, HotArchiveBucketEntry, LedgerEntry,
    LedgerKey, Limits, ReadXdr, WriteXdr,
};

use stellar_core_common::Hash256;

use crate::bucket_list::{HasNextState, HAS_NEXT_STATE_INPUTS, HAS_NEXT_STATE_OUTPUT};
use crate::entry::ledger_entry_to_key;
use crate::{BucketError, Result};

/// First protocol version supporting hot archive bucket list.
pub const FIRST_PROTOCOL_SUPPORTING_HOT_ARCHIVE: u32 = 23;

/// Number of levels in the HotArchiveBucketList (same as live bucket list).
pub const HOT_ARCHIVE_BUCKET_LIST_LEVELS: usize = 11;

/// Storage backend for a hot archive bucket.
#[derive(Clone, Debug)]
enum HotArchiveStorage {
    /// All entries in memory.
    InMemory {
        /// Entries indexed by key (for lookups).
        entries: BTreeMap<Vec<u8>, HotArchiveBucketEntry>,
        /// Entries in original/sorted order (for hash computation).
        /// This preserves C++ stellar-core's entry order which uses semantic comparison,
        /// not XDR byte comparison.
        ordered_entries: Vec<HotArchiveBucketEntry>,
    },
    /// Entries stored on disk in uncompressed XDR format.
    DiskBacked {
        /// Path to the uncompressed `.bucket.xdr` file.
        path: PathBuf,
        /// Index mapping key XDR bytes â†’ file offset of the record mark.
        index: BTreeMap<Vec<u8>, u64>,
        /// Number of entries in the bucket (including metadata).
        entry_count: usize,
    },
}

/// A hot archive bucket containing archived persistent Soroban entries.
#[derive(Clone, Debug)]
pub struct HotArchiveBucket {
    /// Storage backend.
    storage: HotArchiveStorage,
    /// Hash of the bucket contents.
    hash: Hash256,
}

impl HotArchiveBucket {
    /// Create an empty hot archive bucket.
    pub fn empty() -> Self {
        Self {
            storage: HotArchiveStorage::InMemory {
                entries: BTreeMap::new(),
                ordered_entries: Vec::new(),
            },
            hash: Hash256::from_bytes([0u8; 32]),
        }
    }

    /// Create a hot archive bucket from entries.
    ///
    /// **Important**: The entries MUST be pre-sorted in C++ stellar-core order
    /// (using LedgerEntryIdCmp comparison). The entries are stored in the
    /// provided order for hash computation.
    pub fn from_entries(entries: Vec<HotArchiveBucketEntry>) -> Result<Self> {
        let mut entry_map = BTreeMap::new();

        for entry in &entries {
            let key = hot_archive_entry_to_key(entry)?;
            entry_map.insert(key, entry.clone());
        }

        let mut bucket = Self {
            storage: HotArchiveStorage::InMemory {
                entries: entry_map,
                ordered_entries: entries,
            },
            hash: Hash256::from_bytes([0u8; 32]),
        };

        bucket.hash = bucket.compute_hash()?;
        Ok(bucket)
    }

    /// Create a fresh hot archive bucket from archived and restored entries.
    ///
    /// This is the primary way to create a new hot archive bucket when entries
    /// are evicted from the live bucket list.
    ///
    /// Even when there are no data entries, a metaentry-only bucket is created
    /// (matching C++ behavior where BucketOutputIterator always writes metaentry first).
    pub fn fresh(
        protocol_version: u32,
        archived_entries: Vec<LedgerEntry>,
        restored_keys: Vec<LedgerKey>,
    ) -> Result<Self> {
        // In C++, BucketOutputIterator constructor always writes a metaentry first,
        // so fresh buckets always have at least a metaentry (even with no data entries).
        // mObjectsPut is 1 after writing metaentry, so getBucket() creates a bucket file.
        let mut entries = Vec::with_capacity(1 + archived_entries.len() + restored_keys.len());

        // Add metadata - hot archive buckets always use V1 with BucketListType::HotArchive
        // Metadata is only included when there are actual entries to add
        entries.push(HotArchiveBucketEntry::Metaentry(BucketMetadata {
            ledger_version: protocol_version,
            ext: BucketMetadataExt::V1(BucketListType::HotArchive),
        }));

        // Add archived entries
        for entry in archived_entries {
            entries.push(HotArchiveBucketEntry::Archived(entry));
        }

        // Add restored keys (Live markers)
        for key in restored_keys {
            entries.push(HotArchiveBucketEntry::Live(key));
        }

        // Sort entries using C++ stellar-core's comparison order
        // This matches BucketEntryIdCmp<HotArchiveBucket>
        entries.sort_by(compare_hot_archive_entries);

        let bucket = Self::from_entries(entries)?;

        Ok(bucket)
    }

    /// Get the hash of this bucket.
    pub fn hash(&self) -> Hash256 {
        self.hash
    }

    /// Check if the bucket is empty.
    pub fn is_empty(&self) -> bool {
        match &self.storage {
            HotArchiveStorage::InMemory { entries, .. } => entries.is_empty(),
            HotArchiveStorage::DiskBacked { entry_count, .. } => *entry_count == 0,
        }
    }

    /// Get the number of entries in the bucket.
    pub fn len(&self) -> usize {
        match &self.storage {
            HotArchiveStorage::InMemory { entries, .. } => entries.len(),
            HotArchiveStorage::DiskBacked { entry_count, .. } => *entry_count,
        }
    }

    /// Get the protocol version (bucket version) from the metadata entry.
    ///
    /// This matches C++ stellar-core's `getBucketVersion()` method.
    /// Returns the ledger_version from the bucket's metadata entry, or 0 if no metadata.
    pub fn get_protocol_version(&self) -> u32 {
        match &self.storage {
            HotArchiveStorage::InMemory { entries, .. } => {
                // Metadata entry is stored with empty key
                if let Some(HotArchiveBucketEntry::Metaentry(meta)) = entries.get(&Vec::new()) {
                    return meta.ledger_version;
                }
                0
            }
            HotArchiveStorage::DiskBacked { index, path, .. } => {
                // Try to read metadata entry (stored at offset for empty key)
                if let Some(&offset) = index.get(&Vec::new()) {
                    if let Ok(HotArchiveBucketEntry::Metaentry(meta)) =
                        Self::read_entry_at_offset(path, offset)
                    {
                        return meta.ledger_version;
                    }
                }
                0
            }
        }
    }

    /// Look up an entry by key.
    pub fn get(&self, key: &LedgerKey) -> Result<Option<HotArchiveBucketEntry>> {
        let key_bytes = key.to_xdr(Limits::none()).map_err(|e| {
            BucketError::Serialization(format!("failed to serialize ledger key: {}", e))
        })?;
        match &self.storage {
            HotArchiveStorage::InMemory { entries, .. } => Ok(entries.get(&key_bytes).cloned()),
            HotArchiveStorage::DiskBacked { index, path, .. } => {
                if let Some(&offset) = index.get(&key_bytes) {
                    let entry = Self::read_entry_at_offset(path, offset)?;
                    Ok(Some(entry))
                } else {
                    Ok(None)
                }
            }
        }
    }

    /// Iterate over all entries in sorted order.
    ///
    /// For InMemory buckets, iterates over the BTreeMap values.
    /// For DiskBacked buckets, streams entries from the file sequentially.
    pub fn iter(&self) -> HotArchiveIter<'_> {
        match &self.storage {
            HotArchiveStorage::InMemory { entries, .. } => HotArchiveIter::InMemory {
                inner: entries.values(),
            },
            HotArchiveStorage::DiskBacked { path, .. } => {
                // Open the file for streaming iteration
                match std::fs::File::open(path) {
                    Ok(file) => {
                        let file_len = file.metadata().map(|m| m.len()).unwrap_or(0);
                        HotArchiveIter::DiskBacked {
                            reader: BufReader::new(file),
                            file_len,
                            position: 0,
                        }
                    }
                    Err(_) => {
                        // If file can't be opened, return an empty iterator
                        HotArchiveIter::Empty
                    }
                }
            }
        }
    }

    /// Compute the hash of the bucket contents.
    ///
    /// This must match C++ stellar-core's bucket hashing:
    /// Each entry is written with an XDR record mark (4-byte size prefix with high bit set),
    /// and the hash is computed over the entire serialized content including record marks.
    ///
    /// **Important**: We iterate over `ordered_entries` which preserves the original
    /// entry order from the file or from fresh() sorting. This is critical because
    /// C++ uses semantic comparison (LedgerEntryIdCmp) which differs from XDR byte order.
    fn compute_hash(&self) -> Result<Hash256> {
        match &self.storage {
            HotArchiveStorage::InMemory {
                ordered_entries, ..
            } => {
                if ordered_entries.is_empty() {
                    return Ok(Hash256::from_bytes([0u8; 32]));
                }

                let mut hasher = Sha256::new();
                for entry in ordered_entries {
                    let bytes = entry.to_xdr(Limits::none()).map_err(|e| {
                        BucketError::Serialization(format!("failed to serialize entry: {}", e))
                    })?;

                    // Write XDR record mark: 4-byte size (big-endian) with high bit set
                    let sz = bytes.len() as u32;
                    let record_mark: [u8; 4] = [
                        ((sz >> 24) & 0xFF) as u8 | 0x80,
                        ((sz >> 16) & 0xFF) as u8,
                        ((sz >> 8) & 0xFF) as u8,
                        (sz & 0xFF) as u8,
                    ];
                    hasher.update(record_mark);
                    hasher.update(&bytes);
                }

                let result = hasher.finalize();
                let mut hash_bytes = [0u8; 32];
                hash_bytes.copy_from_slice(&result);
                Ok(Hash256::from_bytes(hash_bytes))
            }
            HotArchiveStorage::DiskBacked { .. } => {
                // For disk-backed, hash was computed during construction
                Ok(self.hash)
            }
        }
    }

    /// Read a single entry from the XDR file at a given offset.
    fn read_entry_at_offset(path: &Path, offset: u64) -> Result<HotArchiveBucketEntry> {
        let mut file = std::fs::File::open(path)?;
        file.seek(SeekFrom::Start(offset))?;

        // Read record mark
        let mut mark_bytes = [0u8; 4];
        file.read_exact(&mut mark_bytes)?;
        let record_mark = u32::from_be_bytes(mark_bytes);
        let record_len = (record_mark & 0x7FFFFFFF) as usize;

        // Read entry data
        let mut data = vec![0u8; record_len];
        file.read_exact(&mut data)?;

        HotArchiveBucketEntry::from_xdr(&data, Limits::none()).map_err(|e| {
            BucketError::Serialization(format!("failed to parse hot archive entry: {}", e))
        })
    }

    /// Load a hot archive bucket from a gzipped XDR file.
    ///
    /// This parses a bucket file containing `HotArchiveBucketEntry` values.
    pub fn load_from_file(path: impl AsRef<std::path::Path>) -> Result<Self> {
        use flate2::read::GzDecoder;
        use std::io::{BufReader, Read};

        let path = path.as_ref();
        let file = std::fs::File::open(path)?;
        let reader = BufReader::new(file);
        let mut decoder = GzDecoder::new(reader);

        // Read and decompress
        let mut uncompressed = Vec::new();
        decoder.read_to_end(&mut uncompressed)?;

        Self::from_xdr_bytes(&uncompressed)
    }

    /// Create a hot archive bucket from uncompressed XDR bytes.
    ///
    /// Parses bucket files using XDR Record Marking Standard (RFC 5531).
    /// Preserves the original file order for hash computation.
    pub fn from_xdr_bytes(bytes: &[u8]) -> Result<Self> {
        if bytes.is_empty() {
            return Ok(Self::empty());
        }

        let mut entries = BTreeMap::new();
        let mut ordered_entries = Vec::new();
        let mut offset = 0;

        // Check if the file uses XDR record marking (high bit set in first 4 bytes)
        let uses_record_marks = if bytes.len() >= 4 {
            bytes[0] & 0x80 != 0
        } else {
            false
        };

        if uses_record_marks {
            // Parse using XDR Record Marking Standard
            while offset + 4 <= bytes.len() {
                // Read 4-byte record mark (big-endian)
                let record_mark = u32::from_be_bytes([
                    bytes[offset],
                    bytes[offset + 1],
                    bytes[offset + 2],
                    bytes[offset + 3],
                ]);
                offset += 4;

                // High bit is "last fragment" flag, remaining 31 bits are length
                let record_len = (record_mark & 0x7FFFFFFF) as usize;

                if offset + record_len > bytes.len() {
                    return Err(BucketError::Serialization(format!(
                        "Record length {} exceeds remaining data {} at offset {}",
                        record_len,
                        bytes.len() - offset,
                        offset - 4
                    )));
                }

                // Parse the XDR record as HotArchiveBucketEntry
                let record_data = &bytes[offset..offset + record_len];
                match HotArchiveBucketEntry::from_xdr(record_data, Limits::none()) {
                    Ok(entry) => {
                        let key = hot_archive_entry_to_key(&entry)?;
                        entries.insert(key, entry.clone());
                        ordered_entries.push(entry);
                    }
                    Err(e) => {
                        return Err(BucketError::Serialization(format!(
                            "Failed to parse hot archive bucket entry: {}",
                            e
                        )));
                    }
                }

                offset += record_len;
            }
        } else {
            // Parse as raw XDR stream (legacy format)
            use stellar_xdr::curr::Limited;
            let cursor = std::io::Cursor::new(bytes);
            let mut limited = Limited::new(cursor, Limits::none());

            while limited.inner.position() < bytes.len() as u64 {
                match HotArchiveBucketEntry::read_xdr(&mut limited) {
                    Ok(entry) => {
                        let key = hot_archive_entry_to_key(&entry)?;
                        entries.insert(key, entry.clone());
                        ordered_entries.push(entry);
                    }
                    Err(_) => {
                        // End of stream or error
                        break;
                    }
                }
            }
        }

        // Compute hash from raw bytes (including record marks)
        let hash = Hash256::hash(bytes);

        Ok(Self {
            storage: HotArchiveStorage::InMemory {
                entries,
                ordered_entries,
            },
            hash,
        })
    }

    /// Load a hot archive bucket from an uncompressed XDR file.
    ///
    /// This reads the bucket file directly without any decompression.
    pub fn load_from_xdr_file(path: impl AsRef<std::path::Path>) -> Result<Self> {
        let path = path.as_ref();
        let bytes = std::fs::read(path)?;
        Self::from_xdr_bytes(&bytes)
    }

    /// Serialize this bucket's entries to uncompressed XDR bytes with record marks.
    pub fn to_xdr_bytes(&self) -> Result<Vec<u8>> {
        match &self.storage {
            HotArchiveStorage::InMemory {
                ordered_entries, ..
            } => {
                let mut bytes = Vec::new();
                for entry in ordered_entries {
                    let entry_bytes = entry.to_xdr(Limits::none()).map_err(|e| {
                        BucketError::Serialization(format!("failed to serialize entry: {}", e))
                    })?;
                    let sz = entry_bytes.len() as u32;
                    let record_mark = sz | 0x80000000;
                    bytes.extend_from_slice(&record_mark.to_be_bytes());
                    bytes.extend_from_slice(&entry_bytes);
                }
                Ok(bytes)
            }
            HotArchiveStorage::DiskBacked { path, .. } => {
                // Read the entire file
                std::fs::read(path).map_err(|e| e.into())
            }
        }
    }

    /// Create a disk-backed hot archive bucket from an existing uncompressed XDR file.
    ///
    /// Streams through the file to build an index without loading all entries into memory.
    /// Hash is computed during the streaming pass.
    pub fn from_xdr_file_disk_backed(path: impl AsRef<Path>) -> Result<Self> {
        let path = path.as_ref().to_path_buf();
        let file = std::fs::File::open(&path)?;
        let file_len = file.metadata()?.len();
        let mut reader = BufReader::new(file);

        let mut index = BTreeMap::new();
        let mut hasher = Sha256::new();
        let mut entry_count = 0;
        let mut position: u64 = 0;

        while position + 4 <= file_len {
            let record_offset = position;

            // Read record mark
            let mut mark_bytes = [0u8; 4];
            reader.read_exact(&mut mark_bytes)?;
            let record_mark = u32::from_be_bytes(mark_bytes);
            let record_len = (record_mark & 0x7FFFFFFF) as usize;

            if position + 4 + record_len as u64 > file_len {
                return Err(BucketError::Serialization(format!(
                    "Record length {} exceeds remaining data at offset {}",
                    record_len, position
                )));
            }

            // Read entry data
            let mut data = vec![0u8; record_len];
            reader.read_exact(&mut data)?;

            // Hash: include record mark + data
            hasher.update(mark_bytes);
            hasher.update(&data);

            // Parse entry to extract key for index
            let entry = HotArchiveBucketEntry::from_xdr(&data, Limits::none()).map_err(|e| {
                BucketError::Serialization(format!(
                    "failed to parse hot archive entry at offset {}: {}",
                    position, e
                ))
            })?;

            let key = hot_archive_entry_to_key(&entry)?;
            index.insert(key, record_offset);
            entry_count += 1;

            position += 4 + record_len as u64;
        }

        let result = hasher.finalize();
        let mut hash_bytes = [0u8; 32];
        hash_bytes.copy_from_slice(&result);
        let hash = if entry_count == 0 {
            Hash256::from_bytes([0u8; 32])
        } else {
            Hash256::from_bytes(hash_bytes)
        };

        Ok(Self {
            storage: HotArchiveStorage::DiskBacked {
                path,
                index,
                entry_count,
            },
            hash,
        })
    }
}

/// Iterator over hot archive bucket entries.
///
/// Supports both in-memory (reference) and disk-backed (streaming) iteration.
/// Returns owned entries in both cases for a uniform interface.
pub enum HotArchiveIter<'a> {
    /// In-memory iteration over BTreeMap values.
    InMemory {
        inner: std::collections::btree_map::Values<'a, Vec<u8>, HotArchiveBucketEntry>,
    },
    /// Disk-backed streaming iteration.
    DiskBacked {
        reader: BufReader<std::fs::File>,
        file_len: u64,
        position: u64,
    },
    /// Empty iterator (used when file can't be opened).
    Empty,
}

impl<'a> Iterator for HotArchiveIter<'a> {
    type Item = HotArchiveBucketEntry;

    fn next(&mut self) -> Option<Self::Item> {
        match self {
            HotArchiveIter::InMemory { inner } => inner.next().cloned(),
            HotArchiveIter::DiskBacked {
                reader,
                file_len,
                position,
            } => {
                if *position + 4 > *file_len {
                    return None;
                }

                // Read record mark
                let mut mark_bytes = [0u8; 4];
                if reader.read_exact(&mut mark_bytes).is_err() {
                    return None;
                }
                let record_mark = u32::from_be_bytes(mark_bytes);
                let record_len = (record_mark & 0x7FFFFFFF) as usize;

                if *position + 4 + record_len as u64 > *file_len {
                    return None;
                }

                // Read entry data
                let mut data = vec![0u8; record_len];
                if reader.read_exact(&mut data).is_err() {
                    return None;
                }

                *position += 4 + record_len as u64;

                // Parse entry
                HotArchiveBucketEntry::from_xdr(&data, Limits::none()).ok()
            }
            HotArchiveIter::Empty => None,
        }
    }
}

impl Default for HotArchiveBucket {
    fn default() -> Self {
        Self::empty()
    }
}

/// A single level in the HotArchiveBucketList.
#[derive(Clone, Debug)]
pub struct HotArchiveBucketLevel {
    /// The current bucket.
    pub curr: HotArchiveBucket,
    /// The snapshot bucket.
    pub snap: HotArchiveBucket,
    /// Staged merge result.
    next: Option<HotArchiveBucket>,
    /// Level number (stored for debugging).
    _level: usize,
}

impl HotArchiveBucketLevel {
    /// Create a new empty level.
    pub fn new(level: usize) -> Self {
        Self {
            curr: HotArchiveBucket::empty(),
            snap: HotArchiveBucket::empty(),
            next: None,
            _level: level,
        }
    }

    /// Get the hash of this level.
    pub fn hash(&self) -> Hash256 {
        let curr_hash = self.curr.hash();
        let snap_hash = self.snap.hash();

        let mut hasher = Sha256::new();
        hasher.update(curr_hash.as_bytes());
        hasher.update(snap_hash.as_bytes());
        let result = hasher.finalize();

        let mut bytes = [0u8; 32];
        bytes.copy_from_slice(&result);
        Hash256::from_bytes(bytes)
    }

    /// Get reference to the staged next bucket.
    pub fn next(&self) -> Option<&HotArchiveBucket> {
        self.next.as_ref()
    }

    /// Commit the staged merge.
    fn commit(&mut self) {
        if let Some(next) = self.next.take() {
            self.curr = next;
        }
    }

    /// Snap curr to snap and return the new snap (matches C++ BucketLevel::snap).
    fn snap(&mut self) -> HotArchiveBucket {
        self.snap = std::mem::take(&mut self.curr);
        self.snap.clone()
    }

    /// Prepare a merge with an incoming bucket.
    ///
    /// - `use_empty_curr`: If true, use an empty bucket instead of self.curr for the merge.
    ///   This is used when the level is about to snap its curr (shouldMergeWithEmptyCurr).
    fn prepare(
        &mut self,
        protocol_version: u32,
        incoming: HotArchiveBucket,
        keep_tombstones: bool,
        use_empty_curr: bool,
    ) -> Result<()> {
        if self.next.is_some() {
            return Err(BucketError::Merge(
                "hot archive bucket merge already in progress".to_string(),
            ));
        }

        // Choose curr or empty based on shouldMergeWithEmptyCurr
        let curr_for_merge = if use_empty_curr {
            HotArchiveBucket::empty()
        } else {
            self.curr.clone()
        };

        let merged = merge_hot_archive_buckets(
            &curr_for_merge,
            &incoming,
            protocol_version,
            keep_tombstones,
        )?;
        self.next = Some(merged);
        Ok(())
    }
}

/// The complete HotArchiveBucketList.
#[derive(Clone)]
pub struct HotArchiveBucketList {
    /// The 11 levels.
    levels: Vec<HotArchiveBucketLevel>,
    /// Current ledger sequence.
    ledger_seq: u32,
}

impl HotArchiveBucketList {
    /// Number of levels in the hot archive bucket list.
    pub const NUM_LEVELS: usize = HOT_ARCHIVE_BUCKET_LIST_LEVELS;

    /// Create a new empty HotArchiveBucketList.
    pub fn new() -> Self {
        let levels = (0..HOT_ARCHIVE_BUCKET_LIST_LEVELS)
            .map(HotArchiveBucketLevel::new)
            .collect();

        Self {
            levels,
            ledger_seq: 0,
        }
    }

    /// Get the hash of the entire bucket list.
    pub fn hash(&self) -> Hash256 {
        let mut hasher = Sha256::new();
        for level in self.levels.iter() {
            let level_hash = level.hash();
            hasher.update(level_hash.as_bytes());
        }

        let result = hasher.finalize();
        let mut bytes = [0u8; 32];
        bytes.copy_from_slice(&result);
        Hash256::from_bytes(bytes)
    }

    /// Get the current ledger sequence.
    pub fn ledger_seq(&self) -> u32 {
        self.ledger_seq
    }

    /// Set the ledger sequence.
    ///
    /// This is used after restoring a bucket list from history archive hashes
    /// to set the correct ledger sequence. The `restore_from_hashes` method
    /// sets ledger_seq to 0, so callers must set it to the actual ledger
    /// sequence to ensure proper bucket list advancement behavior.
    pub fn set_ledger_seq(&mut self, ledger_seq: u32) {
        self.ledger_seq = ledger_seq;
    }

    /// Get a reference to all levels.
    pub fn levels(&self) -> &[HotArchiveBucketLevel] {
        &self.levels
    }

    /// Get the hash of each level along with curr and snap bucket hashes.
    ///
    /// Returns an iterator of (level_index, level_hash, curr_hash, snap_hash).
    /// This is useful for debugging bucket list hash mismatches.
    pub fn level_hashes(&self) -> impl Iterator<Item = (usize, Hash256, Hash256, Hash256)> + '_ {
        self.levels
            .iter()
            .enumerate()
            .map(|(idx, level)| (idx, level.hash(), level.curr.hash(), level.snap.hash()))
    }

    /// Get a reference to a level.
    pub fn level(&self, idx: usize) -> Option<&HotArchiveBucketLevel> {
        self.levels.get(idx)
    }

    /// Get a mutable reference to a level.
    pub fn level_mut(&mut self, idx: usize) -> Option<&mut HotArchiveBucketLevel> {
        self.levels.get_mut(idx)
    }

    /// Add a batch of archived and restored entries.
    ///
    /// This is called when entries are evicted from the live bucket list.
    ///
    /// - `archived_entries`: Persistent entries that have expired and are being archived
    /// - `restored_keys`: Keys of entries that were restored (previously archived)
    pub fn add_batch(
        &mut self,
        ledger_seq: u32,
        protocol_version: u32,
        archived_entries: Vec<LedgerEntry>,
        restored_keys: Vec<LedgerKey>,
    ) -> Result<()> {
        if protocol_version < FIRST_PROTOCOL_SUPPORTING_HOT_ARCHIVE {
            return Err(BucketError::Merge(format!(
                "hot archive not supported before protocol {}",
                FIRST_PROTOCOL_SUPPORTING_HOT_ARCHIVE
            )));
        }

        // In C++, HotArchiveBucket::fresh() always creates a bucket with a metaentry,
        // even when there are no data entries. The metaentry is written in the
        // BucketOutputIterator constructor before any put() calls. So a bucket with
        // no archived/restored entries still has mObjectsPut=1 and returns a bucket
        // with hash 95079eba... (the metaentry-only bucket).
        //
        // We match this behavior by always calling fresh(), which creates a bucket
        // with at least a metaentry.
        let new_bucket =
            HotArchiveBucket::fresh(protocol_version, archived_entries, restored_keys)?;

        self.add_batch_internal(ledger_seq, protocol_version, new_bucket)?;
        self.ledger_seq = ledger_seq;

        Ok(())
    }

    /// Advance the hot archive bucket list from its current ledger to a target ledger
    /// by applying empty batches for all intermediate ledgers.
    ///
    /// This is required because the bucket list merge algorithm depends on being
    /// called for every ledger in sequence.
    ///
    /// # Arguments
    ///
    /// * `target_ledger` - The ledger to advance to (exclusive of actual changes)
    /// * `protocol_version` - Protocol version for empty batches
    ///
    /// # Returns
    ///
    /// Ok(()) if successful, or an error if the target is not greater than current.
    pub fn advance_to_ledger(&mut self, target_ledger: u32, protocol_version: u32) -> Result<()> {
        let current = self.ledger_seq;
        if target_ledger <= current {
            // Nothing to do - we're already at or past this ledger
            return Ok(());
        }

        // Apply empty batches for each intermediate ledger
        for seq in (current + 1)..target_ledger {
            tracing::trace!(
                from_ledger = current,
                to_ledger = target_ledger,
                current_seq = seq,
                "Advancing hot archive bucket list through empty ledger"
            );
            self.add_batch(
                seq,
                protocol_version,
                Vec::new(), // empty archived entries
                Vec::new(), // empty restored keys
            )?;
        }

        Ok(())
    }

    fn add_batch_internal(
        &mut self,
        ledger_seq: u32,
        protocol_version: u32,
        new_bucket: HotArchiveBucket,
    ) -> Result<()> {
        if ledger_seq == 0 {
            return Err(BucketError::Merge(
                "ledger sequence must be > 0".to_string(),
            ));
        }

        // Process spills from highest level down
        for i in (1..HOT_ARCHIVE_BUCKET_LIST_LEVELS).rev() {
            if Self::level_should_spill(ledger_seq, i - 1) {
                let spilling_snap = self.levels[i - 1].snap();
                self.levels[i].commit();

                let keep_tombstones = Self::keep_tombstone_entries(i);
                let use_empty_curr = Self::should_merge_with_empty_curr(ledger_seq, i);
                self.levels[i].prepare(
                    protocol_version,
                    spilling_snap,
                    keep_tombstones,
                    use_empty_curr,
                )?;
            }
        }

        // Add new entries to level 0
        // Level 0 never uses empty curr (shouldMergeWithEmptyCurr returns false for level 0)
        let keep_tombstones_0 = Self::keep_tombstone_entries(0);
        self.levels[0].prepare(protocol_version, new_bucket, keep_tombstones_0, false)?;
        self.levels[0].commit();

        Ok(())
    }

    /// Look up an archived entry by key.
    pub fn get(&self, key: &LedgerKey) -> Result<Option<LedgerEntry>> {
        for level in &self.levels {
            for bucket in [&level.curr, &level.snap] {
                if let Some(entry) = bucket.get(key)? {
                    match entry {
                        HotArchiveBucketEntry::Archived(e) => return Ok(Some(e)),
                        HotArchiveBucketEntry::Live(_) => return Ok(None), // Restored, not in archive
                        HotArchiveBucketEntry::Metaentry(_) => continue,
                    }
                }
            }
        }
        Ok(None)
    }

    /// Check if an entry is in the hot archive.
    pub fn contains(&self, key: &LedgerKey) -> Result<bool> {
        Ok(self.get(key)?.is_some())
    }

    /// Round down helper.
    fn round_down(value: u32, modulus: u32) -> u32 {
        if modulus == 0 {
            return 0;
        }
        value & !(modulus - 1)
    }

    /// Level half size (same formula as live bucket list).
    fn level_half(level: usize) -> u32 {
        1u32 << (2 * level + 1)
    }

    /// Level size (same formula as live bucket list).
    fn level_size(level: usize) -> u32 {
        1u32 << (2 * (level + 1))
    }

    /// Returns true if a level should spill at a given ledger.
    /// This matches C++ stellar-core's `levelShouldSpill`:
    ///   return (ledger == roundDown(ledger, levelHalf(level)) ||
    ///           ledger == roundDown(ledger, levelSize(level)));
    ///
    /// Which simplifies to: ledger is a multiple of levelHalf(level).
    /// For level 0 (half=2): spills at ledgers 0, 2, 4, 6, ...
    /// For level 1 (half=8): spills at ledgers 0, 8, 16, 24, ...
    /// For level 2 (half=32): spills at ledgers 0, 32, 64, 96, ...
    fn level_should_spill(ledger_seq: u32, level: usize) -> bool {
        if level == HOT_ARCHIVE_BUCKET_LIST_LEVELS - 1 {
            // There's no level above the highest level, so it can't spill.
            return false;
        }

        let half = Self::level_half(level);
        let size = Self::level_size(level);
        ledger_seq % half == 0 || ledger_seq % size == 0
    }

    /// Check if tombstone entries should be kept at a level.
    fn keep_tombstone_entries(level: usize) -> bool {
        level < HOT_ARCHIVE_BUCKET_LIST_LEVELS - 1
    }

    /// Determines whether to merge with an empty curr bucket instead of the actual curr.
    /// This happens when the level is about to snap its curr bucket - in that case,
    /// we just propagate the snap from the previous level without merging with curr.
    ///
    /// Matches C++ stellar-core's `shouldMergeWithEmptyCurr`.
    fn should_merge_with_empty_curr(ledger_seq: u32, level: usize) -> bool {
        if level == 0 {
            // Level 0 always merges with its curr
            return false;
        }

        // Round down to when the merge was started
        let merge_start_ledger = Self::round_down(ledger_seq, Self::level_half(level - 1));

        // Calculate when the next spill would happen
        let next_change_ledger = merge_start_ledger + Self::level_half(level - 1);

        // If the next spill would affect this level, use empty curr
        // because curr is about to be snapped
        Self::level_should_spill(next_change_ledger, level)
    }

    /// Get statistics about the bucket list.
    pub fn stats(&self) -> HotArchiveBucketListStats {
        let mut total_entries = 0;
        let mut total_buckets = 0;

        for level in &self.levels {
            if !level.curr.is_empty() {
                total_entries += level.curr.len();
                total_buckets += 1;
            }
            if !level.snap.is_empty() {
                total_entries += level.snap.len();
                total_buckets += 1;
            }
        }

        HotArchiveBucketListStats {
            num_levels: HOT_ARCHIVE_BUCKET_LIST_LEVELS,
            total_entries,
            total_buckets,
        }
    }

    /// Debug print level state.
    pub fn debug_print_levels(&self) {
        for (i, level) in self.levels.iter().enumerate() {
            let curr_hash = level.curr.hash();
            let snap_hash = level.snap.hash();
            let next_hash = level
                .next
                .as_ref()
                .map(|b| b.hash().to_hex())
                .unwrap_or_else(|| "None".to_string());
            eprintln!(
                "  HA_L{}: curr={}, snap={}, next={}",
                i,
                curr_hash.to_hex(),
                snap_hash.to_hex(),
                next_hash
            );
        }
    }

    /// Restore a hot archive bucket list from bucket hashes.
    ///
    /// This is used to restore state from a history archive checkpoint.
    ///
    /// # Arguments
    ///
    /// * `hashes` - The bucket hashes (curr and snap for each level, 22 total)
    /// * `load_bucket` - Function to load a HotArchiveBucket by hash
    pub fn restore_from_hashes<F>(hashes: &[Hash256], mut load_bucket: F) -> Result<Self>
    where
        F: FnMut(&Hash256) -> Result<HotArchiveBucket>,
    {
        if hashes.len() != HOT_ARCHIVE_BUCKET_LIST_LEVELS * 2 {
            return Err(BucketError::Serialization(format!(
                "Expected {} hot archive bucket hashes, got {}",
                HOT_ARCHIVE_BUCKET_LIST_LEVELS * 2,
                hashes.len()
            )));
        }

        let mut levels = Vec::with_capacity(HOT_ARCHIVE_BUCKET_LIST_LEVELS);

        for (i, chunk) in hashes.chunks(2).enumerate() {
            let curr_hash = &chunk[0];
            let snap_hash = &chunk[1];

            let curr = if curr_hash.is_zero() {
                HotArchiveBucket::empty()
            } else {
                load_bucket(curr_hash)?
            };

            let snap = if snap_hash.is_zero() {
                HotArchiveBucket::empty()
            } else {
                load_bucket(snap_hash)?
            };

            let mut level = HotArchiveBucketLevel::new(i);
            level.curr = curr;
            level.snap = snap;
            levels.push(level);
        }

        Ok(Self {
            levels,
            ledger_seq: 0,
        })
    }

    /// Restore a hot archive bucket list from History Archive State with full FutureBucket support.
    ///
    /// Unlike `restore_from_hashes`, this function also restores pending merge results
    /// when the HAS indicates a completed merge (state == HAS_NEXT_STATE_OUTPUT). This is
    /// necessary for correct bucket list hash computation at checkpoints.
    ///
    /// # Arguments
    ///
    /// * `hashes` - Vec of (curr_hash, snap_hash) pairs for each level
    /// * `next_states` - Vec of HasNextState for each level
    /// * `load_bucket` - Function to load a HotArchiveBucket by hash
    pub fn restore_from_has<F>(
        hashes: &[(Hash256, Hash256)],
        next_states: &[HasNextState],
        mut load_bucket: F,
    ) -> Result<Self>
    where
        F: FnMut(&Hash256) -> Result<HotArchiveBucket>,
    {
        if hashes.len() != HOT_ARCHIVE_BUCKET_LIST_LEVELS {
            return Err(BucketError::Serialization(format!(
                "Expected {} hot archive bucket level hashes, got {}",
                HOT_ARCHIVE_BUCKET_LIST_LEVELS,
                hashes.len()
            )));
        }

        let mut levels = Vec::with_capacity(HOT_ARCHIVE_BUCKET_LIST_LEVELS);

        for (i, (curr_hash, snap_hash)) in hashes.iter().enumerate() {
            let curr = if curr_hash.is_zero() {
                HotArchiveBucket::empty()
            } else {
                load_bucket(curr_hash)?
            };

            let snap = if snap_hash.is_zero() {
                HotArchiveBucket::empty()
            } else {
                load_bucket(snap_hash)?
            };

            // Check if there's a completed merge (state == HAS_NEXT_STATE_OUTPUT) for this level
            let next = if let Some(state) = next_states.get(i) {
                if state.state == HAS_NEXT_STATE_OUTPUT {
                    if let Some(ref output_hash) = state.output {
                        if !output_hash.is_zero() {
                            tracing::debug!(
                                level = i,
                                output_hash = %output_hash.to_hex(),
                                "hot_archive restore_from_has: loading completed merge output"
                            );
                            Some(load_bucket(output_hash)?)
                        } else {
                            None
                        }
                    } else {
                        None
                    }
                } else {
                    None
                }
            } else {
                None
            };

            let mut level = HotArchiveBucketLevel::new(i);
            level.curr = curr;
            level.snap = snap;
            level.next = next;
            levels.push(level);
        }

        Ok(Self {
            levels,
            ledger_seq: 0,
        })
    }

    /// Restart any pending merges after restoring from a History Archive State (HAS),
    /// using the stored input hashes from the HAS.
    ///
    /// This handles state 2 (HAS_NEXT_STATE_INPUTS) by restarting merges with the
    /// exact input curr and snap hashes stored in the HAS.
    ///
    /// This matches C++ stellar-core's logic in restartMerges() for handling
    /// FutureBuckets with hasHashes() && !isLive().
    pub fn restart_merges_from_has<F>(
        &mut self,
        ledger: u32,
        protocol_version: u32,
        next_states: &[HasNextState],
        mut load_bucket: F,
    ) -> Result<()>
    where
        F: FnMut(&Hash256) -> Result<HotArchiveBucket>,
    {
        tracing::debug!(
            ledger = ledger,
            "hot_archive restart_merges_from_has: restarting merges using HAS input hashes"
        );

        for i in 1..HOT_ARCHIVE_BUCKET_LIST_LEVELS {
            // Skip if there's already a pending merge (from state 1 output)
            if self.levels[i].next.is_some() {
                tracing::trace!(
                    level = i,
                    "hot_archive restart_merges_from_has: level already has pending merge"
                );
                continue;
            }

            // Check if HAS has stored input hashes for this level (state 2)
            if let Some(state) = next_states.get(i) {
                if state.state == HAS_NEXT_STATE_INPUTS {
                    if let (Some(ref curr_hash), Some(ref snap_hash)) =
                        (&state.input_curr, &state.input_snap)
                    {
                        // Load the input buckets from the stored hashes
                        let input_curr = if curr_hash.is_zero() {
                            HotArchiveBucket::empty()
                        } else {
                            load_bucket(curr_hash)?
                        };

                        let input_snap = if snap_hash.is_zero() {
                            HotArchiveBucket::empty()
                        } else {
                            load_bucket(snap_hash)?
                        };

                        tracing::info!(
                            level = i,
                            ledger = ledger,
                            input_curr_hash = %curr_hash.to_hex(),
                            input_snap_hash = %snap_hash.to_hex(),
                            "hot_archive restart_merges_from_has: restarting merge with HAS input hashes"
                        );

                        // Perform the merge with the exact input hashes from HAS
                        // Use the caller's protocol_version (from ledger header) as the
                        // max protocol version, matching C++ behavior in restartMerges
                        // where makeLive() is called with maxProtocolVersion.
                        let keep_tombstones = Self::keep_tombstone_entries(i);

                        let merged = merge_hot_archive_buckets(
                            &input_curr,
                            &input_snap,
                            protocol_version, // Use caller's protocol version, not bucket's
                            keep_tombstones,
                        )?;

                        tracing::info!(
                            level = i,
                            merged_hash = %merged.hash().to_hex(),
                            "hot_archive restart_merges_from_has: merge completed"
                        );

                        self.levels[i].next = Some(merged);
                        continue;
                    }
                }
            }
        }

        // For levels that don't have HAS input hashes, fall back to regular restart_merges
        self.restart_merges(ledger, protocol_version)
    }

    /// Restart any pending merges after restoring from a History Archive State (HAS).
    ///
    /// When a hot archive bucket list is restored from HAS, there may be merges that should
    /// have been in progress at that checkpoint ledger. This function recreates those pending
    /// merges by examining the current and snap buckets and starting merges where appropriate.
    ///
    /// This matches C++ stellar-core's BucketListBase::restartMerges() for hot archive.
    pub fn restart_merges(&mut self, ledger: u32, protocol_version: u32) -> Result<()> {
        tracing::debug!(
            ledger = ledger,
            "hot_archive restart_merges: restarting pending merges after HAS restore"
        );

        for i in 1..HOT_ARCHIVE_BUCKET_LIST_LEVELS {
            // Skip if there's already a pending merge
            if self.levels[i].next.is_some() {
                tracing::trace!(
                    level = i,
                    "hot_archive restart_merges: level already has pending merge"
                );
                continue;
            }

            // Clone the previous level's snap to avoid borrow conflicts
            let prev_snap = self.levels[i - 1].snap.clone();

            // If the previous level's snap is empty, this and all higher levels
            // are uninitialized (haven't received enough data yet)
            if prev_snap.is_empty() {
                tracing::debug!(
                    level = i,
                    "hot_archive restart_merges: previous level snap is empty, stopping"
                );
                break;
            }

            // Calculate the ledger when this merge would have started
            let merge_start_ledger = Self::round_down(ledger, Self::level_half(i - 1));

            tracing::debug!(
                level = i,
                merge_start_ledger = merge_start_ledger,
                prev_snap_hash = %prev_snap.hash(),
                "hot_archive restart_merges: restarting merge"
            );

            // Determine merge parameters
            let merge_protocol_version = match prev_snap.get_protocol_version() {
                0 => protocol_version,
                version => version,
            };
            let keep_tombstones = Self::keep_tombstone_entries(i);
            let use_empty_curr = Self::should_merge_with_empty_curr(merge_start_ledger, i);

            // Start the merge with the previous level's snap
            self.levels[i].prepare(
                merge_protocol_version,
                prev_snap,
                keep_tombstones,
                use_empty_curr,
            )?;

            tracing::debug!(
                level = i,
                "hot_archive restart_merges: merge restarted successfully"
            );
        }

        // Update the ledger sequence to the restored ledger
        self.ledger_seq = ledger;

        Ok(())
    }
}

impl Default for HotArchiveBucketList {
    fn default() -> Self {
        Self::new()
    }
}

impl std::fmt::Debug for HotArchiveBucketList {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("HotArchiveBucketList")
            .field("ledger_seq", &self.ledger_seq)
            .field("hash", &self.hash().to_hex())
            .field("stats", &self.stats())
            .finish()
    }
}

/// Statistics about a HotArchiveBucketList.
#[derive(Debug, Clone)]
pub struct HotArchiveBucketListStats {
    /// Number of levels.
    pub num_levels: usize,
    /// Total number of entries.
    pub total_entries: usize,
    /// Total number of non-empty buckets.
    pub total_buckets: usize,
}

/// Extract key bytes from a hot archive bucket entry.
fn hot_archive_entry_to_key(entry: &HotArchiveBucketEntry) -> Result<Vec<u8>> {
    match entry {
        HotArchiveBucketEntry::Archived(e) => {
            let key = ledger_entry_to_key(e).ok_or_else(|| {
                BucketError::Serialization("failed to extract key from entry".to_string())
            })?;
            key.to_xdr(Limits::none())
                .map_err(|e| BucketError::Serialization(format!("failed to serialize key: {}", e)))
        }
        HotArchiveBucketEntry::Live(key) => key
            .to_xdr(Limits::none())
            .map_err(|e| BucketError::Serialization(format!("failed to serialize key: {}", e))),
        HotArchiveBucketEntry::Metaentry(_) => {
            // Metadata uses a special key (empty)
            Ok(Vec::new())
        }
    }
}

/// Compare two hot archive bucket entries for sorting, matching C++ stellar-core's
/// `BucketEntryIdCmp<HotArchiveBucket>`.
///
/// Comparison order:
/// 1. Metaentry always comes first
/// 2. For other entries, compare by LedgerKey using the same order as C++
pub fn compare_hot_archive_entries(
    a: &HotArchiveBucketEntry,
    b: &HotArchiveBucketEntry,
) -> std::cmp::Ordering {
    use std::cmp::Ordering;

    // Metaentry always sorts first
    match (a, b) {
        (HotArchiveBucketEntry::Metaentry(_), HotArchiveBucketEntry::Metaentry(_)) => {
            return Ordering::Equal;
        }
        (HotArchiveBucketEntry::Metaentry(_), _) => {
            return Ordering::Less;
        }
        (_, HotArchiveBucketEntry::Metaentry(_)) => {
            return Ordering::Greater;
        }
        _ => {}
    }

    // Get LedgerKey from each entry
    let key_a = match a {
        HotArchiveBucketEntry::Archived(e) => ledger_entry_to_key(e),
        HotArchiveBucketEntry::Live(k) => Some(k.clone()),
        HotArchiveBucketEntry::Metaentry(_) => unreachable!(),
    };

    let key_b = match b {
        HotArchiveBucketEntry::Archived(e) => ledger_entry_to_key(e),
        HotArchiveBucketEntry::Live(k) => Some(k.clone()),
        HotArchiveBucketEntry::Metaentry(_) => unreachable!(),
    };

    match (key_a, key_b) {
        (Some(ka), Some(kb)) => compare_ledger_keys(&ka, &kb),
        (Some(_), None) => Ordering::Less,
        (None, Some(_)) => Ordering::Greater,
        (None, None) => Ordering::Equal,
    }
}

/// Compare two LedgerKeys using the same order as C++ stellar-core's `LedgerEntryIdCmp`.
///
/// This matches the comparison order used in bucket files.
fn compare_ledger_keys(a: &LedgerKey, b: &LedgerKey) -> std::cmp::Ordering {
    use std::cmp::Ordering;
    use stellar_xdr::curr::LedgerKey::*;

    // Compare by type first
    let type_a = ledger_key_type_discriminant(a);
    let type_b = ledger_key_type_discriminant(b);
    if type_a != type_b {
        return type_a.cmp(&type_b);
    }

    // Same type, compare by type-specific fields
    match (a, b) {
        (Account(a), Account(b)) => a.account_id.cmp(&b.account_id),
        (Trustline(a), Trustline(b)) => match a.account_id.cmp(&b.account_id) {
            Ordering::Equal => compare_trust_line_asset(&a.asset, &b.asset),
            other => other,
        },
        (Offer(a), Offer(b)) => match a.seller_id.cmp(&b.seller_id) {
            Ordering::Equal => a.offer_id.cmp(&b.offer_id),
            other => other,
        },
        (Data(a), Data(b)) => match a.account_id.cmp(&b.account_id) {
            Ordering::Equal => a.data_name.as_slice().cmp(b.data_name.as_slice()),
            other => other,
        },
        (ClaimableBalance(a), ClaimableBalance(b)) => {
            compare_claimable_balance_id(&a.balance_id, &b.balance_id)
        }
        (LiquidityPool(a), LiquidityPool(b)) => a.liquidity_pool_id.0.cmp(&b.liquidity_pool_id.0),
        (ContractData(a), ContractData(b)) => {
            let addr_cmp = compare_sc_address(&a.contract, &b.contract);
            if addr_cmp != Ordering::Equal {
                return addr_cmp;
            }
            let key_cmp = compare_sc_val(&a.key, &b.key);
            if key_cmp != Ordering::Equal {
                return key_cmp;
            }
            (a.durability as i32).cmp(&(b.durability as i32))
        }
        (ContractCode(a), ContractCode(b)) => a.hash.0.cmp(&b.hash.0),
        (ConfigSetting(a), ConfigSetting(b)) => {
            (a.config_setting_id as i32).cmp(&(b.config_setting_id as i32))
        }
        (Ttl(a), Ttl(b)) => a.key_hash.0.cmp(&b.key_hash.0),
        _ => Ordering::Equal, // Different types should not reach here
    }
}

fn ledger_key_type_discriminant(k: &LedgerKey) -> i32 {
    use stellar_xdr::curr::LedgerKey::*;
    match k {
        Account(_) => 0,
        Trustline(_) => 1,
        Offer(_) => 2,
        Data(_) => 3,
        ClaimableBalance(_) => 4,
        LiquidityPool(_) => 5,
        ContractData(_) => 6,
        ContractCode(_) => 7,
        ConfigSetting(_) => 8,
        Ttl(_) => 9,
    }
}

fn compare_trust_line_asset(
    a: &stellar_xdr::curr::TrustLineAsset,
    b: &stellar_xdr::curr::TrustLineAsset,
) -> std::cmp::Ordering {
    use std::cmp::Ordering;
    use stellar_xdr::curr::TrustLineAsset::*;

    let type_a = match a {
        Native => 0,
        CreditAlphanum4(_) => 1,
        CreditAlphanum12(_) => 2,
        PoolShare(_) => 3,
    };
    let type_b = match b {
        Native => 0,
        CreditAlphanum4(_) => 1,
        CreditAlphanum12(_) => 2,
        PoolShare(_) => 3,
    };

    if type_a != type_b {
        return type_a.cmp(&type_b);
    }

    match (a, b) {
        (Native, Native) => Ordering::Equal,
        (CreditAlphanum4(a), CreditAlphanum4(b)) => match a.asset_code.cmp(&b.asset_code) {
            Ordering::Equal => a.issuer.cmp(&b.issuer),
            other => other,
        },
        (CreditAlphanum12(a), CreditAlphanum12(b)) => match a.asset_code.cmp(&b.asset_code) {
            Ordering::Equal => a.issuer.cmp(&b.issuer),
            other => other,
        },
        (PoolShare(a), PoolShare(b)) => a.0.cmp(&b.0),
        _ => Ordering::Equal,
    }
}

fn compare_claimable_balance_id(
    a: &stellar_xdr::curr::ClaimableBalanceId,
    b: &stellar_xdr::curr::ClaimableBalanceId,
) -> std::cmp::Ordering {
    use stellar_xdr::curr::ClaimableBalanceId::*;
    match (a, b) {
        (ClaimableBalanceIdTypeV0(a), ClaimableBalanceIdTypeV0(b)) => a.0.cmp(&b.0),
    }
}

fn compare_sc_address(
    a: &stellar_xdr::curr::ScAddress,
    b: &stellar_xdr::curr::ScAddress,
) -> std::cmp::Ordering {
    // Compare by type discriminant first, then by content
    // Use XDR byte comparison for simplicity
    let a_bytes = a.to_xdr(Limits::none()).unwrap_or_default();
    let b_bytes = b.to_xdr(Limits::none()).unwrap_or_default();
    a_bytes.cmp(&b_bytes)
}

/// Compare two ScVal values using the same order as C++ stellar-core.
///
/// This uses XDR byte comparison as a fallback for complex types,
/// which should be correct for most practical cases.
fn compare_sc_val(
    a: &stellar_xdr::curr::ScVal,
    b: &stellar_xdr::curr::ScVal,
) -> std::cmp::Ordering {
    use std::cmp::Ordering;
    use stellar_xdr::curr::ScVal::*;

    // Compare by type discriminant first
    let type_a = sc_val_type_discriminant(a);
    let type_b = sc_val_type_discriminant(b);
    if type_a != type_b {
        return type_a.cmp(&type_b);
    }

    // Same type, compare by value
    match (a, b) {
        (Bool(a), Bool(b)) => a.cmp(b),
        (Void, Void) => Ordering::Equal,
        (Error(a), Error(b)) => {
            // Compare by XDR bytes
            let a_bytes = a.to_xdr(Limits::none()).unwrap_or_default();
            let b_bytes = b.to_xdr(Limits::none()).unwrap_or_default();
            a_bytes.cmp(&b_bytes)
        }
        (U32(a), U32(b)) => a.cmp(b),
        (I32(a), I32(b)) => a.cmp(b),
        (U64(a), U64(b)) => a.cmp(b),
        (I64(a), I64(b)) => a.cmp(b),
        (Timepoint(a), Timepoint(b)) => a.cmp(b),
        (Duration(a), Duration(b)) => a.cmp(b),
        (U128(a), U128(b)) => match a.hi.cmp(&b.hi) {
            Ordering::Equal => a.lo.cmp(&b.lo),
            other => other,
        },
        (I128(a), I128(b)) => match a.hi.cmp(&b.hi) {
            Ordering::Equal => a.lo.cmp(&b.lo),
            other => other,
        },
        (U256(a), U256(b)) => {
            for (a_part, b_part) in [
                (a.hi_hi, b.hi_hi),
                (a.hi_lo, b.hi_lo),
                (a.lo_hi, b.lo_hi),
                (a.lo_lo, b.lo_lo),
            ] {
                match a_part.cmp(&b_part) {
                    Ordering::Equal => continue,
                    other => return other,
                }
            }
            Ordering::Equal
        }
        (I256(a), I256(b)) => {
            // I256 has mixed types for hi/lo parts, use XDR bytes
            let a_bytes = a.to_xdr(Limits::none()).unwrap_or_default();
            let b_bytes = b.to_xdr(Limits::none()).unwrap_or_default();
            a_bytes.cmp(&b_bytes)
        }
        (Bytes(a), Bytes(b)) => a.as_slice().cmp(b.as_slice()),
        (String(a), String(b)) => a.as_slice().cmp(b.as_slice()),
        (Symbol(a), Symbol(b)) => a.as_slice().cmp(b.as_slice()),
        (Vec(a_opt), Vec(b_opt)) => match (a_opt, b_opt) {
            (Some(a), Some(b)) => {
                for (a_elem, b_elem) in a.iter().zip(b.iter()) {
                    match compare_sc_val(a_elem, b_elem) {
                        Ordering::Equal => continue,
                        other => return other,
                    }
                }
                a.len().cmp(&b.len())
            }
            (Some(_), None) => Ordering::Greater,
            (None, Some(_)) => Ordering::Less,
            (None, None) => Ordering::Equal,
        },
        (Map(a_opt), Map(b_opt)) => match (a_opt, b_opt) {
            (Some(a), Some(b)) => {
                for (a_entry, b_entry) in a.iter().zip(b.iter()) {
                    match compare_sc_val(&a_entry.key, &b_entry.key) {
                        Ordering::Equal => match compare_sc_val(&a_entry.val, &b_entry.val) {
                            Ordering::Equal => continue,
                            other => return other,
                        },
                        other => return other,
                    }
                }
                a.len().cmp(&b.len())
            }
            (Some(_), None) => Ordering::Greater,
            (None, Some(_)) => Ordering::Less,
            (None, None) => Ordering::Equal,
        },
        (Address(a), Address(b)) => compare_sc_address(a, b),
        (LedgerKeyContractInstance, LedgerKeyContractInstance) => Ordering::Equal,
        (LedgerKeyNonce(a), LedgerKeyNonce(b)) => a.nonce.cmp(&b.nonce),
        (ContractInstance(a), ContractInstance(b)) => {
            // Compare by XDR bytes as fallback
            let a_bytes = a.to_xdr(Limits::none()).unwrap_or_default();
            let b_bytes = b.to_xdr(Limits::none()).unwrap_or_default();
            a_bytes.cmp(&b_bytes)
        }
        // For any remaining cases, use XDR byte comparison
        _ => {
            let a_bytes = a.to_xdr(Limits::none()).unwrap_or_default();
            let b_bytes = b.to_xdr(Limits::none()).unwrap_or_default();
            a_bytes.cmp(&b_bytes)
        }
    }
}

fn sc_val_type_discriminant(v: &stellar_xdr::curr::ScVal) -> i32 {
    use stellar_xdr::curr::ScVal::*;
    // Values must match XDR ScValType enum discriminants
    match v {
        Bool(_) => 0,
        Void => 1,
        Error(_) => 2,
        U32(_) => 3,
        I32(_) => 4,
        U64(_) => 5,
        I64(_) => 6,
        Timepoint(_) => 7,
        Duration(_) => 8,
        U128(_) => 9,
        I128(_) => 10,
        U256(_) => 11,
        I256(_) => 12,
        Bytes(_) => 13,
        String(_) => 14,
        Symbol(_) => 15,
        Vec(_) => 16,
        Map(_) => 17,
        Address(_) => 18,
        ContractInstance(_) => 19,
        LedgerKeyContractInstance => 20,
        LedgerKeyNonce(_) => 21,
    }
}

/// Check if a hot archive entry is a tombstone (Live marker).
pub fn is_hot_archive_tombstone(entry: &HotArchiveBucketEntry) -> bool {
    matches!(entry, HotArchiveBucketEntry::Live(_))
}

/// Merge two hot archive buckets.
///
/// Hot archive merge rules:
/// - Archived + Live = Annihilate (entry was restored)
/// - Live + Archived = Keep Archived (re-archived)
/// - Archived + Archived = Keep newer (from curr)
/// - At bottom level: Live entries are dropped
pub fn merge_hot_archive_buckets(
    curr: &HotArchiveBucket,
    snap: &HotArchiveBucket,
    protocol_version: u32,
    keep_tombstones: bool,
) -> Result<HotArchiveBucket> {
    tracing::trace!(
        curr_hash = %curr.hash().to_hex(),
        curr_len = curr.len(),
        snap_hash = %snap.hash().to_hex(),
        snap_len = snap.len(),
        protocol_version = protocol_version,
        keep_tombstones = keep_tombstones,
        "hot_archive merge: starting merge"
    );

    // NOTE: We intentionally do NOT optimize for empty buckets here.
    // C++ stellar-core always goes through the full merge process even when
    // one input is empty. This is important because:
    // 1. The output bucket gets new metadata (protocol version)
    // 2. The bucket hash includes metadata
    // 3. Returning input unchanged would preserve old metadata and wrong hash
    //
    // The only optimization is when BOTH inputs are empty and protocol version is 0.
    // In C++, even an empty merge produces a bucket with metadata.
    if snap.is_empty() && curr.is_empty() && protocol_version == 0 {
        tracing::trace!("hot_archive merge: both empty and no protocol, returning empty");
        return Ok(HotArchiveBucket::empty());
    }

    let mut merged_entries: HashMap<Vec<u8>, HotArchiveBucketEntry> = HashMap::new();

    // Process curr entries first (older entries)
    for entry in curr.iter() {
        if matches!(entry, HotArchiveBucketEntry::Metaentry(_)) {
            continue;
        }

        let key = hot_archive_entry_to_key(&entry)?;
        merged_entries.insert(key, entry);
    }

    // Process snap entries (newer entries)
    // C++ HotArchiveBucket::mergeCasesWithEqualKeys always takes the newer entry,
    // so snap entries always win when there's a key match.
    for entry in snap.iter() {
        if matches!(entry, HotArchiveBucketEntry::Metaentry(_)) {
            continue;
        }

        let key = hot_archive_entry_to_key(&entry)?;
        // Always insert snap entry - it will either add a new entry
        // or replace an existing curr entry (newer always wins)
        merged_entries.insert(key, entry);
    }

    // Drop tombstones at bottom level
    if !keep_tombstones {
        merged_entries.retain(|_, v| !is_hot_archive_tombstone(v));
    }

    // Build result
    // NOTE: Even if merged_entries is empty, we still create a bucket with metaentry.
    // In C++, BucketOutputIterator constructor ALWAYS writes a metaentry first,
    // so merge output always has at least a metaentry (mObjectsPut >= 1).
    // This is critical for hash consistency.
    let mut result_entries = Vec::with_capacity(merged_entries.len() + 1);

    // Calculate output protocol version using max(curr, snap), matching C++ behavior
    // in calculateMergeProtocolVersion(). The passed protocol_version is only used as
    // a constraint (maxProtocolVersion), not as the output version.
    let output_version = curr.get_protocol_version().max(snap.get_protocol_version());

    // Validate that calculated version doesn't exceed the max
    if protocol_version > 0 && output_version > protocol_version {
        return Err(BucketError::Merge(format!(
            "hot archive bucket protocol version {} exceeds max {}",
            output_version, protocol_version
        )));
    }

    // Add metadata - hot archive buckets always use V1 with BucketListType::HotArchive
    // for Protocol 23+.
    let mut meta = BucketMetadata {
        ledger_version: output_version,
        ext: BucketMetadataExt::V0,
    };
    if output_version >= FIRST_PROTOCOL_SUPPORTING_HOT_ARCHIVE {
        meta.ext = BucketMetadataExt::V1(BucketListType::HotArchive);
    }
    result_entries.push(HotArchiveBucketEntry::Metaentry(meta));

    // Add merged entries
    result_entries.extend(merged_entries.into_values());

    // Sort entries using C++ stellar-core's comparison order
    // This is critical for hash consistency with C++
    result_entries.sort_by(compare_hot_archive_entries);

    let result = HotArchiveBucket::from_entries(result_entries)?;
    tracing::trace!(
        result_hash = %result.hash().to_hex(),
        result_len = result.len(),
        "hot_archive merge: merge complete"
    );
    Ok(result)
}

#[cfg(test)]
mod tests {
    use super::*;
    use stellar_xdr::curr::*;

    fn make_contract_data_key(contract_id: [u8; 32], key_bytes: &[u8]) -> LedgerKey {
        LedgerKey::ContractData(LedgerKeyContractData {
            contract: ScAddress::Contract(Hash(contract_id).into()),
            key: ScVal::Bytes(key_bytes.to_vec().try_into().unwrap()),
            durability: ContractDataDurability::Persistent,
        })
    }

    fn make_contract_data_entry(
        contract_id: [u8; 32],
        key_bytes: &[u8],
        value: i64,
    ) -> LedgerEntry {
        LedgerEntry {
            last_modified_ledger_seq: 1,
            data: LedgerEntryData::ContractData(ContractDataEntry {
                ext: ExtensionPoint::V0,
                contract: ScAddress::Contract(Hash(contract_id).into()),
                key: ScVal::Bytes(key_bytes.to_vec().try_into().unwrap()),
                durability: ContractDataDurability::Persistent,
                val: ScVal::I64(value),
            }),
            ext: LedgerEntryExt::V0,
        }
    }

    #[test]
    fn test_hot_archive_bucket_empty() {
        let bucket = HotArchiveBucket::empty();
        assert!(bucket.is_empty());
        assert_eq!(bucket.len(), 0);
        assert!(bucket.hash().is_zero());
    }

    #[test]
    fn test_hot_archive_bucket_fresh() {
        let entry = make_contract_data_entry([1u8; 32], b"key1", 100);
        let bucket = HotArchiveBucket::fresh(25, vec![entry], vec![]).unwrap();

        assert!(!bucket.is_empty());
        assert_eq!(bucket.len(), 2); // metadata + 1 entry
        assert!(!bucket.hash().is_zero());
    }

    #[test]
    fn test_hot_archive_bucket_fresh_metaentry_only() {
        // Fresh bucket with no data entries should still have metaentry
        // This matches C++ behavior where BucketOutputIterator always writes metaentry first
        let bucket = HotArchiveBucket::fresh(25, vec![], vec![]).unwrap();

        // Should have 1 entry (just the metaentry)
        assert_eq!(bucket.len(), 1);
        // Should NOT be considered empty (has metaentry)
        assert!(!bucket.is_empty());
        // Should have non-zero hash
        assert!(!bucket.hash().is_zero());
        // Should have correct protocol version
        assert_eq!(bucket.get_protocol_version(), 25);
    }

    #[test]
    fn test_hot_archive_merge_metaentry_only_buckets() {
        // Merging two metaentry-only buckets should produce a metaentry-only bucket
        // This is critical for hash consistency with C++
        let bucket1 = HotArchiveBucket::fresh(25, vec![], vec![]).unwrap();
        let bucket2 = HotArchiveBucket::fresh(25, vec![], vec![]).unwrap();

        let merged = merge_hot_archive_buckets(&bucket1, &bucket2, 25, true).unwrap();

        // Should have 1 entry (just the metaentry)
        assert_eq!(merged.len(), 1);
        // Should NOT be considered empty
        assert!(!merged.is_empty());
        // Should have non-zero hash
        assert!(!merged.hash().is_zero());
        // Should have correct protocol version
        assert_eq!(merged.get_protocol_version(), 25);
    }

    #[test]
    fn test_hot_archive_metaentry_only_hash_matches_cpp() {
        // This test verifies the hash of a metaentry-only bucket matches C++ stellar-core
        // Bucket 95079eba2ff8ef53c179aa3dedb62b78acd7aa9ba5ddcc391436812c5f7084aa from testnet
        // Contains only a METAENTRY with protocol version 25
        //
        // Raw bytes (hex): 80000010 ffffffff 00000019 00000001 00000001
        // - 80000010: XDR record mark (0x10 = 16 bytes, high bit set)
        // - ffffffff: HotArchiveBucketEntryType::Metaentry = -1
        // - 00000019: ledger_version = 25
        // - 00000001: ext.v = 1
        // - 00000001: BucketListType::HotArchive = 1
        let bucket = HotArchiveBucket::fresh(25, vec![], vec![]).unwrap();

        let expected_hash = "95079eba2ff8ef53c179aa3dedb62b78acd7aa9ba5ddcc391436812c5f7084aa";
        assert_eq!(
            bucket.hash().to_hex(),
            expected_hash,
            "Metaentry-only bucket hash should match C++ stellar-core"
        );
    }

    #[test]
    fn test_hot_archive_bucket_lookup() {
        let entry = make_contract_data_entry([1u8; 32], b"key1", 100);
        let key = make_contract_data_key([1u8; 32], b"key1");

        let bucket = HotArchiveBucket::fresh(25, vec![entry.clone()], vec![]).unwrap();
        let found = bucket.get(&key).unwrap();

        assert!(found.is_some());
        match found.unwrap() {
            HotArchiveBucketEntry::Archived(e) => {
                assert_eq!(e, entry);
            }
            _ => panic!("expected Archived entry"),
        }
    }

    #[test]
    fn test_hot_archive_bucket_list_new() {
        let list = HotArchiveBucketList::new();
        assert_eq!(list.ledger_seq(), 0);
        assert_eq!(list.stats().total_entries, 0);
    }

    #[test]
    fn test_hot_archive_bucket_list_add_batch() {
        let mut list = HotArchiveBucketList::new();
        let entry = make_contract_data_entry([1u8; 32], b"key1", 100);

        list.add_batch(1, 25, vec![entry.clone()], vec![]).unwrap();

        let key = make_contract_data_key([1u8; 32], b"key1");
        let found = list.get(&key).unwrap();
        assert!(found.is_some());
    }

    #[test]
    fn test_hot_archive_bucket_list_restoration() {
        let mut list = HotArchiveBucketList::new();
        let entry = make_contract_data_entry([1u8; 32], b"key1", 100);
        let key = make_contract_data_key([1u8; 32], b"key1");

        // Archive the entry
        list.add_batch(1, 25, vec![entry.clone()], vec![]).unwrap();
        assert!(list.contains(&key).unwrap());

        // Mark as restored
        list.add_batch(2, 25, vec![], vec![key.clone()]).unwrap();

        // Entry should not be found (restored = Live marker shadows Archived)
        assert!(!list.contains(&key).unwrap());
    }

    #[test]
    fn test_merge_newer_always_wins_archived_then_live() {
        let entry = make_contract_data_entry([1u8; 32], b"key1", 100);
        let key = make_contract_data_key([1u8; 32], b"key1");

        // curr (old) has Archived, snap (new) has Live
        // C++ always takes newer entry, so Live should win
        let curr = HotArchiveBucket::from_entries(vec![
            HotArchiveBucketEntry::Metaentry(BucketMetadata {
                ledger_version: 25,
                ext: BucketMetadataExt::V1(BucketListType::HotArchive),
            }),
            HotArchiveBucketEntry::Archived(entry),
        ])
        .unwrap();

        let snap = HotArchiveBucket::from_entries(vec![
            HotArchiveBucketEntry::Metaentry(BucketMetadata {
                ledger_version: 25,
                ext: BucketMetadataExt::V1(BucketListType::HotArchive),
            }),
            HotArchiveBucketEntry::Live(key.clone()),
        ])
        .unwrap();

        let merged = merge_hot_archive_buckets(&curr, &snap, 25, true).unwrap();

        // Live from snap (newer) wins - entry exists but is a tombstone
        let entry_result = merged.get(&key).unwrap();
        assert!(entry_result.is_some());
        assert!(matches!(
            entry_result.unwrap(),
            HotArchiveBucketEntry::Live(_)
        ));
    }

    #[test]
    fn test_merge_newer_always_wins_live_then_archived() {
        let entry = make_contract_data_entry([1u8; 32], b"key1", 100);
        let key = make_contract_data_key([1u8; 32], b"key1");

        // curr (old) has Live, snap (new) has Archived
        // C++ always takes newer entry, so Archived should win
        let curr = HotArchiveBucket::from_entries(vec![
            HotArchiveBucketEntry::Metaentry(BucketMetadata {
                ledger_version: 25,
                ext: BucketMetadataExt::V1(BucketListType::HotArchive),
            }),
            HotArchiveBucketEntry::Live(key.clone()),
        ])
        .unwrap();

        let snap = HotArchiveBucket::from_entries(vec![
            HotArchiveBucketEntry::Metaentry(BucketMetadata {
                ledger_version: 25,
                ext: BucketMetadataExt::V1(BucketListType::HotArchive),
            }),
            HotArchiveBucketEntry::Archived(entry.clone()),
        ])
        .unwrap();

        let merged = merge_hot_archive_buckets(&curr, &snap, 25, true).unwrap();

        // Archived from snap (newer) wins
        let entry_result = merged.get(&key).unwrap();
        assert!(entry_result.is_some());
        match entry_result.unwrap() {
            HotArchiveBucketEntry::Archived(e) => {
                assert_eq!(e.last_modified_ledger_seq, entry.last_modified_ledger_seq);
            }
            _ => panic!("Expected Archived entry"),
        }
    }

    #[test]
    fn test_tombstones_dropped_at_bottom() {
        let key = make_contract_data_key([1u8; 32], b"key1");

        let bucket = HotArchiveBucket::from_entries(vec![
            HotArchiveBucketEntry::Metaentry(BucketMetadata {
                ledger_version: 25,
                ext: BucketMetadataExt::V1(BucketListType::HotArchive),
            }),
            HotArchiveBucketEntry::Live(key.clone()),
        ])
        .unwrap();

        // keep_tombstones = false (bottom level)
        let merged =
            merge_hot_archive_buckets(&bucket, &HotArchiveBucket::empty(), 25, false).unwrap();

        // Live entry should be dropped
        assert!(merged.get(&key).unwrap().is_none());
    }

    // ============ Protocol Version Handling Regression Tests ============

    #[test]
    fn test_hot_archive_merge_uses_max_of_inputs() {
        // Regression test: hot archive merge uses max(curr, snap) for output version,
        // NOT the passed protocol_version which is only a constraint.
        let entry1 = make_contract_data_entry([1u8; 32], b"key1", 100);
        let entry2 = make_contract_data_entry([2u8; 32], b"key2", 200);

        // curr has protocol version 23
        let curr = HotArchiveBucket::from_entries(vec![
            HotArchiveBucketEntry::Metaentry(BucketMetadata {
                ledger_version: 23,
                ext: BucketMetadataExt::V1(BucketListType::HotArchive),
            }),
            HotArchiveBucketEntry::Archived(entry1),
        ])
        .unwrap();

        // snap has protocol version 24
        let snap = HotArchiveBucket::from_entries(vec![
            HotArchiveBucketEntry::Metaentry(BucketMetadata {
                ledger_version: 24,
                ext: BucketMetadataExt::V1(BucketListType::HotArchive),
            }),
            HotArchiveBucketEntry::Archived(entry2),
        ])
        .unwrap();

        // protocol_version = 25 (constraint from ledger header)
        let merged = merge_hot_archive_buckets(&curr, &snap, 25, true).unwrap();

        // Output should use max(23, 24) = 24, NOT 25
        assert_eq!(
            merged.get_protocol_version(),
            24,
            "Hot archive merge should use max(curr=23, snap=24)=24, NOT max_protocol_version=25"
        );
    }

    #[test]
    fn test_hot_archive_merge_validates_constraint() {
        // Regression test: protocol_version should be validated as a constraint
        let entry = make_contract_data_entry([1u8; 32], b"key1", 100);

        // Bucket with protocol version 26 (exceeds constraint)
        let bucket = HotArchiveBucket::from_entries(vec![
            HotArchiveBucketEntry::Metaentry(BucketMetadata {
                ledger_version: 26,
                ext: BucketMetadataExt::V1(BucketListType::HotArchive),
            }),
            HotArchiveBucketEntry::Archived(entry),
        ])
        .unwrap();

        // Should fail because bucket version 26 > constraint 25
        let result = merge_hot_archive_buckets(&bucket, &HotArchiveBucket::empty(), 25, true);
        assert!(
            result.is_err(),
            "Should fail when bucket version exceeds protocol_version constraint"
        );
    }

    #[test]
    fn test_hot_archive_merge_same_version_uses_that_version() {
        // When both buckets have the same version, output should use that version
        let entry1 = make_contract_data_entry([1u8; 32], b"key1", 100);
        let entry2 = make_contract_data_entry([2u8; 32], b"key2", 200);

        let curr = HotArchiveBucket::from_entries(vec![
            HotArchiveBucketEntry::Metaentry(BucketMetadata {
                ledger_version: 24,
                ext: BucketMetadataExt::V1(BucketListType::HotArchive),
            }),
            HotArchiveBucketEntry::Archived(entry1),
        ])
        .unwrap();

        let snap = HotArchiveBucket::from_entries(vec![
            HotArchiveBucketEntry::Metaentry(BucketMetadata {
                ledger_version: 24,
                ext: BucketMetadataExt::V1(BucketListType::HotArchive),
            }),
            HotArchiveBucketEntry::Archived(entry2),
        ])
        .unwrap();

        // max_protocol_version = 25, but both inputs are 24
        let merged = merge_hot_archive_buckets(&curr, &snap, 25, true).unwrap();

        // Output should be 24 (max of inputs), not 25
        assert_eq!(
            merged.get_protocol_version(),
            24,
            "Output should be max(24, 24)=24, not constraint=25"
        );
    }
}
