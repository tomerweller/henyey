//! rs-stellar-core - Pure Rust implementation of Stellar Core
//!
//! This binary provides a command-line interface for running a Stellar Core
//! validator node, catching up from history archives, and performing other
//! Stellar network operations.
//!
//! # Quick Start
//!
//! ```bash
//! # Run a node on testnet (default)
//! rs-stellar-core --testnet run
//!
//! # Catch up to the latest ledger
//! rs-stellar-core --testnet catchup current
//!
//! # Generate a new keypair
//! rs-stellar-core new-keypair
//! ```
//!
//! # Commands
//!
//! The CLI provides the following main commands:
//!
//! - **run**: Start the node (as watcher, full node, or validator)
//! - **catchup**: Catch up from history archives to a specific ledger
//! - **new-db**: Create a new database
//! - **upgrade-db**: Upgrade database schema
//! - **new-keypair**: Generate a new node keypair
//! - **info**: Print node information
//! - **verify-history**: Verify history archives
//! - **publish-history**: Publish history to archives (validators only)
//! - **check-quorum-intersection**: Verify quorum intersection from JSON
//! - **sample-config**: Print sample configuration
//! - **offline**: Offline utilities (XDR tools, replay testing)
//!
//! # Configuration
//!
//! Configuration can be provided via:
//! - A TOML configuration file (`--config <FILE>`)
//! - Built-in network defaults (`--testnet` or `--mainnet`)
//! - Environment variables (prefixed with `STELLAR_`)
//!
//! See `rs-stellar-core sample-config` for an example configuration.
//!
//! # Architecture
//!
//! This crate is a thin CLI wrapper around the [`henyey_app`] crate,
//! which contains the core application logic. The CLI handles:
//!
//! - Argument parsing with `clap`
//! - Configuration loading and merging
//! - Logging initialization
//! - Command dispatch
//!
//! The actual node implementation, catchup logic, and subsystem coordination
//! are handled by the underlying library crates.

mod quorum_intersection;

use std::path::PathBuf;

use clap::{Parser, Subcommand};
use stellar_xdr::curr::WriteXdr;

use henyey_app::{
    logging, run_catchup, run_node, App, AppConfig, CatchupMode as CatchupModeInternal,
    CatchupOptions, LogConfig, LogFormat, RunMode, RunOptions,
};

/// Pure Rust implementation of Stellar Core
#[derive(Parser)]
#[command(name = "rs-stellar-core")]
#[command(author, version, about, long_about = None)]
struct Cli {
    /// Path to configuration file
    #[arg(short, long, value_name = "FILE", global = true)]
    config: Option<PathBuf>,

    /// Enable verbose logging (debug level)
    #[arg(short, long, global = true)]
    verbose: bool,

    /// Enable trace logging (most verbose)
    #[arg(long, global = true)]
    trace: bool,

    /// Log output format
    #[arg(long, default_value = "text", global = true)]
    log_format: CliLogFormat,

    /// Use testnet configuration (default)
    #[arg(long, global = true)]
    testnet: bool,

    /// Use mainnet configuration
    #[arg(long, global = true)]
    mainnet: bool,

    /// Stream LedgerCloseMeta XDR frames to a file, named pipe, or fd:N
    #[arg(long = "metadata-output-stream", value_name = "STREAM", global = true)]
    metadata_output_stream: Option<String>,

    #[command(subcommand)]
    command: Commands,
}

/// Log output format for CLI
#[derive(Clone, Copy, Debug, Default, clap::ValueEnum)]
enum CliLogFormat {
    #[default]
    Text,
    Json,
}

impl From<CliLogFormat> for LogFormat {
    fn from(fmt: CliLogFormat) -> Self {
        match fmt {
            CliLogFormat::Text => LogFormat::Text,
            CliLogFormat::Json => LogFormat::Json,
        }
    }
}

/// Available commands
#[derive(Subcommand)]
enum Commands {
    /// Run the Stellar Core node
    Run {
        /// Run in validator mode (participate in consensus)
        #[arg(long)]
        validator: bool,

        /// Run in watcher mode (observe only, no catchup)
        #[arg(long)]
        watcher: bool,

        /// Force catchup even if state exists
        #[arg(long)]
        force_catchup: bool,
    },

    /// Catch up from history archives
    Catchup {
        /// Target ledger: "current", a ledger number, or "ledger/count"
        #[arg(value_name = "TARGET", default_value = "current")]
        target: String,

        /// Catchup mode: minimal, complete, or recent:N (e.g., recent:128)
        #[arg(long, default_value = "minimal")]
        mode: String,

        /// Skip verification after catchup
        #[arg(long)]
        no_verify: bool,

        /// Number of parallel downloads
        #[arg(long, default_value = "8")]
        parallelism: usize,
    },

    /// Create a new database
    NewDb {
        /// Path to the database file (overrides config)
        #[arg(value_name = "PATH")]
        path: Option<PathBuf>,

        /// Force creation even if database exists
        #[arg(long)]
        force: bool,
    },

    /// Upgrade database schema
    UpgradeDb,

    /// Generate a new node keypair
    NewKeypair,

    /// Print node information
    Info,

    /// Verify history archives
    VerifyHistory {
        /// Start ledger sequence
        #[arg(long)]
        from: Option<u32>,

        /// End ledger sequence
        #[arg(long)]
        to: Option<u32>,
    },

    /// Publish history to archives (validators only)
    PublishHistory {
        /// Force republishing of existing checkpoints
        #[arg(long)]
        force: bool,
    },

    /// Check quorum intersection from a JSON file
    CheckQuorumIntersection {
        /// Path to the JSON file
        path: PathBuf,
    },

    /// Print sample configuration
    SampleConfig,

    /// Send a command to a running stellar-core node
    ///
    /// Makes an HTTP GET request to the local node's command interface.
    /// Examples:
    ///   http-command info
    ///   http-command "peers?fullkeys=true"
    ///   http-command "ll?level=DEBUG"
    HttpCommand {
        /// The command to send (e.g., "info", "peers", "ll?level=DEBUG")
        command: String,

        /// HTTP port of the running node (default: 11626)
        #[arg(long, short, default_value = "11626")]
        port: u16,
    },

    /// Offline commands (no network required)
    #[command(subcommand)]
    Offline(OfflineCommands),
}

/// Offline commands that don't require network access
#[derive(Subcommand)]
enum OfflineCommands {
    /// Convert Stellar keys between formats
    ConvertKey {
        /// The key to convert
        key: String,
    },

    /// Decode an XDR value
    DecodeXdr {
        /// XDR type name
        #[arg(long, value_name = "TYPE")]
        r#type: String,

        /// Base64-encoded XDR
        value: String,
    },

    /// Encode a value to XDR
    EncodeXdr {
        /// XDR type name
        #[arg(long, value_name = "TYPE")]
        r#type: String,

        /// JSON value to encode
        value: String,
    },

    /// Print bucket list information
    BucketInfo {
        /// Path to bucket directory
        path: PathBuf,
    },

    /// Test transaction execution by comparing our results against CDP metadata
    ///
    /// This validates that our transaction execution (Soroban host, classic ops)
    /// produces the same ledger entry changes as C++ stellar-core. Differences
    /// indicate execution divergence that needs investigation.
    VerifyExecution {
        /// Start ledger sequence (defaults to a recent checkpoint)
        #[arg(long)]
        from: Option<u32>,

        /// End ledger sequence (defaults to latest available)
        #[arg(long)]
        to: Option<u32>,

        /// Stop on first mismatch
        #[arg(long)]
        stop_on_error: bool,

        /// Show detailed diff of mismatched entries
        #[arg(long)]
        show_diff: bool,

        /// CDP data lake URL (default: network-specific - testnet or pubnet)
        #[arg(long)]
        cdp_url: Option<String>,

        /// CDP date partition (default: 2025-12-18 for testnet, empty for mainnet)
        #[arg(long)]
        cdp_date: Option<String>,

        /// Cache directory for buckets and CDP metadata (default: ~/.cache/rs-stellar-core)
        #[arg(long)]
        cache_dir: Option<std::path::PathBuf>,

        /// Disable caching (use temp directories)
        #[arg(long)]
        no_cache: bool,

        /// Quiet mode: only output summary and errors
        #[arg(long, short = 'q')]
        quiet: bool,
    },

    /// Debug: inspect an account in the bucket list at a checkpoint
    DebugBucketEntry {
        /// Checkpoint ledger sequence
        #[arg(long)]
        checkpoint: u32,

        /// Account ID (hex)
        #[arg(long)]
        account: String,
    },

    /// Add a signature to a transaction envelope
    ///
    /// Reads a transaction envelope (base64), prompts for a secret key,
    /// and outputs the signed envelope.
    SignTransaction {
        /// Network passphrase for signing (required)
        #[arg(long)]
        netid: String,

        /// Input transaction envelope in base64 (or "-" for stdin)
        #[arg(default_value = "-")]
        input: String,

        /// Output as base64 (default: true)
        #[arg(long, default_value = "true")]
        base64: bool,
    },

    /// Print the public key corresponding to a secret key
    ///
    /// Reads a secret key seed (S...) from stdin and prints the public key.
    SecToPub,

    /// Dump ledger entries to JSON
    ///
    /// Dumps ledger entries from the bucket list to a JSON file for debugging.
    /// Supports filtering by entry type and limiting output count.
    DumpLedger {
        /// Output file path
        #[arg(long, short)]
        output: PathBuf,

        /// Filter by entry type (account, trustline, offer, data, claimable_balance,
        /// liquidity_pool, contract_data, contract_code, config_setting, ttl)
        #[arg(long)]
        entry_type: Option<String>,

        /// Maximum number of entries to output
        #[arg(long)]
        limit: Option<u64>,

        /// Only include entries modified in the last N ledgers
        #[arg(long)]
        last_modified_ledger_count: Option<u32>,
    },

    /// Perform diagnostic self-checks
    ///
    /// This command performs comprehensive diagnostic checks including:
    /// - Header chain verification (verify hash linkage)
    /// - Bucket hash verification (verify all buckets have correct hashes)
    /// - Crypto benchmarking (Ed25519 sign/verify performance)
    ///
    /// Unlike the HTTP /self-check endpoint (which only runs quick online checks),
    /// this offline command performs full bucket verification which may take
    /// significant time on large databases.
    SelfCheck,

    /// Write verified checkpoint ledger hashes to a file
    ///
    /// Downloads checkpoint headers from history archives, verifies the header
    /// chain, and writes verified checkpoint hashes to a JSON file. This file
    /// can be used with `--trusted-checkpoint-hashes` during catchup to verify
    /// against known-good hashes.
    VerifyCheckpoints {
        /// Output file path for verified checkpoint hashes
        #[arg(long, short, required = true)]
        output: PathBuf,

        /// Start ledger sequence (defaults to genesis)
        #[arg(long)]
        from: Option<u32>,

        /// End ledger sequence (defaults to current)
        #[arg(long)]
        to: Option<u32>,
    },
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let cli = Cli::parse();

    // Initialize logging
    init_logging(&cli)?;

    // Load or create configuration
    let config = load_config(&cli)?;

    // Execute command
    match cli.command {
        Commands::Run {
            validator,
            watcher,
            force_catchup,
        } => cmd_run(config, validator, watcher, force_catchup).await,

        Commands::Catchup {
            target,
            mode,
            no_verify,
            parallelism,
        } => cmd_catchup(config, target, mode, !no_verify, parallelism).await,

        Commands::NewDb { path, force } => cmd_new_db(config, path, force).await,

        Commands::UpgradeDb => cmd_upgrade_db(config).await,

        Commands::NewKeypair => cmd_new_keypair(),

        Commands::Info => cmd_info(config).await,

        Commands::VerifyHistory { from, to } => cmd_verify_history(config, from, to).await,

        Commands::PublishHistory { force } => cmd_publish_history(config, force).await,

        Commands::CheckQuorumIntersection { path } => cmd_check_quorum_intersection(&path),

        Commands::SampleConfig => cmd_sample_config(),

        Commands::HttpCommand { command, port } => cmd_http_command(&command, port).await,

        Commands::Offline(cmd) => cmd_offline(cmd, config).await,
    }
}

/// Initialize the logging subsystem.
fn init_logging(cli: &Cli) -> anyhow::Result<()> {
    let level = if cli.trace {
        "trace"
    } else if cli.verbose {
        "debug"
    } else {
        "info"
    };

    let config = LogConfig::default().with_level(level);

    let config = match cli.log_format {
        CliLogFormat::Text => config,
        CliLogFormat::Json => LogConfig {
            format: LogFormat::Json,
            ansi_colors: false,
            ..config
        },
    };

    logging::init(&config)?;

    tracing::debug!("Logging initialized");
    Ok(())
}

/// Load configuration from file or use defaults.
fn load_config(cli: &Cli) -> anyhow::Result<AppConfig> {
    let mut config = if let Some(ref config_path) = cli.config {
        tracing::info!(path = ?config_path, "Loading configuration from file");
        AppConfig::from_file_with_env(config_path)?
    } else if cli.mainnet {
        tracing::info!("Using mainnet configuration");
        let mut config = AppConfig::mainnet();
        config.apply_env_overrides();
        config
    } else {
        tracing::info!("Using testnet configuration (default)");
        let mut config = AppConfig::testnet();
        config.apply_env_overrides();
        config
    };

    // CLI --metadata-output-stream overrides config file
    if let Some(ref stream) = cli.metadata_output_stream {
        config.metadata.output_stream = Some(stream.clone());
    }

    Ok(config)
}

/// Run command handler.
async fn cmd_run(
    config: AppConfig,
    validator: bool,
    watcher: bool,
    force_catchup: bool,
) -> anyhow::Result<()> {
    if validator && watcher {
        anyhow::bail!("Cannot run as both validator and watcher");
    }

    let mode = if validator {
        RunMode::Validator
    } else if watcher {
        RunMode::Watcher
    } else {
        RunMode::Full
    };

    let options = RunOptions {
        mode,
        force_catchup,
        ..Default::default()
    };

    run_node(config, options).await
}

/// Catchup command handler.
async fn cmd_catchup(
    config: AppConfig,
    target: String,
    mode: String,
    verify: bool,
    parallelism: usize,
) -> anyhow::Result<()> {
    // Parse mode string into CatchupMode (supports "minimal", "complete", "recent:N")
    let mode: CatchupModeInternal = mode.parse()?;

    let options = CatchupOptions {
        target,
        mode,
        verify,
        parallelism,
        keep_temp: false,
    };

    let result = run_catchup(config, options).await?;
    println!("{}", result);
    Ok(())
}

/// New database command handler.
async fn cmd_new_db(
    mut config: AppConfig,
    path: Option<PathBuf>,
    force: bool,
) -> anyhow::Result<()> {
    // Override path if provided
    if let Some(p) = path {
        config.database.path = p;
    }

    let db_path = &config.database.path;

    // Check if database already exists
    if db_path.exists() {
        if force {
            tracing::warn!(path = ?db_path, "Removing existing database");
            std::fs::remove_file(db_path)?;
        } else {
            anyhow::bail!(
                "Database already exists at {:?}. Use --force to overwrite.",
                db_path
            );
        }
    }

    tracing::info!(path = ?db_path, "Creating new database");

    // Ensure parent directory exists
    if let Some(parent) = db_path.parent() {
        if !parent.as_os_str().is_empty() {
            std::fs::create_dir_all(parent)?;
        }
    }

    // Create the database
    let _db = henyey_db::Database::open(db_path)?;

    println!("Database created successfully at: {}", db_path.display());
    Ok(())
}

/// Upgrade database command handler.
async fn cmd_upgrade_db(config: AppConfig) -> anyhow::Result<()> {
    tracing::info!(path = ?config.database.path, "Upgrading database schema");

    let _db = henyey_db::Database::open(&config.database.path)?;

    // Database initialization already applies the latest schema
    // In a full implementation, this would run migrations

    println!("Database schema is up to date");
    Ok(())
}

/// Generate keypair command handler.
fn cmd_new_keypair() -> anyhow::Result<()> {
    let keypair = henyey_crypto::SecretKey::generate();

    println!("Generated new keypair:");
    println!();
    println!("Public Key:  {}", keypair.public_key().to_strkey());
    println!("Secret Seed: {}", keypair.to_strkey());
    println!();
    println!("IMPORTANT: Store the secret seed securely! It cannot be recovered.");

    Ok(())
}

/// Info command handler.
async fn cmd_info(config: AppConfig) -> anyhow::Result<()> {
    // Try to create app to get full info, or show basic info if that fails
    match App::new(config.clone()).await {
        Ok(app) => {
            let info = app.info();
            print!("{}", info);
        }
        Err(e) => {
            tracing::debug!(error = %e, "Could not initialize app, showing basic info");

            println!("rs-stellar-core {}", env!("CARGO_PKG_VERSION"));
            println!();
            println!("Configuration:");
            println!("  Network: {}", config.network.passphrase);
            println!("  Database: {}", config.database.path.display());
            println!("  Validator: {}", config.node.is_validator);
            println!();

            if !config.database.path.exists() {
                println!("Note: Database does not exist. Run 'new-db' to create it.");
            }
        }
    }

    Ok(())
}

/// Verify history command handler.
async fn cmd_verify_history(
    config: AppConfig,
    from: Option<u32>,
    to: Option<u32>,
) -> anyhow::Result<()> {
    use henyey_history::{verify, HistoryArchive};
    use henyey_ledger::TransactionSetVariant;

    println!("Verifying history archives...");
    println!();

    // Create archive clients from config
    let archives: Vec<HistoryArchive> = config
        .history
        .archives
        .iter()
        .filter(|a| a.get_enabled)
        .filter_map(|a| match HistoryArchive::new(&a.url) {
            Ok(archive) => Some(archive),
            Err(e) => {
                println!("Warning: Failed to create archive {}: {}", a.url, e);
                None
            }
        })
        .collect();

    if archives.is_empty() {
        anyhow::bail!("No history archives available");
    }

    println!("Using {} archive(s)", archives.len());

    // Get the range to verify
    let archive = &archives[0];
    let root_has = archive.get_root_has().await?;
    let current_ledger = root_has.current_ledger;

    let start = from.unwrap_or(1);
    let end = to.unwrap_or(current_ledger);

    println!("Verifying ledger range: {} to {}", start, end);
    println!();

    // Verify in checkpoint intervals (64 ledgers)
    let mut verified_count = 0;
    let mut error_count = 0;

    // Calculate checkpoint-aligned start
    let start_checkpoint = henyey_history::checkpoint::checkpoint_containing(start);
    let end_checkpoint = henyey_history::checkpoint::checkpoint_containing(end);

    let mut checkpoint = start_checkpoint;
    while checkpoint <= end_checkpoint {
        // Get checkpoint HAS (History Archive State)
        match archive.get_checkpoint_has(checkpoint).await {
            Ok(has) => {
                // Verify the HAS structure
                match verify::verify_has_structure(&has) {
                    Ok(()) => {
                        // Verify checkpoint matches
                        match verify::verify_has_checkpoint(&has, checkpoint) {
                            Ok(()) => {
                                println!("  Checkpoint {}: OK", checkpoint);
                                verified_count += 1;
                            }
                            Err(e) => {
                                println!("  Checkpoint {}: FAIL - {}", checkpoint, e);
                                error_count += 1;
                            }
                        }
                    }
                    Err(e) => {
                        println!(
                            "  Checkpoint {}: FAIL (invalid structure) - {}",
                            checkpoint, e
                        );
                        error_count += 1;
                    }
                }

                // Get and verify ledger headers for this checkpoint range
                match archive.get_ledger_headers(checkpoint).await {
                    Ok(history_entries) => {
                        // Extract LedgerHeader from LedgerHeaderHistoryEntry
                        let headers: Vec<stellar_xdr::curr::LedgerHeader> = history_entries
                            .iter()
                            .map(|entry| entry.header.clone())
                            .collect();
                        if let Err(e) = verify::verify_header_chain(&headers) {
                            println!("    Header chain verification FAILED: {}", e);
                            error_count += 1;
                        }

                        match archive.get_transactions(checkpoint).await {
                            Ok(tx_entries) => {
                                match archive.get_results(checkpoint).await {
                                    Ok(tx_results) => {
                                        let tx_map = tx_entries
                                            .iter()
                                            .map(|entry| (entry.ledger_seq, entry))
                                            .collect::<std::collections::HashMap<_, _>>();
                                        let result_map = tx_results
                                            .iter()
                                            .map(|entry| (entry.ledger_seq, entry))
                                            .collect::<std::collections::HashMap<_, _>>();

                                        for header in &headers {
                                            let Some(tx_entry) = tx_map.get(&header.ledger_seq)
                                            else {
                                                println!(
                                                    "    Missing transaction history entry for ledger {}",
                                                    header.ledger_seq
                                                );
                                                error_count += 1;
                                                continue;
                                            };
                                            let Some(result_entry) =
                                                result_map.get(&header.ledger_seq)
                                            else {
                                                println!(
                                                    "    Missing transaction result entry for ledger {}",
                                                    header.ledger_seq
                                                );
                                                error_count += 1;
                                                continue;
                                            };

                                            let tx_set = match &tx_entry.ext {
                                                stellar_xdr::curr::TransactionHistoryEntryExt::V1(generalized) => {
                                                    TransactionSetVariant::Generalized(generalized.clone())
                                                }
                                                stellar_xdr::curr::TransactionHistoryEntryExt::V0 => {
                                                    TransactionSetVariant::Classic(tx_entry.tx_set.clone())
                                                }
                                            };
                                            if let Err(e) = verify::verify_tx_set(header, &tx_set) {
                                                println!(
                                                    "    Tx set hash verification FAILED (ledger {}): {}",
                                                    header.ledger_seq, e
                                                );
                                                error_count += 1;
                                            }

                                            let result_xdr = result_entry
                                                .tx_result_set
                                                .to_xdr(stellar_xdr::curr::Limits::none());
                                            match result_xdr {
                                                Ok(bytes) => {
                                                    if let Err(e) =
                                                        verify::verify_tx_result_set(header, &bytes)
                                                    {
                                                        println!(
                                                            "    Tx result hash verification FAILED (ledger {}): {}",
                                                            header.ledger_seq, e
                                                        );
                                                        error_count += 1;
                                                    }
                                                }
                                                Err(e) => {
                                                    println!(
                                                        "    Failed to encode tx result set for ledger {}: {}",
                                                        header.ledger_seq, e
                                                    );
                                                    error_count += 1;
                                                }
                                            }
                                        }
                                    }
                                    Err(e) => {
                                        println!("    Warning: Could not verify tx results: {}", e);
                                    }
                                }
                            }
                            Err(e) => {
                                println!("    Warning: Could not verify transactions: {}", e);
                            }
                        }
                    }
                    Err(e) => {
                        println!("    Warning: Could not verify headers: {}", e);
                    }
                }

                match archive.get_scp_history(checkpoint).await {
                    Ok(entries) => {
                        if let Err(e) = verify::verify_scp_history_entries(&entries) {
                            println!("    SCP history verification FAILED: {}", e);
                            error_count += 1;
                        }
                    }
                    Err(e) => {
                        println!("    Warning: Could not verify SCP history: {}", e);
                    }
                }
            }
            Err(e) => {
                println!("  Checkpoint {}: ERROR - {}", checkpoint, e);
                error_count += 1;
            }
        }

        // Move to next checkpoint
        checkpoint = henyey_history::checkpoint::next_checkpoint(checkpoint);
    }

    println!();
    println!("Verification complete:");
    println!("  Checkpoints verified: {}", verified_count);
    println!("  Errors: {}", error_count);

    if error_count > 0 {
        anyhow::bail!("Verification failed with {} errors", error_count);
    }

    Ok(())
}

/// Publish history command handler.
async fn cmd_publish_history(config: AppConfig, force: bool) -> anyhow::Result<()> {
    use std::fs;
    use std::path::PathBuf;
    use henyey_bucket::{BucketList, BucketManager};
    use henyey_common::Hash256;
    use henyey_history::archive_state::HistoryArchiveState;
    use henyey_history::checkpoint::{checkpoint_containing, next_checkpoint};
    use henyey_history::paths::root_has_path;
    use henyey_history::publish::{
        build_history_archive_state, PublishConfig, PublishManager,
    };
    use henyey_history::verify;
    use henyey_history::CHECKPOINT_FREQUENCY;
    use henyey_ledger::compute_header_hash;
    use henyey_ledger::TransactionSetVariant;
    use stellar_xdr::curr::TransactionHistoryEntryExt;
    use url::Url;

    if !config.node.is_validator {
        anyhow::bail!("Only validators can publish history");
    }

    println!("Publishing history to archives...");
    println!();

    // Check for writable archives
    let writable_archives: Vec<_> = config
        .history
        .archives
        .iter()
        .filter(|a| a.put_enabled)
        .collect();

    if writable_archives.is_empty() {
        anyhow::bail!(
            "No writable history archives configured. Add 'put = true' to an archive config."
        );
    }

    let mut local_targets = Vec::new();
    let mut command_targets = Vec::new();
    for archive in &writable_archives {
        if let Some(put) = archive.put.clone() {
            command_targets.push(CommandArchiveTarget {
                name: archive.name.clone(),
                put,
                mkdir: archive.mkdir.clone(),
            });
            continue;
        }

        let path = match Url::parse(&archive.url) {
            Ok(url) if url.scheme() == "file" => url
                .to_file_path()
                .map_err(|_| anyhow::anyhow!("Invalid file URL: {}", archive.url))?,
            Ok(_) => {
                tracing::warn!(archive = %archive.url, "Remote publish not supported (missing put command)");
                continue;
            }
            Err(_) => PathBuf::from(&archive.url),
        };
        local_targets.push((archive.url.as_str(), path));
    }

    if local_targets.is_empty() && command_targets.is_empty() {
        anyhow::bail!("No publish archives configured (local paths or put commands required).");
    }

    println!(
        "Writable archives: {}",
        local_targets.len() + command_targets.len()
    );
    for (url, _) in &local_targets {
        println!("  - {}", url);
    }
    for archive in &command_targets {
        println!("  - {} (command)", archive.name);
    }
    println!();

    // Open database to get current state
    let db = henyey_db::Database::open(&config.database.path)?;

    // Get current ledger from database
    let current_ledger = db
        .get_latest_ledger_seq()?
        .ok_or_else(|| anyhow::anyhow!("No ledger data in database. Run the node first."))?;

    println!("Current ledger in database: {}", current_ledger);

    // Calculate checkpoints to publish
    let latest_checkpoint =
        henyey_history::checkpoint::latest_checkpoint_before_or_at(current_ledger)
            .ok_or_else(|| {
                anyhow::anyhow!("No checkpoint available for ledger {}", current_ledger)
            })?;

    println!("Latest publishable checkpoint: {}", latest_checkpoint);

    let queued_checkpoints = db.load_publish_queue(None)?;
    let mut queued_checkpoints = queued_checkpoints
        .into_iter()
        .filter(|checkpoint| *checkpoint <= latest_checkpoint)
        .collect::<Vec<_>>();
    queued_checkpoints.sort_unstable();

    // Check what's already published across local archives.
    let mut published_ledger = latest_checkpoint;
    for (_, path) in &local_targets {
        let root_path = path.join(root_has_path());
        let ledger = if root_path.exists() {
            let json = fs::read_to_string(&root_path)?;
            let has = HistoryArchiveState::from_json(&json)?;
            has.current_ledger()
        } else {
            0
        };
        published_ledger = published_ledger.min(ledger);
    }

    println!("Already published up to: {}", published_ledger);

    if published_ledger >= latest_checkpoint && !force && queued_checkpoints.is_empty() {
        println!();
        println!("Archive is up to date. Use --force to republish.");
        return Ok(());
    }

    let mut checkpoints_to_publish = Vec::new();
    if !queued_checkpoints.is_empty() && !force {
        checkpoints_to_publish = queued_checkpoints;
    } else {
        // Calculate range to publish
        let start_checkpoint = if published_ledger > 0 && !force {
            next_checkpoint(published_ledger)
        } else {
            // Start from the first checkpoint we have
            checkpoint_containing(1)
        };

        if start_checkpoint > latest_checkpoint {
            println!("Nothing new to publish.");
            return Ok(());
        }

        let mut checkpoint = start_checkpoint;
        while checkpoint <= latest_checkpoint {
            checkpoints_to_publish.push(checkpoint);
            checkpoint = next_checkpoint(checkpoint);
        }
    }

    if checkpoints_to_publish.is_empty() {
        println!("Nothing queued to publish.");
        return Ok(());
    }

    println!();
    let first_checkpoint = *checkpoints_to_publish
        .first()
        .expect("checked non-empty above");
    let last_checkpoint = *checkpoints_to_publish
        .last()
        .expect("checked non-empty above");
    println!(
        "Publishing checkpoints {} to {}...",
        first_checkpoint, last_checkpoint
    );

    let bucket_manager = BucketManager::with_cache_size(
        config.buckets.directory.clone(),
        config.buckets.cache_size,
    )?;

    let mut published_count = 0;
    for checkpoint in checkpoints_to_publish {
        print!("  Publishing checkpoint {}... ", checkpoint);

        let start_ledger = checkpoint.saturating_sub(CHECKPOINT_FREQUENCY - 1);
        let start_ledger = if start_ledger == 0 { 1 } else { start_ledger };
        let mut headers = Vec::new();
        let mut tx_entries = Vec::new();
        let mut tx_results = Vec::new();

        for seq in start_ledger..=checkpoint {
            let header = db
                .get_ledger_header(seq)?
                .ok_or_else(|| anyhow::anyhow!("Missing ledger header {}", seq))?;
            let hash = compute_header_hash(&header)?;
            headers.push(stellar_xdr::curr::LedgerHeaderHistoryEntry {
                header,
                hash: stellar_xdr::curr::Hash(hash.0),
                ext: stellar_xdr::curr::LedgerHeaderHistoryEntryExt::V0,
            });

            let tx_entry = db
                .get_tx_history_entry(seq)?
                .ok_or_else(|| anyhow::anyhow!("Missing tx history entry {}", seq))?;
            tx_entries.push(tx_entry);

            let tx_result = db
                .get_tx_result_entry(seq)?
                .ok_or_else(|| anyhow::anyhow!("Missing tx result entry {}", seq))?;
            tx_results.push(tx_result);
        }

        let scp_entries = build_scp_history_entries(&db, start_ledger, checkpoint)?;

        for idx in 0..headers.len() {
            let header_entry = &headers[idx];
            let tx_entry = &tx_entries[idx];
            let tx_result_entry = &tx_results[idx];
            let header = &header_entry.header;
            let tx_set = match &tx_entry.ext {
                TransactionHistoryEntryExt::V1(generalized) => {
                    TransactionSetVariant::Generalized(generalized.clone())
                }
                TransactionHistoryEntryExt::V0 => {
                    TransactionSetVariant::Classic(tx_entry.tx_set.clone())
                }
            };
            let tx_set_hash = verify::compute_tx_set_hash(&tx_set).unwrap_or(Hash256::ZERO);
            let expected_tx_set = Hash256::from(header.scp_value.tx_set_hash.0);
            if tx_set_hash != expected_tx_set {
                anyhow::bail!(
                    "Tx set hash mismatch at {} (expected {}, got {})",
                    header.ledger_seq,
                    expected_tx_set.to_hex(),
                    tx_set_hash.to_hex()
                );
            }

            let tx_result_hash =
                Hash256::hash_xdr(&tx_result_entry.tx_result_set).unwrap_or(Hash256::ZERO);
            let expected_tx_result = Hash256::from(header.tx_set_result_hash.0);
            if tx_result_hash != expected_tx_result {
                anyhow::bail!(
                    "Tx result hash mismatch at {} (expected {}, got {})",
                    header.ledger_seq,
                    expected_tx_result.to_hex(),
                    tx_result_hash.to_hex()
                );
            }
        }

        let levels = db
            .load_bucket_list(checkpoint)?
            .ok_or_else(|| anyhow::anyhow!("Missing bucket list snapshot {}", checkpoint))?;
        let mut bucket_list = BucketList::new();
        bucket_list.set_bucket_dir(bucket_manager.bucket_dir().to_path_buf());
        for (idx, (curr_hash, snap_hash)) in levels.iter().enumerate() {
            let curr_bucket = bucket_manager.load_bucket(curr_hash)?;
            let snap_bucket = bucket_manager.load_bucket(snap_hash)?;
            let level = bucket_list
                .level_mut(idx)
                .ok_or_else(|| anyhow::anyhow!("Missing bucket level {}", idx))?;
            level.set_curr((*curr_bucket).clone());
            level.set_snap((*snap_bucket).clone());
        }

        let expected_hash = Hash256::from(headers.last().unwrap().header.bucket_list_hash.0);
        let actual_hash = bucket_list.hash();
        if expected_hash != actual_hash {
            anyhow::bail!(
                "Bucket list hash mismatch at {} (expected {}, got {})",
                checkpoint,
                expected_hash.to_hex(),
                actual_hash.to_hex()
            );
        }

        let command_publish_dir = if command_targets.is_empty() {
            None
        } else {
            let sanitized_name = config
                .node
                .name
                .chars()
                .map(|c| if c.is_ascii_alphanumeric() { c } else { '_' })
                .collect::<String>();
            let publish_dir = std::env::temp_dir().join(format!(
                "rs-stellar-core-publish-{}-{}",
                sanitized_name, checkpoint
            ));
            if publish_dir.exists() {
                std::fs::remove_dir_all(&publish_dir)?;
            }
            std::fs::create_dir_all(&publish_dir)?;

            let publish_config = PublishConfig {
                local_path: publish_dir.clone(),
                network_passphrase: Some(config.network.passphrase.clone()),
                ..Default::default()
            };
            let manager = PublishManager::new(publish_config);
            manager
                .publish_checkpoint(checkpoint, &headers, &tx_entries, &tx_results, &bucket_list)
                .await?;

            let has = build_history_archive_state(
                checkpoint,
                &bucket_list,
                None,
                Some(config.network.passphrase.clone()),
            )?;
            let root_path = publish_dir.join(root_has_path());
            if let Some(parent) = root_path.parent() {
                fs::create_dir_all(parent)?;
            }
            fs::write(&root_path, has.to_json()?)?;
            Some(publish_dir)
        };

        let mut published_any = false;
        for (url, path) in &local_targets {
            let publish_config = PublishConfig {
                local_path: path.clone(),
                network_passphrase: Some(config.network.passphrase.clone()),
                ..Default::default()
            };
            let manager = PublishManager::new(publish_config);
            if !force && manager.is_published(checkpoint) {
                continue;
            }
            manager
                .publish_checkpoint(checkpoint, &headers, &tx_entries, &tx_results, &bucket_list)
                .await?;
            write_scp_history_file(path, checkpoint, &scp_entries)?;
            let has = build_history_archive_state(
                checkpoint,
                &bucket_list,
                None,
                Some(config.network.passphrase.clone()),
            )?;
            let root_path = path.join(root_has_path());
            if let Some(parent) = root_path.parent() {
                fs::create_dir_all(parent)?;
            }
            fs::write(&root_path, has.to_json()?)?;
            println!("OK ({})", url);
            published_any = true;
        }

        if let Some(ref publish_dir) = command_publish_dir {
            write_scp_history_file(publish_dir, checkpoint, &scp_entries)?;
            for archive in &command_targets {
                upload_publish_directory(archive, publish_dir)?;
                println!("OK (command: {})", archive.name);
                published_any = true;
            }
        }

        if let Some(publish_dir) = command_publish_dir {
            if let Err(err) = std::fs::remove_dir_all(&publish_dir) {
                tracing::warn!(
                    path = %publish_dir.display(),
                    error = %err,
                    "Failed to remove publish temp directory"
                );
            }
        }

        if published_any {
            published_count += 1;
        } else {
            println!("SKIP (already published)");
        }

        if let Err(err) = db.remove_publish(checkpoint) {
            tracing::warn!(checkpoint, error = %err, "Failed to remove publish queue entry");
        }
    }

    println!();
    println!("Publishing complete:");
    println!("  Checkpoints processed: {}", published_count);
    println!();

    Ok(())
}

/// Builds SCP history entries for a checkpoint range from the database.
///
/// Collects SCP envelopes and quorum sets for each ledger in the range,
/// packaging them into the format required for history archive publishing.
fn build_scp_history_entries(
    db: &henyey_db::Database,
    start_ledger: u32,
    checkpoint: u32,
) -> anyhow::Result<Vec<stellar_xdr::curr::ScpHistoryEntry>> {
    use std::collections::HashSet;
    use henyey_common::Hash256;
    use stellar_xdr::curr::{LedgerScpMessages, ScpHistoryEntry, ScpHistoryEntryV0};

    let mut entries = Vec::new();
    for seq in start_ledger..=checkpoint {
        let envelopes = db.load_scp_history(seq)?;
        if envelopes.is_empty() {
            continue;
        }

        let mut qset_hashes = HashSet::new();
        for envelope in &envelopes {
            if let Some(hash) = scp_quorum_set_hash(&envelope.statement) {
                qset_hashes.insert(Hash256::from_bytes(hash.0));
            }
        }

        let mut qset_hashes = qset_hashes.into_iter().collect::<Vec<_>>();
        qset_hashes.sort_by(|a, b| a.as_bytes().cmp(b.as_bytes()));

        let mut qsets = Vec::new();
        for hash in qset_hashes {
            let qset = db
                .load_scp_quorum_set(&hash)?
                .ok_or_else(|| anyhow::anyhow!("Missing quorum set {}", hash.to_hex()))?;
            qsets.push(qset);
        }

        let quorum_sets = qsets
            .try_into()
            .map_err(|_| anyhow::anyhow!("Too many quorum sets for ledger {}", seq))?;
        let messages = envelopes
            .try_into()
            .map_err(|_| anyhow::anyhow!("Too many SCP envelopes for ledger {}", seq))?;
        let entry = ScpHistoryEntry::V0(ScpHistoryEntryV0 {
            quorum_sets,
            ledger_messages: LedgerScpMessages {
                ledger_seq: seq,
                messages,
            },
        });
        entries.push(entry);
    }

    Ok(entries)
}

/// Writes SCP history entries to a gzip-compressed XDR file.
///
/// Creates the file at the standard history archive path for SCP data.
fn write_scp_history_file(
    base_dir: &std::path::Path,
    checkpoint: u32,
    entries: &[stellar_xdr::curr::ScpHistoryEntry],
) -> anyhow::Result<()> {
    use flate2::write::GzEncoder;
    use flate2::Compression;
    use std::io::Write;
    use henyey_history::paths::checkpoint_path;
    use stellar_xdr::curr::{Limits, WriteXdr};

    let path = base_dir.join(checkpoint_path("scp", checkpoint, "xdr.gz"));
    if let Some(parent) = path.parent() {
        std::fs::create_dir_all(parent)?;
    }
    let file = std::fs::File::create(&path)?;
    let mut encoder = GzEncoder::new(file, Compression::default());

    for entry in entries {
        let xdr = entry.to_xdr(Limits::none())?;
        encoder.write_all(&xdr)?;
    }
    encoder.finish()?;
    Ok(())
}

/// Extracts the quorum set hash from an SCP statement.
///
/// Different SCP pledge types store the quorum set hash in different fields.
fn scp_quorum_set_hash(
    statement: &stellar_xdr::curr::ScpStatement,
) -> Option<stellar_xdr::curr::Hash> {
    match &statement.pledges {
        stellar_xdr::curr::ScpStatementPledges::Nominate(nom) => Some(nom.quorum_set_hash.clone()),
        stellar_xdr::curr::ScpStatementPledges::Prepare(prep) => Some(prep.quorum_set_hash.clone()),
        stellar_xdr::curr::ScpStatementPledges::Confirm(conf) => Some(conf.quorum_set_hash.clone()),
        stellar_xdr::curr::ScpStatementPledges::Externalize(ext) => {
            Some(ext.commit_quorum_set_hash.clone())
        }
    }
}

/// Configuration for publishing to a remote archive via shell commands.
#[derive(Clone)]
struct CommandArchiveTarget {
    /// Human-readable name for the archive.
    name: String,
    /// Shell command template for uploading files ({0} = local path, {1} = remote path).
    put: String,
    /// Optional shell command template for creating directories ({0} = remote dir).
    mkdir: Option<String>,
}

/// Uploads a local publish directory to a remote archive using shell commands.
///
/// Iterates through all files in the directory, creating remote directories
/// as needed, then uploads each file using the configured put command.
fn upload_publish_directory(
    target: &CommandArchiveTarget,
    publish_dir: &std::path::Path,
) -> anyhow::Result<()> {
    use std::collections::HashSet;

    let mut files = collect_files(publish_dir)?;
    files.sort();

    let mut created_dirs = HashSet::new();
    for file in files {
        let rel = file
            .strip_prefix(publish_dir)
            .map_err(|e| anyhow::anyhow!("invalid publish path: {}", e))?;
        let rel_str = path_to_unix_string(rel)?;

        if let Some(ref mkdir_cmd) = target.mkdir {
            if let Some(parent) = rel.parent() {
                if !parent.as_os_str().is_empty() {
                    let remote_dir = path_to_unix_string(parent)?;
                    if created_dirs.insert(remote_dir.clone()) {
                        let cmd = render_mkdir_command(mkdir_cmd, &remote_dir);
                        run_shell_command(&cmd)?;
                    }
                }
            }
        }

        let cmd = render_put_command(&target.put, &file, &rel_str);
        run_shell_command(&cmd)?;
    }

    Ok(())
}

/// Recursively collects all files under a directory.
fn collect_files(root: &std::path::Path) -> anyhow::Result<Vec<std::path::PathBuf>> {
    let mut files = Vec::new();
    let mut stack = vec![root.to_path_buf()];

    while let Some(dir) = stack.pop() {
        for entry in std::fs::read_dir(&dir)? {
            let entry = entry?;
            let path = entry.path();
            if path.is_dir() {
                stack.push(path);
            } else if path.is_file() {
                files.push(path);
            }
        }
    }

    Ok(files)
}

/// Converts a path to a Unix-style string with forward slashes.
fn path_to_unix_string(path: &std::path::Path) -> anyhow::Result<String> {
    let mut parts = Vec::new();
    for component in path.components() {
        let part = component.as_os_str().to_string_lossy();
        parts.push(part.to_string());
    }
    Ok(parts.join("/"))
}

/// Renders a put command template with local and remote paths.
fn render_put_command(template: &str, local_path: &std::path::Path, remote_path: &str) -> String {
    template
        .replace("{0}", local_path.to_string_lossy().as_ref())
        .replace("{1}", remote_path)
}

/// Renders a mkdir command template with the remote directory.
fn render_mkdir_command(template: &str, remote_dir: &str) -> String {
    template.replace("{0}", remote_dir)
}

/// Executes a shell command and returns an error if it fails.
fn run_shell_command(cmd: &str) -> anyhow::Result<()> {
    use std::process::Command;

    let status = Command::new("sh").arg("-c").arg(cmd).status()?;
    if status.success() {
        Ok(())
    } else {
        anyhow::bail!("command failed (exit {}): {}", status, cmd);
    }
}

/// Downloads buckets in parallel from a history archive.
///
/// This function downloads multiple buckets concurrently (up to 16 at a time),
/// significantly speeding up initial state restoration compared to sequential downloads.
///
/// # Arguments
///
/// * `archive` - The history archive to download from
/// * `bucket_manager` - The bucket manager to import buckets into
/// * `hashes` - The bucket hashes to download
///
/// # Returns
///
/// Returns `Ok(())` if all buckets were downloaded successfully, or an error
/// if any download failed.
/// Returns (cached_count, downloaded_count)
async fn download_buckets_parallel(
    archive: &henyey_history::HistoryArchive,
    bucket_manager: std::sync::Arc<henyey_bucket::BucketManager>,
    hashes: Vec<&henyey_common::Hash256>,
) -> anyhow::Result<(usize, usize)> {
    use futures::stream::{self, StreamExt};
    use std::collections::HashSet;
    use std::sync::atomic::{AtomicU32, Ordering};

    const MAX_CONCURRENT_DOWNLOADS: usize = 16;
    const MAX_CONCURRENT_LOADS: usize = 8;

    let total_count = hashes.len();

    // Collect unique non-zero hashes for later parallel loading
    let unique_hashes: HashSet<henyey_common::Hash256> = hashes
        .iter()
        .filter(|h| !h.is_zero())
        .map(|h| **h)
        .collect();

    // Filter out already-present buckets using fast existence check (no loading)
    let to_download: Vec<_> = hashes
        .into_iter()
        .filter(|hash| !bucket_manager.bucket_exists(hash))
        .collect();

    let cached_count = total_count - to_download.len();

    if !to_download.is_empty() {
        let download_count = to_download.len();
        let downloaded = AtomicU32::new(0);

        println!(
            "  {} cached, {} to download...",
            cached_count, download_count
        );

        let results: Vec<anyhow::Result<()>> = stream::iter(to_download.into_iter())
            .map(|hash| {
                let downloaded = &downloaded;
                let bm = &bucket_manager;
                async move {
                    let bucket_data = archive.get_bucket(hash).await.map_err(|e| {
                        anyhow::anyhow!("Failed to download bucket {}: {}", hash.to_hex(), e)
                    })?;
                    bm.import_bucket(&bucket_data).map_err(|e| {
                        anyhow::anyhow!("Failed to import bucket {}: {}", hash.to_hex(), e)
                    })?;

                    let count = downloaded.fetch_add(1, Ordering::Relaxed) + 1;
                    if count % 5 == 0 || count == download_count as u32 {
                        println!("  Downloaded {}/{} buckets", count, download_count);
                    }
                    Ok(())
                }
            })
            .buffer_unordered(MAX_CONCURRENT_DOWNLOADS)
            .collect()
            .await;

        // Check for any failures
        for result in results {
            result?;
        }
    }

    // Parallel loading: load all unique buckets into cache using spawn_blocking
    // This builds the in-memory index for each bucket (SHA256 hash + offset index).
    // Thread safety: load_bucket takes read lock to check cache, then brief write lock to insert.
    let load_start = std::time::Instant::now();
    let unique_hashes_vec: Vec<henyey_common::Hash256> =
        unique_hashes.into_iter().collect();
    let load_count = unique_hashes_vec.len();

    let load_results: Vec<anyhow::Result<()>> = stream::iter(unique_hashes_vec.into_iter())
        .map(|hash| {
            let bm = bucket_manager.clone();
            async move {
                tokio::task::spawn_blocking(move || {
                    bm.load_bucket(&hash).map_err(|e| {
                        anyhow::anyhow!("Failed to load bucket {}: {}", hash.to_hex(), e)
                    })?;
                    Ok(())
                })
                .await?
            }
        })
        .buffer_unordered(MAX_CONCURRENT_LOADS)
        .collect()
        .await;

    for result in load_results {
        result?;
    }

    let download_count = total_count - cached_count;
    println!(
        "  Loaded {} buckets into cache in {:.2}s",
        load_count,
        load_start.elapsed().as_secs_f64()
    );

    Ok((cached_count, download_count))
}

/// Verifies transaction execution by comparing results against CDP metadata.
///
/// This test re-executes transactions using `close_ledger` and compares the
/// resulting ledger close metadata against what C++ stellar-core produced (from CDP).
/// Differences indicate execution divergence that needs investigation.
///
/// # Verification Process
///
/// 1. Restores bucket list state from a checkpoint before the test range
/// 2. For each ledger, calls `close_ledger` with the transaction set from CDP
/// 3. Compares the resulting `LedgerCloseResult` against CDP:
///    - Header hash
///    - Transaction result hash
///    - Ledger close metadata (if both present)
/// 4. Reports any mismatches in detail
#[allow(clippy::too_many_arguments)]
async fn cmd_verify_execution(
    config: AppConfig,
    from: Option<u32>,
    to: Option<u32>,
    stop_on_error: bool,
    show_diff: bool,
    cdp_url: Option<String>,
    cdp_date: Option<String>,
    cache_dir: Option<std::path::PathBuf>,
    no_cache: bool,
    quiet: bool,
) -> anyhow::Result<()> {
    use std::sync::Arc;
    use henyey_bucket::{BucketList, BucketManager, HasNextState, HotArchiveBucketList};
    use henyey_common::Hash256;
    use henyey_history::cdp::{
        extract_ledger_close_data, extract_ledger_header, extract_transaction_results,
        CachedCdpDataLake,
    };
    use henyey_history::{checkpoint, HistoryArchive};
    use henyey_ledger::{LedgerManager, LedgerManagerConfig};

    let init_start = std::time::Instant::now();

    if !quiet {
        println!("Transaction Execution Verification");
        println!("===================================");
        println!("Executes transactions via close_ledger and compares against CDP.");
        println!();
    }

    // Determine network ID and network name
    let (_network_id, network_name, is_mainnet) = if config.network.passphrase.contains("Test") {
        (henyey_common::NetworkId::testnet(), "testnet", false)
    } else {
        (henyey_common::NetworkId::mainnet(), "mainnet", true)
    };

    // Set network-specific CDP defaults
    let cdp_url = cdp_url.unwrap_or_else(|| {
        if is_mainnet {
            "https://aws-public-blockchain.s3.us-east-2.amazonaws.com/v1.1/stellar/ledgers/pubnet".to_string()
        } else {
            "https://aws-public-blockchain.s3.us-east-2.amazonaws.com/v1.1/stellar/ledgers/testnet".to_string()
        }
    });
    let cdp_date = cdp_date.unwrap_or_else(|| {
        if is_mainnet {
            String::new()
        } else {
            "2025-12-18".to_string()
        }
    });

    // Determine cache directory
    let cache_base = if no_cache {
        None
    } else {
        cache_dir.or_else(|| dirs::cache_dir().map(|p| p.join("rs-stellar-core")))
    };

    // Create archive client
    let archive = config
        .history
        .archives
        .iter()
        .filter(|a| a.get_enabled)
        .find_map(|a| HistoryArchive::new(&a.url).ok())
        .ok_or_else(|| anyhow::anyhow!("No history archives available"))?;

    if !quiet {
        println!("Archive: {}", config.history.archives[0].url);
        let cdp_date_display = if cdp_date.is_empty() { "none (range-based)" } else { &cdp_date };
        println!("CDP: {} (date: {})", cdp_url, cdp_date_display);
        if let Some(ref cache) = cache_base {
            println!("Cache: {}", cache.display());
        } else {
            println!("Cache: disabled");
        }
    }

    // Get current ledger and calculate range
    let root_has = archive.get_root_has().await?;
    let current_ledger = root_has.current_ledger;

    let end_ledger = to.unwrap_or_else(|| {
        checkpoint::latest_checkpoint_before_or_at(current_ledger).unwrap_or(current_ledger)
    });
    let start_ledger = from.unwrap_or_else(|| {
        let freq = henyey_history::CHECKPOINT_FREQUENCY;
        checkpoint::checkpoint_containing(end_ledger)
            .saturating_sub(4 * freq)
            .max(freq)
    });

    let freq = henyey_history::CHECKPOINT_FREQUENCY;
    let init_checkpoint =
        checkpoint::latest_checkpoint_before_or_at(start_ledger.saturating_sub(1))
            .unwrap_or(freq - 1);
    let end_checkpoint = checkpoint::checkpoint_containing(end_ledger);

    if !quiet {
        println!("Ledger range: {} to {}", start_ledger, end_ledger);
        println!("Initial state: checkpoint {}", init_checkpoint);
        println!();
    }

    // Create CDP client with caching
    let _cdp_dir_holder: Box<dyn std::any::Any>;
    let cdp = if let Some(ref cache) = cache_base {
        _cdp_dir_holder = Box::new(());
        CachedCdpDataLake::new(&cdp_url, &cdp_date, cache, network_name)?
    } else {
        let temp = tempfile::tempdir()?;
        let cdp = CachedCdpDataLake::new(&cdp_url, &cdp_date, temp.path(), network_name)?;
        _cdp_dir_holder = Box::new(temp);
        cdp
    };

    // Setup bucket manager
    let _bucket_dir_holder: Box<dyn std::any::Any>;
    let bucket_path = if let Some(ref cache) = cache_base {
        let path = cache.join("buckets").join(network_name);
        std::fs::create_dir_all(&path)?;
        _bucket_dir_holder = Box::new(());
        path
    } else {
        let temp = tempfile::tempdir()?;
        let path = temp.path().to_path_buf();
        _bucket_dir_holder = Box::new(temp);
        path
    };
    let bucket_manager = Arc::new(BucketManager::new(bucket_path.clone())?);

    // Download initial state
    if !quiet {
        println!("Downloading initial state at checkpoint {}...", init_checkpoint);
    }
    let init_has = archive.get_checkpoint_has(init_checkpoint).await?;

    // Extract bucket hashes
    let bucket_hashes: Vec<(Hash256, Hash256)> = init_has
        .current_buckets
        .iter()
        .map(|level| {
            (
                Hash256::from_hex(&level.curr).unwrap_or(Hash256::ZERO),
                Hash256::from_hex(&level.snap).unwrap_or(Hash256::ZERO),
            )
        })
        .collect();

    let live_next_states: Vec<HasNextState> = init_has
        .current_buckets
        .iter()
        .map(|level| HasNextState {
            state: level.next.state,
            output: level.next.output.as_ref().and_then(|h| Hash256::from_hex(h).ok()),
            input_curr: level.next.curr.as_ref().and_then(|h| Hash256::from_hex(h).ok()),
            input_snap: level.next.snap.as_ref().and_then(|h| Hash256::from_hex(h).ok()),
        })
        .collect();

    // Extract hot archive bucket hashes (protocol 23+)
    let hot_archive_hashes: Option<Vec<(Hash256, Hash256)>> =
        init_has.hot_archive_buckets.as_ref().map(|levels| {
            levels
                .iter()
                .map(|level| {
                    (
                        Hash256::from_hex(&level.curr).unwrap_or(Hash256::ZERO),
                        Hash256::from_hex(&level.snap).unwrap_or(Hash256::ZERO),
                    )
                })
                .collect()
        });

    let hot_archive_next_states: Option<Vec<HasNextState>> =
        init_has.hot_archive_buckets.as_ref().map(|levels| {
            levels
                .iter()
                .map(|level| HasNextState {
                    state: level.next.state,
                    output: level.next.output.as_ref().and_then(|h| Hash256::from_hex(h).ok()),
                    input_curr: level.next.curr.as_ref().and_then(|h| Hash256::from_hex(h).ok()),
                    input_snap: level.next.snap.as_ref().and_then(|h| Hash256::from_hex(h).ok()),
                })
                .collect()
        });

    // Collect all bucket hashes to download
    let mut all_hashes: Vec<Hash256> = Vec::new();
    for (curr, snap) in &bucket_hashes {
        all_hashes.push(*curr);
        all_hashes.push(*snap);
    }
    for state in &live_next_states {
        if let Some(ref output) = state.output {
            all_hashes.push(*output);
        }
    }
    if let Some(ref ha_hashes) = hot_archive_hashes {
        for (curr, snap) in ha_hashes {
            all_hashes.push(*curr);
            all_hashes.push(*snap);
        }
    }
    if let Some(ref ha_states) = hot_archive_next_states {
        for state in ha_states {
            if let Some(ref output) = state.output {
                all_hashes.push(*output);
            }
        }
    }
    let all_hashes: Vec<&Hash256> = all_hashes.iter().filter(|h| !h.is_zero()).collect();

    // Download buckets
    let (cached, downloaded) =
        download_buckets_parallel(&archive, bucket_manager.clone(), all_hashes).await?;
    println!("[INIT] Bucket download: {} cached, {} downloaded", cached, downloaded);

    // Restore bucket lists
    let mut bucket_list = BucketList::restore_from_has(
        &bucket_hashes,
        &live_next_states,
        |hash| bucket_manager.load_bucket(hash).map(|b| (*b).clone()),
    )?;
    bucket_list.set_bucket_dir(bucket_manager.bucket_dir().to_path_buf());

    let mut hot_archive_bucket_list = match (&hot_archive_hashes, &hot_archive_next_states) {
        (Some(ref hashes), Some(ref next_states)) => HotArchiveBucketList::restore_from_has(
            hashes,
            next_states,
            |hash| bucket_manager.load_hot_archive_bucket(hash),
        )?,
        _ => HotArchiveBucketList::new(),
    };

    // Get init header and restart merges
    let init_headers = archive.get_ledger_headers(init_checkpoint).await?;
    let init_header_entry = init_headers
        .iter()
        .find(|h| h.header.ledger_seq == init_checkpoint);
    let init_protocol_version = init_header_entry
        .map(|h| h.header.ledger_version)
        .unwrap_or(25);

    // Enable structure-based merge restarts to match C++ online mode behavior.
    //
    // Although C++ standalone offline commands skip restartMerges, we are comparing
    // our results against CDP headers produced by C++ in ONLINE mode. The C++ node
    // that produced those headers had full structure-based merge restarts enabled.
    //
    // In C++ online mode, restartMerges uses mLevels[i-1].getSnap() (the old snap
    // from HAS) to start merges. Without structure-based restarts, add_batch would
    // use snap() which returns the snapped curr (different input!).
    bucket_list.restart_merges_from_has(
        init_checkpoint,
        init_protocol_version,
        &live_next_states,
        |hash| bucket_manager.load_bucket(hash).map(|b| (*b).clone()),
        true, // restart_structure_based = true to match C++ online mode
    ).await?;

    if let Some(ref ha_next_states) = hot_archive_next_states {
        hot_archive_bucket_list.restart_merges_from_has(
            init_checkpoint,
            init_protocol_version,
            ha_next_states,
            |hash| bucket_manager.load_hot_archive_bucket(hash),
            true, // restart_structure_based = true to match C++ online mode
        )?;
    }

    // Create and initialize LedgerManager
    let ledger_manager = LedgerManager::new(
        config.network.passphrase.clone(),
        LedgerManagerConfig {
            validate_bucket_hash: true,
            ..Default::default()
        },
    );

    let init_header_entry = init_header_entry
        .ok_or_else(|| anyhow::anyhow!("No header found for checkpoint {}", init_checkpoint))?;
    let init_header_hash = Hash256::from(init_header_entry.hash.0);
    ledger_manager.initialize(
        bucket_list,
        hot_archive_bucket_list,
        init_header_entry.header.clone(),
        init_header_hash,
    )?;

    println!("[INIT] TOTAL initialization: {:.2}s", init_start.elapsed().as_secs_f64());

    // Track results
    let mut ledgers_verified = 0u32;
    let mut ledgers_matched = 0u32;
    let mut ledgers_mismatched = 0u32;
    let mut header_mismatches = 0u32;
    let mut tx_result_mismatches = 0u32;
    let mut meta_mismatches = 0u32;

    // Track previous ledger hash for close_ledger
    let mut prev_ledger_hash = init_header_hash;

    // Main verification loop
    let verification_start = std::time::Instant::now();
    let process_from = init_checkpoint + 1;
    let process_from_cp = checkpoint::checkpoint_containing(process_from);

    let mut current_cp = process_from_cp;
    while current_cp <= end_checkpoint {
        let headers = archive.get_ledger_headers(current_cp).await?;

        for header_entry in &headers {
            let header = &header_entry.header;
            let seq = header.ledger_seq;

            // Skip ledgers outside our range
            if seq <= init_checkpoint || seq > end_ledger {
                if seq > init_checkpoint {
                    prev_ledger_hash = Hash256::from(header_entry.hash.0);
                }
                continue;
            }

            let in_test_range = seq >= start_ledger && seq <= end_ledger;

            // Fetch CDP metadata
            let lcm = match cdp.get_ledger_close_meta(seq).await {
                Ok(lcm) => lcm,
                Err(e) => {
                    if in_test_range {
                        println!("  Ledger {}: CDP fetch failed: {}", seq, e);
                    }
                    prev_ledger_hash = Hash256::from(header_entry.hash.0);
                    continue;
                }
            };

            let cdp_header = extract_ledger_header(&lcm);

            // Validate CDP data matches archive
            if header.scp_value.close_time.0 != cdp_header.scp_value.close_time.0 {
                if in_test_range {
                    println!("  Ledger {}: EPOCH MISMATCH - skipping", seq);
                }
                if stop_on_error {
                    anyhow::bail!("CDP epoch mismatch at ledger {}", seq);
                }
                prev_ledger_hash = Hash256::from(header_entry.hash.0);
                continue;
            }

            // Create LedgerCloseData from CDP
            let close_data = extract_ledger_close_data(&lcm, prev_ledger_hash);

            // Execute via close_ledger
            let result = match ledger_manager.close_ledger(close_data, None) {
                Ok(r) => r,
                Err(e) => {
                    println!("  Ledger {}: close_ledger failed: {}", seq, e);
                    if stop_on_error {
                        anyhow::bail!("close_ledger failed at ledger {}: {}", seq, e);
                    }
                    prev_ledger_hash = Hash256::from(header_entry.hash.0);
                    continue;
                }
            };

            if in_test_range {
                ledgers_verified += 1;

                // Compare header hash
                let expected_header_hash = Hash256::from(header_entry.hash.0);
                let header_matches = result.header_hash == expected_header_hash;

                // Compare tx result hash
                let cdp_tx_results = extract_transaction_results(&lcm);
                let expected_tx_result_hash = Hash256::from(cdp_header.tx_set_result_hash.0);
                let our_tx_result_hash = result.tx_result_hash();
                let tx_result_matches = our_tx_result_hash == expected_tx_result_hash;

                // Compare meta (only if both present)
                let meta_matches = match (&result.meta, true) {
                    (Some(_our_meta), true) => {
                        // For now, consider meta matching if tx results match
                        // Full meta comparison would be more complex
                        tx_result_matches
                    }
                    _ => true, // Tolerate missing meta on either side
                };

                let all_match = header_matches && tx_result_matches && meta_matches;

                if all_match {
                    ledgers_matched += 1;
                    if !quiet {
                        print!(".");
                        if ledgers_verified % 64 == 0 {
                            println!(" {}", seq);
                        }
                        std::io::Write::flush(&mut std::io::stdout()).ok();
                    }
                } else {
                    ledgers_mismatched += 1;
                    if !header_matches {
                        header_mismatches += 1;
                    }
                    if !tx_result_matches {
                        tx_result_mismatches += 1;
                    }
                    if !meta_matches {
                        meta_mismatches += 1;
                    }

                    println!();
                    println!("  Ledger {}: MISMATCH", seq);
                    if !header_matches {
                        println!("    Header hash: ours={} expected={}",
                            result.header_hash.to_hex(), expected_header_hash.to_hex());
                        // Exhaustive header field comparison
                        let h = &result.header;
                        let c = &cdp_header;
                        if h.ledger_version != c.ledger_version {
                            println!("    DIFF ledger_version: ours={} expected={}", h.ledger_version, c.ledger_version);
                        }
                        if h.previous_ledger_hash != c.previous_ledger_hash {
                            println!("    DIFF previous_ledger_hash: ours={} expected={}",
                                hex::encode(&h.previous_ledger_hash.0), hex::encode(&c.previous_ledger_hash.0));
                        }
                        if h.scp_value != c.scp_value {
                            println!("    DIFF scp_value");
                            if h.scp_value.tx_set_hash != c.scp_value.tx_set_hash {
                                println!("      tx_set_hash: ours={} expected={}",
                                    hex::encode(&h.scp_value.tx_set_hash.0), hex::encode(&c.scp_value.tx_set_hash.0));
                            }
                            if h.scp_value.close_time != c.scp_value.close_time {
                                println!("      close_time: ours={} expected={}", h.scp_value.close_time.0, c.scp_value.close_time.0);
                            }
                            if h.scp_value.upgrades != c.scp_value.upgrades {
                                println!("      upgrades: ours={:?} expected={:?}", h.scp_value.upgrades, c.scp_value.upgrades);
                            }
                            if h.scp_value.ext != c.scp_value.ext {
                                println!("      ext: differs");
                            }
                        }
                        let our_bl_hash = Hash256::from(h.bucket_list_hash.0);
                        let expected_bl_hash = Hash256::from(c.bucket_list_hash.0);
                        if our_bl_hash != expected_bl_hash {
                            println!("    DIFF bucket_list_hash: ours={} expected={}",
                                our_bl_hash.to_hex(), expected_bl_hash.to_hex());
                            let level_info = ledger_manager.bucket_list_levels();
                            for (i, (curr_hash, snap_hash)) in level_info.iter().enumerate() {
                                println!("      Level {}: curr={} snap={}",
                                    i, curr_hash.to_hex(), snap_hash.to_hex());
                            }
                        }
                        if h.tx_set_result_hash != c.tx_set_result_hash {
                            println!("    DIFF tx_set_result_hash: ours={} expected={}",
                                hex::encode(&h.tx_set_result_hash.0), hex::encode(&c.tx_set_result_hash.0));
                        }
                        if h.ledger_seq != c.ledger_seq {
                            println!("    DIFF ledger_seq: ours={} expected={}", h.ledger_seq, c.ledger_seq);
                        }
                        if h.total_coins != c.total_coins {
                            println!("    DIFF total_coins: ours={} expected={}", h.total_coins, c.total_coins);
                        }
                        if h.fee_pool != c.fee_pool {
                            println!("    DIFF fee_pool: ours={} expected={}", h.fee_pool, c.fee_pool);
                        }
                        if h.inflation_seq != c.inflation_seq {
                            println!("    DIFF inflation_seq: ours={} expected={}", h.inflation_seq, c.inflation_seq);
                        }
                        if h.id_pool != c.id_pool {
                            println!("    DIFF id_pool: ours={} expected={}", h.id_pool, c.id_pool);
                        }
                        if h.base_fee != c.base_fee {
                            println!("    DIFF base_fee: ours={} expected={}", h.base_fee, c.base_fee);
                        }
                        if h.base_reserve != c.base_reserve {
                            println!("    DIFF base_reserve: ours={} expected={}", h.base_reserve, c.base_reserve);
                        }
                        if h.max_tx_set_size != c.max_tx_set_size {
                            println!("    DIFF max_tx_set_size: ours={} expected={}", h.max_tx_set_size, c.max_tx_set_size);
                        }
                        if h.skip_list != c.skip_list {
                            println!("    DIFF skip_list:");
                            for (i, (ours, exp)) in h.skip_list.iter().zip(c.skip_list.iter()).enumerate() {
                                if ours != exp {
                                    println!("      [{}]: ours={} expected={}", i, hex::encode(&ours.0), hex::encode(&exp.0));
                                }
                            }
                        }
                        if h.ext != c.ext {
                            println!("    DIFF ext: ours={:?} expected={:?}", h.ext, c.ext);
                        }
                    }
                    if !tx_result_matches {
                        println!("    TX result hash: ours={} expected={}",
                            our_tx_result_hash.to_hex(), expected_tx_result_hash.to_hex());
                    }

                    if show_diff && !tx_result_matches {
                        println!("    TX count: ours={} CDP={}",
                            result.tx_results.len(), cdp_tx_results.len());
                        // Detailed TX-by-TX comparison using full XDR
                        let mut diff_count = 0;
                        for (i, (our_tx, cdp_tx)) in result.tx_results.iter().zip(cdp_tx_results.iter()).enumerate() {
                            let our_xdr = our_tx.result.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                            let cdp_xdr = cdp_tx.result.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();

                            if our_xdr != cdp_xdr {
                                diff_count += 1;
                                use stellar_xdr::curr::TransactionResultResult;
                                let our_result = &our_tx.result.result;
                                let cdp_result = &cdp_tx.result.result;

                                let our_result_code = match our_result {
                                    TransactionResultResult::TxSuccess(_) => "txSuccess".to_string(),
                                    TransactionResultResult::TxFailed(_) => "txFailed".to_string(),
                                    TransactionResultResult::TxFeeBumpInnerSuccess(_) => "txFeeBumpInnerSuccess".to_string(),
                                    TransactionResultResult::TxFeeBumpInnerFailed(_) => "txFeeBumpInnerFailed".to_string(),
                                    _ => format!("{:?}", our_result),
                                };
                                let cdp_result_code = match cdp_result {
                                    TransactionResultResult::TxSuccess(_) => "txSuccess".to_string(),
                                    TransactionResultResult::TxFailed(_) => "txFailed".to_string(),
                                    TransactionResultResult::TxFeeBumpInnerSuccess(_) => "txFeeBumpInnerSuccess".to_string(),
                                    TransactionResultResult::TxFeeBumpInnerFailed(_) => "txFeeBumpInnerFailed".to_string(),
                                    _ => format!("{:?}", cdp_result),
                                };

                                println!("      TX {}: MISMATCH (XDR differs)", i);
                                println!("        Result: ours={} CDP={}", our_result_code, cdp_result_code);
                                println!("        Fee: ours={} CDP={}", our_tx.result.fee_charged, cdp_tx.result.fee_charged);
                                println!("        TX hash: {}", hex::encode(&our_tx.transaction_hash.0));

                                // If both failed but with different op results, show details
                                if let (TransactionResultResult::TxFailed(our_ops), TransactionResultResult::TxFailed(cdp_ops)) = (our_result, cdp_result) {
                                    for (j, (our_op, cdp_op)) in our_ops.iter().zip(cdp_ops.iter()).enumerate() {
                                        let our_op_xdr = our_op.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                        let cdp_op_xdr = cdp_op.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                        if our_op_xdr != cdp_op_xdr {
                                            println!("          Op {} differs:", j);
                                            println!("            Ours: {:?}", our_op);
                                            println!("            CDP:  {:?}", cdp_op);
                                        }
                                    }
                                }
                                // Show inner operation results for txSuccess too if they differ
                                if let (TransactionResultResult::TxSuccess(our_ops), TransactionResultResult::TxSuccess(cdp_ops)) = (our_result, cdp_result) {
                                    for (j, (our_op, cdp_op)) in our_ops.iter().zip(cdp_ops.iter()).enumerate() {
                                        let our_op_xdr = our_op.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                        let cdp_op_xdr = cdp_op.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                        if our_op_xdr != cdp_op_xdr {
                                            println!("          Op {} differs:", j);
                                            println!("            Ours: {:?}", our_op);
                                            println!("            CDP:  {:?}", cdp_op);
                                        }
                                    }
                                }

                                // Show ops when one succeeds and other fails
                                if let (TransactionResultResult::TxSuccess(our_ops), TransactionResultResult::TxFailed(cdp_ops)) = (our_result, cdp_result) {
                                    println!("        Ours ops ({}):", our_ops.len());
                                    for (j, op) in our_ops.iter().enumerate() {
                                        println!("          Op {}: {:?}", j, op);
                                    }
                                    println!("        CDP ops ({}):", cdp_ops.len());
                                    for (j, op) in cdp_ops.iter().enumerate() {
                                        println!("          Op {}: {:?}", j, op);
                                    }
                                }
                                if let (TransactionResultResult::TxFailed(our_ops), TransactionResultResult::TxSuccess(cdp_ops)) = (our_result, cdp_result) {
                                    println!("        Ours ops ({}):", our_ops.len());
                                    for (j, op) in our_ops.iter().enumerate() {
                                        println!("          Op {}: {:?}", j, op);
                                    }
                                    println!("        CDP ops ({}):", cdp_ops.len());
                                    for (j, op) in cdp_ops.iter().enumerate() {
                                        println!("          Op {}: {:?}", j, op);
                                    }
                                }

                                // Show fee bump inner result details
                                if let (TransactionResultResult::TxFeeBumpInnerFailed(our_inner), TransactionResultResult::TxFeeBumpInnerFailed(cdp_inner)) = (our_result, cdp_result) {
                                    println!("        Inner fee: ours={} CDP={}", our_inner.result.fee_charged, cdp_inner.result.fee_charged);
                                    let our_inner_code = format!("{:?}", std::mem::discriminant(&our_inner.result.result));
                                    let cdp_inner_code = format!("{:?}", std::mem::discriminant(&cdp_inner.result.result));
                                    println!("        Inner result type: ours={} CDP={}", our_inner_code, cdp_inner_code);
                                    if let (stellar_xdr::curr::InnerTransactionResultResult::TxFailed(our_ops), stellar_xdr::curr::InnerTransactionResultResult::TxFailed(cdp_ops)) = (&our_inner.result.result, &cdp_inner.result.result) {
                                        for (j, (our_op, cdp_op)) in our_ops.iter().zip(cdp_ops.iter()).enumerate() {
                                            let our_op_xdr = our_op.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                            let cdp_op_xdr = cdp_op.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                            if our_op_xdr != cdp_op_xdr {
                                                println!("          Inner Op {} differs:", j);
                                                println!("            Ours: {:?}", our_op);
                                                println!("            CDP:  {:?}", cdp_op);
                                            }
                                        }
                                        if our_ops.len() != cdp_ops.len() {
                                            println!("          Inner op count: ours={} CDP={}", our_ops.len(), cdp_ops.len());
                                        }
                                    } else {
                                        println!("        Inner result ours: {:?}", our_inner.result.result);
                                        println!("        Inner result CDP:  {:?}", cdp_inner.result.result);
                                    }
                                }
                                if let (TransactionResultResult::TxFeeBumpInnerSuccess(our_inner), TransactionResultResult::TxFeeBumpInnerSuccess(cdp_inner)) = (our_result, cdp_result) {
                                    println!("        Inner fee: ours={} CDP={}", our_inner.result.fee_charged, cdp_inner.result.fee_charged);
                                    if let (stellar_xdr::curr::InnerTransactionResultResult::TxSuccess(our_ops), stellar_xdr::curr::InnerTransactionResultResult::TxSuccess(cdp_ops)) = (&our_inner.result.result, &cdp_inner.result.result) {
                                        for (j, (our_op, cdp_op)) in our_ops.iter().zip(cdp_ops.iter()).enumerate() {
                                            let our_op_xdr = our_op.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                            let cdp_op_xdr = cdp_op.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                            if our_op_xdr != cdp_op_xdr {
                                                println!("          Inner Op {} differs:", j);
                                                println!("            Ours: {:?}", our_op);
                                                println!("            CDP:  {:?}", cdp_op);
                                            }
                                        }
                                    }
                                }

                                // Show CDP ops when ours is TxNotSupported or other non-standard result
                                if !matches!(our_result, TransactionResultResult::TxSuccess(_) | TransactionResultResult::TxFailed(_)
                                    | TransactionResultResult::TxFeeBumpInnerSuccess(_) | TransactionResultResult::TxFeeBumpInnerFailed(_)) {
                                    if let TransactionResultResult::TxFailed(cdp_ops) = cdp_result {
                                        println!("        CDP txFailed ops ({}):", cdp_ops.len());
                                        for (j, op) in cdp_ops.iter().enumerate() {
                                            println!("          Op {}: {:?}", j, op);
                                        }
                                    }
                                }

                                // Limit output to first 10 diffs
                                if diff_count >= 10 {
                                    println!("      ... (showing first 10 of potentially more diffs)");
                                    break;
                                }
                            }
                        }
                        if diff_count > 0 {
                            println!("    Total TX diffs: {} out of {}", diff_count, result.tx_results.len().min(cdp_tx_results.len()));
                        }
                    }

                    // Compare eviction data when header mismatches but TX results match
                    if !header_matches && tx_result_matches {
                        // Extract eviction data from CDP meta
                        let cdp_evicted_keys = henyey_history::cdp::extract_evicted_keys(&lcm);
                        let tx_metas = henyey_history::cdp::extract_transaction_metas(&lcm);
                        let cdp_restored_keys = henyey_history::cdp::extract_restored_keys(&tx_metas);

                        // Count CDP entry changes
                        let mut cdp_creates = 0u32;
                        let mut cdp_updates = 0u32;
                        let mut cdp_deletes = 0u32;
                        for tx_meta in &tx_metas {
                            fn count_changes(changes: &[stellar_xdr::curr::LedgerEntryChange], creates: &mut u32, updates: &mut u32, deletes: &mut u32) {
                                for change in changes {
                                    match change {
                                        stellar_xdr::curr::LedgerEntryChange::Created(_) => *creates += 1,
                                        stellar_xdr::curr::LedgerEntryChange::Updated(_) => *updates += 1,
                                        stellar_xdr::curr::LedgerEntryChange::Removed(_) => *deletes += 1,
                                        stellar_xdr::curr::LedgerEntryChange::Restored(_) => *updates += 1,
                                        stellar_xdr::curr::LedgerEntryChange::State(_) => {},
                                    }
                                }
                            }
                            match tx_meta {
                                stellar_xdr::curr::TransactionMeta::V3(v3) => {
                                    count_changes(&v3.tx_changes_before, &mut cdp_creates, &mut cdp_updates, &mut cdp_deletes);
                                    for op in v3.operations.iter() {
                                        count_changes(&op.changes, &mut cdp_creates, &mut cdp_updates, &mut cdp_deletes);
                                    }
                                    count_changes(&v3.tx_changes_after, &mut cdp_creates, &mut cdp_updates, &mut cdp_deletes);
                                }
                                stellar_xdr::curr::TransactionMeta::V4(v4) => {
                                    count_changes(&v4.tx_changes_before, &mut cdp_creates, &mut cdp_updates, &mut cdp_deletes);
                                    for op in v4.operations.iter() {
                                        count_changes(&op.changes, &mut cdp_creates, &mut cdp_updates, &mut cdp_deletes);
                                    }
                                    count_changes(&v4.tx_changes_after, &mut cdp_creates, &mut cdp_updates, &mut cdp_deletes);
                                }
                                _ => {}
                            }
                        }

                        // Extract upgrade meta entry counts
                        let cdp_upgrade_metas = henyey_history::cdp::extract_upgrade_metas(&lcm);
                        let mut upgrade_creates = 0u32;
                        let mut upgrade_updates = 0u32;
                        for um in &cdp_upgrade_metas {
                            for change in um.changes.iter() {
                                match change {
                                    stellar_xdr::curr::LedgerEntryChange::Created(_) => upgrade_creates += 1,
                                    stellar_xdr::curr::LedgerEntryChange::Updated(_) => upgrade_updates += 1,
                                    _ => {}
                                }
                            }
                        }

                        println!("    CDP meta: creates={}, updates={}, deletes={}, evicted={}, restored={}, upgrade_creates={}, upgrade_updates={}",
                            cdp_creates, cdp_updates, cdp_deletes, cdp_evicted_keys.len(), cdp_restored_keys.len(),
                            upgrade_creates, upgrade_updates);

                        // Dump expected upgrade entries from CDP meta for comparison
                        if !cdp_upgrade_metas.is_empty() {
                            use sha2::{Digest, Sha256};
                            println!("    CDP upgrade entries (expected):");
                            for (ui, um) in cdp_upgrade_metas.iter().enumerate() {
                                for change in um.changes.iter() {
                                    match change {
                                        stellar_xdr::curr::LedgerEntryChange::Updated(entry) => {
                                            let key_str = match &entry.data {
                                                stellar_xdr::curr::LedgerEntryData::ConfigSetting(cs) => {
                                                    format!("ConfigSetting({:?})", cs.discriminant())
                                                }
                                                other => format!("{:?}", std::mem::discriminant(other)),
                                            };
                                            let xdr_bytes = entry.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                            let xdr_size = xdr_bytes.len();
                                            let hash = {
                                                let mut h = Sha256::new();
                                                h.update(&xdr_bytes);
                                                let r = h.finalize();
                                                format!("{:x}", r)
                                            };
                                            println!("      upgrade[{}] Updated: key={}, last_modified={}, xdr_size={}, xdr_hash={}",
                                                ui, key_str, entry.last_modified_ledger_seq, xdr_size, hash);
                                        }
                                        stellar_xdr::curr::LedgerEntryChange::Created(entry) => {
                                            let key_str = match &entry.data {
                                                stellar_xdr::curr::LedgerEntryData::ConfigSetting(cs) => {
                                                    format!("ConfigSetting({:?})", cs.discriminant())
                                                }
                                                other => format!("{:?}", std::mem::discriminant(other)),
                                            };
                                            let xdr_bytes = entry.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                            let xdr_size = xdr_bytes.len();
                                            let hash = {
                                                let mut h = Sha256::new();
                                                h.update(&xdr_bytes);
                                                let r = h.finalize();
                                                format!("{:x}", r)
                                            };
                                            println!("      upgrade[{}] Created: key={}, last_modified={}, xdr_size={}, xdr_hash={}",
                                                ui, key_str, entry.last_modified_ledger_seq, xdr_size, hash);
                                        }
                                        stellar_xdr::curr::LedgerEntryChange::State(entry) => {
                                            let key_str = match &entry.data {
                                                stellar_xdr::curr::LedgerEntryData::ConfigSetting(cs) => {
                                                    format!("ConfigSetting({:?})", cs.discriminant())
                                                }
                                                other => format!("{:?}", std::mem::discriminant(other)),
                                            };
                                            println!("      upgrade[{}] State(before): key={}", ui, key_str);
                                        }
                                        _ => {}
                                    }
                                }
                            }
                        }

                        // Also dump expected final TX entries from CDP meta
                        {
                            use sha2::{Digest, Sha256};
                            // Coalesce: keep last Updated entry per key
                            // Include ALL change sources: fee_processing, tx_apply_processing, and post_tx_apply_fee_processing
                            let mut final_entries: std::collections::HashMap<Vec<u8>, stellar_xdr::curr::LedgerEntry> = std::collections::HashMap::new();
                            // Helper to process a slice of changes into the coalesced map
                            let coalesce_changes = |changes: &[stellar_xdr::curr::LedgerEntryChange], map: &mut std::collections::HashMap<Vec<u8>, stellar_xdr::curr::LedgerEntry>| {
                                for change in changes {
                                    match change {
                                        stellar_xdr::curr::LedgerEntryChange::Updated(entry)
                                        | stellar_xdr::curr::LedgerEntryChange::Created(entry)
                                        | stellar_xdr::curr::LedgerEntryChange::Restored(entry) => {
                                            if let Ok(key) = henyey_ledger::entry_to_key(entry) {
                                                if let Ok(kb) = key.to_xdr(stellar_xdr::curr::Limits::none()) {
                                                    map.insert(kb, entry.clone());
                                                }
                                            }
                                        }
                                        stellar_xdr::curr::LedgerEntryChange::Removed(key) => {
                                            if let Ok(kb) = key.to_xdr(stellar_xdr::curr::Limits::none()) {
                                                map.remove(&kb);
                                            }
                                        }
                                        _ => {}
                                    }
                                }
                            };
                            let coalesce_tx_meta = |meta: &stellar_xdr::curr::TransactionMeta, map: &mut std::collections::HashMap<Vec<u8>, stellar_xdr::curr::LedgerEntry>| {
                                match meta {
                                    stellar_xdr::curr::TransactionMeta::V3(v3) => {
                                        coalesce_changes(&v3.tx_changes_before, map);
                                        for op in v3.operations.iter() {
                                            coalesce_changes(&op.changes, map);
                                        }
                                        coalesce_changes(&v3.tx_changes_after, map);
                                    }
                                    stellar_xdr::curr::TransactionMeta::V4(v4) => {
                                        coalesce_changes(&v4.tx_changes_before, map);
                                        for op in v4.operations.iter() {
                                            coalesce_changes(&op.changes, map);
                                        }
                                        coalesce_changes(&v4.tx_changes_after, map);
                                    }
                                    _ => {}
                                }
                            };
                            // Process ALL change sources from LCM tx_processing
                            match &lcm {
                                stellar_xdr::curr::LedgerCloseMeta::V0(v0) => {
                                    for tp in v0.tx_processing.iter() {
                                        coalesce_changes(&tp.fee_processing, &mut final_entries);
                                        coalesce_tx_meta(&tp.tx_apply_processing, &mut final_entries);
                                        // V0 TransactionResultMeta has no post_tx_apply_fee_processing
                                    }
                                }
                                stellar_xdr::curr::LedgerCloseMeta::V1(v1) => {
                                    for tp in v1.tx_processing.iter() {
                                        coalesce_changes(&tp.fee_processing, &mut final_entries);
                                        coalesce_tx_meta(&tp.tx_apply_processing, &mut final_entries);
                                        // V1 TransactionResultMeta has no post_tx_apply_fee_processing
                                    }
                                }
                                stellar_xdr::curr::LedgerCloseMeta::V2(v2) => {
                                    for tp in v2.tx_processing.iter() {
                                        coalesce_changes(&tp.fee_processing, &mut final_entries);
                                        coalesce_tx_meta(&tp.tx_apply_processing, &mut final_entries);
                                        coalesce_changes(&tp.post_tx_apply_fee_processing, &mut final_entries);
                                    }
                                }
                            }
                            println!("    CDP TX final entries (coalesced, {} unique keys)", final_entries.len());

                            // Compare CDP entries with our bucket list state
                            let bl = ledger_manager.bucket_list();
                            let bl_snapshot = henyey_bucket::BucketListSnapshot::new(&bl, result.header.clone());
                            drop(bl);
                            let mut diffs = 0;
                            let mut missing = 0;
                            for (key_bytes, cdp_entry) in final_entries.iter() {
                                use stellar_xdr::curr::ReadXdr;
                                if let Ok(key) = stellar_xdr::curr::LedgerKey::from_xdr(key_bytes.as_slice(), stellar_xdr::curr::Limits::none()) {
                                    let cdp_xdr = cdp_entry.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                    let cdp_hash = {
                                        let mut h = Sha256::new();
                                        h.update(&cdp_xdr);
                                        format!("{:x}", h.finalize())
                                    };
                                    match bl_snapshot.get(&key) {
                                        Some(our_entry) => {
                                            let our_xdr = our_entry.to_xdr(stellar_xdr::curr::Limits::none()).unwrap_or_default();
                                            let our_hash = {
                                                let mut h = Sha256::new();
                                                h.update(&our_xdr);
                                                format!("{:x}", h.finalize())
                                            };
                                            if our_hash != cdp_hash {
                                                diffs += 1;
                                                let key_str = format!("{:?}", std::mem::discriminant(&cdp_entry.data));
                                                println!("    ENTRY DIFF #{}: key={:?}", diffs, key_str);
                                                println!("      CDP:  lm={} hash={}", cdp_entry.last_modified_ledger_seq, cdp_hash);
                                                println!("      Ours: lm={} hash={}", our_entry.last_modified_ledger_seq, our_hash);
                                                println!("      CDP  xdr: {}", hex::encode(&cdp_xdr[..cdp_xdr.len().min(200)]));
                                                println!("      Ours xdr: {}", hex::encode(&our_xdr[..our_xdr.len().min(200)]));
                                                // For offers, show readable details
                                                if let (stellar_xdr::curr::LedgerEntryData::Offer(cdp_o), stellar_xdr::curr::LedgerEntryData::Offer(our_o)) = (&cdp_entry.data, &our_entry.data) {
                                                    println!("      CDP  offer: seller={:?} amount={} price={}/{}", hex::encode(&{let stellar_xdr::curr::PublicKey::PublicKeyTypeEd25519(ref pk) = cdp_o.seller_id.0; pk.0}[..8]), cdp_o.amount, cdp_o.price.n, cdp_o.price.d);
                                                    println!("      Ours offer: seller={:?} amount={} price={}/{}", hex::encode(&{let stellar_xdr::curr::PublicKey::PublicKeyTypeEd25519(ref pk) = our_o.seller_id.0; pk.0}[..8]), our_o.amount, our_o.price.n, our_o.price.d);
                                                }
                                                if let (stellar_xdr::curr::LedgerEntryData::Account(cdp_a), stellar_xdr::curr::LedgerEntryData::Account(our_a)) = (&cdp_entry.data, &our_entry.data) {
                                                    println!("      CDP  account: balance={} seq={}", cdp_a.balance, cdp_a.seq_num.0);
                                                    println!("      Ours account: balance={} seq={}", our_a.balance, our_a.seq_num.0);
                                                }
                                                if let (stellar_xdr::curr::LedgerEntryData::Trustline(cdp_t), stellar_xdr::curr::LedgerEntryData::Trustline(our_t)) = (&cdp_entry.data, &our_entry.data) {
                                                    println!("      CDP  trustline: balance={} asset={:?}", cdp_t.balance, cdp_t.asset);
                                                    println!("      Ours trustline: balance={} asset={:?}", our_t.balance, our_t.asset);
                                                }
                                                if let (stellar_xdr::curr::LedgerEntryData::LiquidityPool(cdp_p), stellar_xdr::curr::LedgerEntryData::LiquidityPool(our_p)) = (&cdp_entry.data, &our_entry.data) {
                                                    let stellar_xdr::curr::LiquidityPoolEntryBody::LiquidityPoolConstantProduct(ref cdp_cp) = cdp_p.body;
                                                    let stellar_xdr::curr::LiquidityPoolEntryBody::LiquidityPoolConstantProduct(ref our_cp) = our_p.body;
                                                    println!("      CDP  pool: reserve_a={} reserve_b={}", cdp_cp.reserve_a, cdp_cp.reserve_b);
                                                    println!("      Ours pool: reserve_a={} reserve_b={}", our_cp.reserve_a, our_cp.reserve_b);
                                                }
                                                if diffs >= 20 { break; }
                                            }
                                        }
                                        None => {
                                            missing += 1;
                                            if missing <= 10 {
                                                let key_str = match &cdp_entry.data {
                                                    stellar_xdr::curr::LedgerEntryData::Account(a) => {
                                                        let stellar_xdr::curr::PublicKey::PublicKeyTypeEd25519(ref pk) = a.account_id.0;
                                                        format!("Account({})", hex::encode(&pk.0[..8]))
                                                    }
                                                    stellar_xdr::curr::LedgerEntryData::Trustline(t) => {
                                                        let stellar_xdr::curr::PublicKey::PublicKeyTypeEd25519(ref pk) = t.account_id.0;
                                                        format!("Trustline(acct={}, asset={:?}, balance={})", hex::encode(&pk.0[..8]), t.asset, t.balance)
                                                    }
                                                    stellar_xdr::curr::LedgerEntryData::Offer(o) => {
                                                        format!("Offer(id={}, amount={})", o.offer_id, o.amount)
                                                    }
                                                    stellar_xdr::curr::LedgerEntryData::LiquidityPool(p) => {
                                                        let stellar_xdr::curr::LiquidityPoolEntryBody::LiquidityPoolConstantProduct(ref cp) = p.body;
                                                        format!("Pool(ra={}, rb={})", cp.reserve_a, cp.reserve_b)
                                                    }
                                                    other => format!("{:?}", std::mem::discriminant(other)),
                                                };
                                                println!("    MISSING in our state: {} cdp_lm={} hash={}", key_str, cdp_entry.last_modified_ledger_seq, cdp_hash);
                                                println!("      cdp_xdr: {}", hex::encode(&cdp_xdr[..cdp_xdr.len().min(200)]));
                                            }
                                        }
                                    }
                                }
                            }
                            println!("    Entry comparison: {} diffs, {} missing (out of {} CDP entries)", diffs, missing, final_entries.len());
                        }
                    }

                    if stop_on_error {
                        anyhow::bail!("Mismatch at ledger {}", seq);
                    }
                }
            }

            // Update prev hash for next ledger
            prev_ledger_hash = result.header_hash;
        }

        current_cp = checkpoint::next_checkpoint(current_cp);
    }

    let verification_time = verification_start.elapsed();

    // Print summary
    println!();
    println!();
    println!("Verification Summary");
    println!("====================");
    println!("  Ledgers verified: {}", ledgers_verified);
    println!("  Ledgers matched:  {}", ledgers_matched);
    println!("  Ledgers with mismatches: {}", ledgers_mismatched);
    if ledgers_mismatched > 0 {
        println!("    - Header hash mismatches: {}", header_mismatches);
        println!("    - TX result hash mismatches: {}", tx_result_mismatches);
        println!("    - Meta mismatches: {}", meta_mismatches);
    }
    println!();
    println!("  Total time: {:.2}s", verification_time.as_secs_f64());
    if ledgers_verified > 0 {
        println!("  Average per ledger: {:.2}ms",
            verification_time.as_millis() as f64 / ledgers_verified as f64);
    }

    if ledgers_mismatched > 0 {
        anyhow::bail!("Verification failed with {} mismatched ledgers", ledgers_mismatched);
    }

    Ok(())
}

/// Inspects a specific account entry in the bucket list at a checkpoint.
///
/// This debugging tool shows all occurrences of an account across all bucket
/// levels, helping diagnose issues like shadowed entries or incorrect lookups.
/// It displays both the normal lookup result and a full scan across all buckets.
///
/// # Arguments
///
/// * `config` - Application configuration with history archive URLs
/// * `checkpoint_seq` - Checkpoint ledger sequence to inspect
/// * `account_hex` - Account public key as 64-character hex string
async fn cmd_debug_bucket_entry(
    config: AppConfig,
    checkpoint_seq: u32,
    account_hex: &str,
) -> anyhow::Result<()> {
    use std::sync::Arc;
    use henyey_bucket::{BucketEntry, BucketList, BucketManager};
    use henyey_common::Hash256;
    use henyey_history::{is_checkpoint_ledger, HistoryArchive};
    use stellar_xdr::curr::{
        AccountId, LedgerEntryData, LedgerKey, LedgerKeyAccount, PublicKey, Uint256,
    };

    // Parse account hex to AccountId
    let account_bytes = hex::decode(account_hex)?;
    if account_bytes.len() != 32 {
        anyhow::bail!("Account hex must be 32 bytes (64 hex chars)");
    }
    let account_id = AccountId(PublicKey::PublicKeyTypeEd25519(Uint256(
        account_bytes.try_into().unwrap(),
    )));
    let account_key = LedgerKey::Account(LedgerKeyAccount {
        account_id: account_id.clone(),
    });

    println!("Debug Bucket Entry Inspection");
    println!("==============================");
    println!("Checkpoint: {}", checkpoint_seq);
    println!("Account: {}", account_hex);
    println!();

    // Verify checkpoint is valid
    if !is_checkpoint_ledger(checkpoint_seq) {
        anyhow::bail!("{} is not a valid checkpoint ledger", checkpoint_seq);
    }

    // Create archive client from config
    let archive = config
        .history
        .archives
        .iter()
        .filter(|a| a.get_enabled)
        .find_map(|a| HistoryArchive::new(&a.url).ok())
        .ok_or_else(|| anyhow::anyhow!("No history archives available"))?;

    println!("Archive: {}", config.history.archives[0].url);

    // Get bucket list hashes at this checkpoint
    let has_entry = archive.get_checkpoint_has(checkpoint_seq).await?;
    let bucket_hashes: Vec<Hash256> = has_entry
        .current_buckets
        .iter()
        .flat_map(|level| {
            vec![
                Hash256::from_hex(&level.curr).unwrap_or(Hash256::ZERO),
                Hash256::from_hex(&level.snap).unwrap_or(Hash256::ZERO),
            ]
        })
        .collect();

    println!("Loading bucket list...");

    // Create bucket manager and load buckets
    let bucket_dir = tempfile::tempdir()?;
    let bucket_manager = Arc::new(BucketManager::new(bucket_dir.path().to_path_buf())?);

    // Download all required buckets
    let all_hashes: Vec<&Hash256> = bucket_hashes
        .iter()
        .filter(|h: &&Hash256| !h.is_zero())
        .collect();

    print!("Buckets ({} required):", all_hashes.len());
    let (cached, downloaded) =
        download_buckets_parallel(&archive, bucket_manager.clone(), all_hashes).await?;
    if downloaded == 0 {
        println!(" {} cached", cached);
    }

    // Restore bucket list
    let mut bucket_list = BucketList::restore_from_hashes(&bucket_hashes, |hash| {
        bucket_manager.load_bucket(hash).map(|b| (*b).clone())
    })?;
    bucket_list.set_bucket_dir(bucket_manager.bucket_dir().to_path_buf());

    // Restart pending merges (for correct state)
    bucket_list.restart_merges(checkpoint_seq, 25)?;

    println!("Bucket list hash: {}", bucket_list.hash().to_hex());
    println!();

    // Look up the account normally
    println!("Normal lookup result:");
    match bucket_list.get(&account_key)? {
        Some(entry) => {
            if let LedgerEntryData::Account(acc) = &entry.data {
                println!("  Balance: {}", acc.balance);
                println!("  Sequence: {}", acc.seq_num.0);
                println!("  Last modified: {}", entry.last_modified_ledger_seq);
                println!("  Thresholds: {:?}", acc.thresholds.0);
                println!("  Num signers: {}", acc.signers.len());
                for (i, signer) in acc.signers.iter().enumerate() {
                    println!("    Signer {}: weight={}", i, signer.weight);
                }
            }
        }
        None => {
            println!("  NOT FOUND");
        }
    }
    println!();

    // Find ALL occurrences across all buckets
    println!("All occurrences in bucket list:");
    let occurrences = bucket_list.find_all_occurrences(&account_key)?;
    if occurrences.is_empty() {
        println!("  No occurrences found in any bucket");
    } else {
        for (level, bucket_type, entry) in &occurrences {
            println!("  Level {} {}: ", level, bucket_type);
            match entry {
                BucketEntry::Live(e) | BucketEntry::Init(e) => {
                    if let LedgerEntryData::Account(acc) = &e.data {
                        println!("    Type: Live/Init");
                        println!("    Balance: {}", acc.balance);
                        println!("    Sequence: {}", acc.seq_num.0);
                        println!("    Last modified: {}", e.last_modified_ledger_seq);
                        println!("    Thresholds: {:?}", acc.thresholds.0);
                        println!("    Num signers: {}", acc.signers.len());
                        for (i, signer) in acc.signers.iter().enumerate() {
                            println!("      Signer {}: weight={}", i, signer.weight);
                        }
                    }
                }
                BucketEntry::Dead(_) => {
                    println!("    Type: Dead (deleted)");
                }
                BucketEntry::Metadata(_) => {
                    println!("    Type: Metadata (unexpected)");
                }
            }
        }
    }

    Ok(())
}


/// Sample config command handler.
fn cmd_sample_config() -> anyhow::Result<()> {
    let sample = AppConfig::sample_config();
    println!("{}", sample);
    Ok(())
}


/// Offline commands handler.
async fn cmd_offline(cmd: OfflineCommands, config: AppConfig) -> anyhow::Result<()> {
    match cmd {
        OfflineCommands::ConvertKey { key } => convert_key(&key),
        OfflineCommands::DecodeXdr { r#type, value } => decode_xdr(&r#type, &value),
        OfflineCommands::EncodeXdr { r#type, value } => encode_xdr(&r#type, &value),
        OfflineCommands::BucketInfo { path } => bucket_info(&path),
        OfflineCommands::VerifyExecution {
            from,
            to,
            stop_on_error,
            show_diff,
            cdp_url,
            cdp_date,
            cache_dir,
            no_cache,
            quiet,
        } => {
            cmd_verify_execution(
                config,
                from,
                to,
                stop_on_error,
                show_diff,
                cdp_url,
                cdp_date,
                cache_dir,
                no_cache,
                quiet,
            )
            .await
        }
        OfflineCommands::DebugBucketEntry {
            checkpoint,
            account,
        } => cmd_debug_bucket_entry(config, checkpoint, &account).await,
        OfflineCommands::SignTransaction {
            netid,
            input,
            base64,
        } => sign_transaction(&netid, &input, base64),
        OfflineCommands::SecToPub => sec_to_pub(),
        OfflineCommands::DumpLedger {
            output,
            entry_type,
            limit,
            last_modified_ledger_count,
        } => {
            cmd_dump_ledger(
                config,
                output,
                entry_type,
                limit,
                last_modified_ledger_count,
            )
            .await
        }
        OfflineCommands::SelfCheck => cmd_self_check(config).await,
        OfflineCommands::VerifyCheckpoints { output, from, to } => {
            cmd_verify_checkpoints(config, output, from, to).await
        }
    }
}

/// Converts Stellar IDs between formats (equivalent to C++ convert-id).
///
/// Handles the following input formats:
/// - `G...` - Public key (account ID) -> displays strKey and hex
/// - `S...` - Secret seed -> displays seed strKey and derived public key
/// - `T...` - Pre-auth transaction hash -> displays type and hex
/// - `X...` - SHA256 hash -> displays type and hex
/// - `M...` - Muxed account -> displays type, account ID, and memo ID
/// - `C...` - Contract address -> displays type and hex
/// - `P...` - Signed payload -> displays type and hex
/// - 64 hex chars - 32-byte hex -> displays all possible interpretations
fn convert_key(key: &str) -> anyhow::Result<()> {
    use henyey_crypto::{
        decode_contract, decode_muxed_account, decode_pre_auth_tx, decode_sha256_hash,
        decode_signed_payload, encode_account_id, encode_contract, encode_muxed_account,
        encode_pre_auth_tx, encode_sha256_hash,
    };

    let key = key.trim();

    // Try public key (G...)
    if key.starts_with('G') {
        let pk = henyey_crypto::PublicKey::from_strkey(key)?;
        println!("PublicKey:");
        println!("  strKey: {}", pk.to_strkey());
        println!("  hex: {}", hex::encode(pk.as_bytes()));
        return Ok(());
    }

    // Try secret seed (S...)
    if key.starts_with('S') {
        let sk = henyey_crypto::SecretKey::from_strkey(key)?;
        let pk = sk.public_key();
        println!("Seed:");
        println!("  strKey: {}", sk.to_strkey());
        println!("PublicKey:");
        println!("  strKey: {}", pk.to_strkey());
        println!("  hex: {}", hex::encode(pk.as_bytes()));
        return Ok(());
    }

    // Try pre-auth transaction hash (T...)
    if key.starts_with('T') {
        let hash = decode_pre_auth_tx(key)?;
        println!("StrKey:");
        println!("  type: STRKEY_PRE_AUTH_TX");
        println!("  hex: {}", hex::encode(hash));
        return Ok(());
    }

    // Try SHA256 hash (X...)
    if key.starts_with('X') {
        let hash = decode_sha256_hash(key)?;
        println!("StrKey:");
        println!("  type: STRKEY_HASH_X");
        println!("  hex: {}", hex::encode(hash));
        return Ok(());
    }

    // Try muxed account (M...)
    if key.starts_with('M') {
        let (account_id, memo_id) = decode_muxed_account(key)?;
        println!("StrKey:");
        println!("  type: STRKEY_MUXED_ACCOUNT_ED25519");
        println!("  accountId: {}", encode_account_id(&account_id));
        println!("  memoId: {}", memo_id);
        println!("  hex: {}", hex::encode(account_id));
        return Ok(());
    }

    // Try contract address (C...)
    if key.starts_with('C') {
        let hash = decode_contract(key)?;
        println!("StrKey:");
        println!("  type: STRKEY_CONTRACT");
        println!("  hex: {}", hex::encode(hash));
        return Ok(());
    }

    // Try signed payload (P...)
    if key.starts_with('P') {
        let (signer, payload) = decode_signed_payload(key)?;
        println!("StrKey:");
        println!("  type: STRKEY_SIGNED_PAYLOAD_ED25519");
        println!("  signer: {}", hex::encode(signer));
        println!("  payload: {}", hex::encode(payload));
        return Ok(());
    }

    // Try 64-character hex string (32 bytes)
    if key.len() == 64 {
        if let Ok(bytes) = hex::decode(key) {
            if bytes.len() == 32 {
                let data: [u8; 32] = bytes.try_into().unwrap();

                // Show all possible interpretations
                println!("Interpreted as PublicKey:");
                println!("  strKey: {}", encode_account_id(&data));
                println!("  hex: {}", key);

                // Show as seed -> public key derivation
                let sk = henyey_crypto::SecretKey::from_seed(&data);
                let pk = sk.public_key();
                println!();
                println!("Interpreted as Seed:");
                println!("  strKey: {}", sk.to_strkey());
                println!("PublicKey:");
                println!("  strKey: {}", pk.to_strkey());
                println!("  hex: {}", hex::encode(pk.as_bytes()));

                println!();
                println!("Other interpretations:");
                println!("  STRKEY_PRE_AUTH_TX: {}", encode_pre_auth_tx(&data));
                println!("  STRKEY_HASH_X: {}", encode_sha256_hash(&data));
                println!("  STRKEY_MUXED_ACCOUNT: {}", encode_muxed_account(&data, 0));
                println!("  STRKEY_CONTRACT: {}", encode_contract(&data));

                return Ok(());
            }
        }
    }

    anyhow::bail!("Unknown key format: {}. Expected G.../S.../T.../X.../M.../C.../P... strkey or 64-character hex", key);
}

/// Decodes XDR from base64 and prints it in debug format.
///
/// Supports: LedgerHeader, TransactionEnvelope, TransactionResult
fn decode_xdr(type_name: &str, value: &str) -> anyhow::Result<()> {
    use base64::{engine::general_purpose::STANDARD, Engine};
    use stellar_xdr::curr::ReadXdr;

    let bytes = STANDARD.decode(value)?;

    // This is a simplified version - a full implementation would handle all XDR types
    match type_name.to_lowercase().as_str() {
        "ledgerheader" => {
            let header = stellar_xdr::curr::LedgerHeader::from_xdr(
                &bytes,
                stellar_xdr::curr::Limits::none(),
            )?;
            println!("{:#?}", header);
        }
        "transactionenvelope" => {
            let env = stellar_xdr::curr::TransactionEnvelope::from_xdr(
                &bytes,
                stellar_xdr::curr::Limits::none(),
            )?;
            println!("{:#?}", env);
        }
        "transactionresult" => {
            let result = stellar_xdr::curr::TransactionResult::from_xdr(
                &bytes,
                stellar_xdr::curr::Limits::none(),
            )?;
            println!("{:#?}", result);
        }
        _ => {
            anyhow::bail!("Unknown XDR type: {}. Supported types: LedgerHeader, TransactionEnvelope, TransactionResult", type_name);
        }
    }

    Ok(())
}

/// Encodes a value to XDR and prints it as base64.
///
/// Supports: LedgerHeader, TransactionEnvelope, TransactionResult,
/// AccountId, MuxedAccount, Asset, Hash, Uint256
fn encode_xdr(type_name: &str, value: &str) -> anyhow::Result<()> {
    use base64::{engine::general_purpose::STANDARD, Engine};
    use stellar_xdr::curr::{Limits, WriteXdr};

    // Parse JSON and encode to XDR based on type
    match type_name.to_lowercase().as_str() {
        "ledgerheader" => {
            let header: stellar_xdr::curr::LedgerHeader = serde_json::from_str(value)
                .map_err(|e| anyhow::anyhow!("Invalid JSON for LedgerHeader: {}", e))?;
            let xdr_bytes = header.to_xdr(Limits::none())?;
            println!("{}", STANDARD.encode(&xdr_bytes));
        }
        "transactionenvelope" => {
            let env: stellar_xdr::curr::TransactionEnvelope = serde_json::from_str(value)
                .map_err(|e| anyhow::anyhow!("Invalid JSON for TransactionEnvelope: {}", e))?;
            let xdr_bytes = env.to_xdr(Limits::none())?;
            println!("{}", STANDARD.encode(&xdr_bytes));
        }
        "transactionresult" => {
            let result: stellar_xdr::curr::TransactionResult = serde_json::from_str(value)
                .map_err(|e| anyhow::anyhow!("Invalid JSON for TransactionResult: {}", e))?;
            let xdr_bytes = result.to_xdr(Limits::none())?;
            println!("{}", STANDARD.encode(&xdr_bytes));
        }
        "accountid" => {
            // Parse from strkey (G...) format
            let pk = henyey_crypto::PublicKey::from_strkey(value.trim())?;
            let account_id =
                stellar_xdr::curr::AccountId(stellar_xdr::curr::PublicKey::PublicKeyTypeEd25519(
                    stellar_xdr::curr::Uint256(*pk.as_bytes()),
                ));
            let xdr_bytes = account_id.to_xdr(Limits::none())?;
            println!("{}", STANDARD.encode(&xdr_bytes));
        }
        "muxedaccount" => {
            // Parse from strkey (G... or M...) format
            let value = value.trim();
            let muxed = if value.starts_with('G') {
                let pk = henyey_crypto::PublicKey::from_strkey(value)?;
                stellar_xdr::curr::MuxedAccount::Ed25519(stellar_xdr::curr::Uint256(*pk.as_bytes()))
            } else {
                // For M... addresses, parse the muxed account
                anyhow::bail!("Muxed account (M...) parsing not yet supported");
            };
            let xdr_bytes = muxed.to_xdr(Limits::none())?;
            println!("{}", STANDARD.encode(&xdr_bytes));
        }
        "asset" => {
            // Parse asset in format "native" or "CODE:ISSUER"
            let value = value.trim();
            let asset = if value.to_lowercase() == "native" {
                stellar_xdr::curr::Asset::Native
            } else if let Some((code, issuer)) = value.split_once(':') {
                let issuer_pk = henyey_crypto::PublicKey::from_strkey(issuer)?;
                let issuer_id = stellar_xdr::curr::AccountId(
                    stellar_xdr::curr::PublicKey::PublicKeyTypeEd25519(stellar_xdr::curr::Uint256(
                        *issuer_pk.as_bytes(),
                    )),
                );

                if code.len() <= 4 {
                    let mut asset_code = [0u8; 4];
                    asset_code[..code.len()].copy_from_slice(code.as_bytes());
                    stellar_xdr::curr::Asset::CreditAlphanum4(stellar_xdr::curr::AlphaNum4 {
                        asset_code: stellar_xdr::curr::AssetCode4(asset_code),
                        issuer: issuer_id,
                    })
                } else if code.len() <= 12 {
                    let mut asset_code = [0u8; 12];
                    asset_code[..code.len()].copy_from_slice(code.as_bytes());
                    stellar_xdr::curr::Asset::CreditAlphanum12(stellar_xdr::curr::AlphaNum12 {
                        asset_code: stellar_xdr::curr::AssetCode12(asset_code),
                        issuer: issuer_id,
                    })
                } else {
                    anyhow::bail!("Asset code too long (max 12 characters)");
                }
            } else {
                anyhow::bail!("Invalid asset format. Use 'native' or 'CODE:ISSUER'");
            };
            let xdr_bytes = asset.to_xdr(Limits::none())?;
            println!("{}", STANDARD.encode(&xdr_bytes));
        }
        "hash" | "uint256" => {
            // Parse from hex
            let value = value.trim();
            let bytes = hex::decode(value).map_err(|e| anyhow::anyhow!("Invalid hex: {}", e))?;
            if bytes.len() != 32 {
                anyhow::bail!("Hash must be exactly 32 bytes (64 hex characters)");
            }
            let hash = stellar_xdr::curr::Uint256(bytes.try_into().unwrap());
            let xdr_bytes = hash.to_xdr(Limits::none())?;
            println!("{}", STANDARD.encode(&xdr_bytes));
        }
        _ => {
            anyhow::bail!(
                "Unknown XDR type: {}. Supported types: LedgerHeader, TransactionEnvelope, \
                TransactionResult, AccountId, MuxedAccount, Asset, Hash, Uint256",
                type_name
            );
        }
    }

    Ok(())
}

/// Prints information about bucket files.
///
/// If given a directory, lists all bucket files with their sizes.
/// If given a single file, prints its metadata.
fn bucket_info(path: &PathBuf) -> anyhow::Result<()> {
    if !path.exists() {
        anyhow::bail!("Bucket path does not exist: {}", path.display());
    }

    if path.is_dir() {
        // List bucket files in directory
        println!("Bucket directory: {}", path.display());
        println!();

        let mut count = 0;
        let mut total_size = 0u64;

        for entry in std::fs::read_dir(path)? {
            let entry = entry?;
            let metadata = entry.metadata()?;
            if metadata.is_file() {
                let name = entry.file_name();
                let size = metadata.len();
                println!("  {} ({} bytes)", name.to_string_lossy(), size);
                count += 1;
                total_size += size;
            }
        }

        println!();
        println!("Total: {} files, {} bytes", count, total_size);
    } else {
        // Single bucket file
        println!("Bucket file: {}", path.display());

        let metadata = std::fs::metadata(path)?;
        println!("Size: {} bytes", metadata.len());

        // Would parse and display bucket contents
        println!("(Bucket content parsing not yet implemented)");
    }

    Ok(())
}

/// Checks quorum intersection from a JSON network configuration.
///
/// Loads the quorum set definitions and verifies that the network enjoys
/// quorum intersection (all quorums share at least one node).
fn cmd_check_quorum_intersection(path: &std::path::Path) -> anyhow::Result<()> {
    let enjoys = quorum_intersection::check_quorum_intersection_from_json(path)?;
    if enjoys {
        println!("network enjoys quorum intersection");
        Ok(())
    } else {
        anyhow::bail!("quorum sets do not have intersection");
    }
}

/// Sign a transaction envelope with a secret key.
///
/// Reads a transaction envelope (base64 or from file), prompts for a secret key,
/// signs the transaction, and outputs the signed envelope.
///
/// Equivalent to C++ stellar-core sign-transaction command.
fn sign_transaction(netid: &str, input: &str, output_base64: bool) -> anyhow::Result<()> {
    use henyey_crypto::{sha256, SecretKey};
    use stellar_xdr::curr::{
        DecoratedSignature, ReadXdr, SignatureHint, TransactionEnvelope,
        TransactionSignaturePayload, TransactionSignaturePayloadTaggedTransaction,
    };

    // Read the transaction envelope
    let envelope_bytes = if input == "-" {
        // Read from stdin
        let mut line = String::new();
        std::io::stdin().read_line(&mut line)?;
        let trimmed = line.trim();
        base64::Engine::decode(&base64::engine::general_purpose::STANDARD, trimmed)?
    } else {
        // Treat input as base64 string
        base64::Engine::decode(&base64::engine::general_purpose::STANDARD, input)?
    };

    let mut tx_env =
        TransactionEnvelope::from_xdr(envelope_bytes, stellar_xdr::curr::Limits::none())?;

    // Prompt for secret key
    eprint!("Secret key seed [network id: '{}']: ", netid);
    let mut secret_line = String::new();
    std::io::stdin().read_line(&mut secret_line)?;
    let secret_str = secret_line.trim();

    let secret_key = SecretKey::from_strkey(secret_str)
        .map_err(|e| anyhow::anyhow!("Invalid secret key: {:?}", e))?;

    // Compute the network ID hash
    let network_id_hash = sha256(netid.as_bytes());

    // Create the signature payload
    let payload = match &tx_env {
        TransactionEnvelope::TxV0(v0) => {
            // Convert V0 to V1 for signing (same signature semantics)
            let tx = stellar_xdr::curr::Transaction {
                source_account: stellar_xdr::curr::MuxedAccount::Ed25519(
                    v0.tx.source_account_ed25519.clone(),
                ),
                fee: v0.tx.fee,
                seq_num: v0.tx.seq_num.clone(),
                cond: stellar_xdr::curr::Preconditions::Time(v0.tx.time_bounds.clone().unwrap_or(
                    stellar_xdr::curr::TimeBounds {
                        min_time: 0.into(),
                        max_time: 0.into(),
                    },
                )),
                memo: v0.tx.memo.clone(),
                operations: v0.tx.operations.clone(),
                ext: stellar_xdr::curr::TransactionExt::V0,
            };
            TransactionSignaturePayload {
                network_id: stellar_xdr::curr::Hash(network_id_hash.0),
                tagged_transaction: TransactionSignaturePayloadTaggedTransaction::Tx(tx),
            }
        }
        TransactionEnvelope::Tx(v1) => TransactionSignaturePayload {
            network_id: stellar_xdr::curr::Hash(network_id_hash.0),
            tagged_transaction: TransactionSignaturePayloadTaggedTransaction::Tx(v1.tx.clone()),
        },
        TransactionEnvelope::TxFeeBump(fee_bump) => TransactionSignaturePayload {
            network_id: stellar_xdr::curr::Hash(network_id_hash.0),
            tagged_transaction: TransactionSignaturePayloadTaggedTransaction::TxFeeBump(
                fee_bump.tx.clone(),
            ),
        },
    };

    // Serialize and hash the payload
    let payload_bytes = payload.to_xdr(stellar_xdr::curr::Limits::none())?;
    let payload_hash = sha256(&payload_bytes);

    // Sign the hash
    let signature = secret_key.sign(&payload_hash.0);

    // Create the decorated signature
    let public_key = secret_key.public_key();
    let hint_bytes = public_key.as_bytes();
    let hint = SignatureHint([
        hint_bytes[28],
        hint_bytes[29],
        hint_bytes[30],
        hint_bytes[31],
    ]);
    let decorated_sig = DecoratedSignature {
        hint,
        signature: stellar_xdr::curr::Signature(signature.as_bytes().to_vec().try_into()?),
    };

    // Add the signature to the envelope
    // VecM doesn't support direct mutation, so we convert to Vec, modify, and convert back
    match &mut tx_env {
        TransactionEnvelope::TxV0(v0) => {
            let mut sigs: Vec<_> = v0.signatures.to_vec();
            if sigs.len() >= 20 {
                anyhow::bail!("Envelope already contains maximum number of signatures");
            }
            sigs.push(decorated_sig);
            v0.signatures = sigs.try_into()?;
        }
        TransactionEnvelope::Tx(v1) => {
            let mut sigs: Vec<_> = v1.signatures.to_vec();
            if sigs.len() >= 20 {
                anyhow::bail!("Envelope already contains maximum number of signatures");
            }
            sigs.push(decorated_sig);
            v1.signatures = sigs.try_into()?;
        }
        TransactionEnvelope::TxFeeBump(fee_bump) => {
            let mut sigs: Vec<_> = fee_bump.signatures.to_vec();
            if sigs.len() >= 20 {
                anyhow::bail!("Envelope already contains maximum number of signatures");
            }
            sigs.push(decorated_sig);
            fee_bump.signatures = sigs.try_into()?;
        }
    }

    // Output the signed envelope
    let out_bytes = tx_env.to_xdr(stellar_xdr::curr::Limits::none())?;
    if output_base64 {
        println!(
            "{}",
            base64::Engine::encode(&base64::engine::general_purpose::STANDARD, &out_bytes)
        );
    } else {
        use std::io::Write;
        std::io::stdout().write_all(&out_bytes)?;
    }

    Ok(())
}

/// Convert a secret key to its corresponding public key.
///
/// Reads a secret key seed (S...) from stdin and prints the public key.
/// Equivalent to C++ stellar-core sec-to-pub command.
fn sec_to_pub() -> anyhow::Result<()> {
    use henyey_crypto::{encode_account_id, SecretKey};

    eprint!("Secret key seed: ");
    let mut line = String::new();
    std::io::stdin().read_line(&mut line)?;
    let secret_str = line.trim();

    let secret_key = SecretKey::from_strkey(secret_str)
        .map_err(|e| anyhow::anyhow!("Invalid secret key: {:?}", e))?;

    let public_key = secret_key.public_key();
    println!("{}", encode_account_id(public_key.as_bytes()));

    Ok(())
}

/// Dump ledger entries from the bucket list to a JSON file.
///
/// This is equivalent to C++ stellar-core dump-ledger command.
/// It iterates over all entries in the bucket list and outputs them as JSON.
async fn cmd_dump_ledger(
    config: AppConfig,
    output: PathBuf,
    entry_type: Option<String>,
    limit: Option<u64>,
    last_modified_ledger_count: Option<u32>,
) -> anyhow::Result<()> {
    use std::io::Write;
    use henyey_bucket::BucketManager;
    use stellar_xdr::curr::LedgerEntryType;

    // Parse entry type filter if provided
    let type_filter: Option<LedgerEntryType> = if let Some(ref type_str) = entry_type {
        Some(match type_str.to_lowercase().as_str() {
            "account" => LedgerEntryType::Account,
            "trustline" => LedgerEntryType::Trustline,
            "offer" => LedgerEntryType::Offer,
            "data" => LedgerEntryType::Data,
            "claimable_balance" | "claimablebalance" => LedgerEntryType::ClaimableBalance,
            "liquidity_pool" | "liquiditypool" => LedgerEntryType::LiquidityPool,
            "contract_data" | "contractdata" => LedgerEntryType::ContractData,
            "contract_code" | "contractcode" => LedgerEntryType::ContractCode,
            "config_setting" | "configsetting" => LedgerEntryType::ConfigSetting,
            "ttl" => LedgerEntryType::Ttl,
            _ => anyhow::bail!(
                "Unknown entry type: {}. Valid types: account, trustline, offer, data, \
                claimable_balance, liquidity_pool, contract_data, contract_code, \
                config_setting, ttl",
                type_str
            ),
        })
    } else {
        None
    };

    // Open database and bucket manager
    let db = henyey_db::Database::open(&config.database.path)?;
    let bucket_manager = BucketManager::with_cache_size(
        config.buckets.directory.clone(),
        config.buckets.cache_size,
    )?;

    // Get current ledger
    let current_ledger = db
        .get_latest_ledger_seq()?
        .ok_or_else(|| anyhow::anyhow!("No ledger data in database. Run catchup first."))?;

    println!("Current ledger: {}", current_ledger);

    // Calculate minimum last modified ledger if filter is set
    let min_last_modified: Option<u32> =
        last_modified_ledger_count.map(|count| current_ledger.saturating_sub(count));

    // Load bucket list snapshot for the current checkpoint
    let checkpoint =
        henyey_history::checkpoint::latest_checkpoint_before_or_at(current_ledger)
            .ok_or_else(|| {
                anyhow::anyhow!("No checkpoint available for ledger {}", current_ledger)
            })?;

    println!("Using checkpoint: {}", checkpoint);

    let levels = db
        .load_bucket_list(checkpoint)?
        .ok_or_else(|| anyhow::anyhow!("Missing bucket list snapshot at {}", checkpoint))?;

    // Open output file
    let mut file = std::fs::File::create(&output)?;

    let mut entry_count: u64 = 0;
    let limit_val = limit.unwrap_or(u64::MAX);

    println!("Dumping entries to {}...", output.display());

    // Iterate over all bucket levels
    for (level_idx, (curr_hash, snap_hash)) in levels.iter().enumerate() {
        if entry_count >= limit_val {
            break;
        }

        // Process curr bucket
        for hash in [curr_hash, snap_hash] {
            if entry_count >= limit_val {
                break;
            }

            let bucket = bucket_manager.load_bucket(hash)?;
            for entry in bucket.entries() {
                if entry_count >= limit_val {
                    break;
                }

                // Skip dead entries and metadata
                let live_entry = match entry {
                    henyey_bucket::BucketEntry::Live(e)
                    | henyey_bucket::BucketEntry::Init(e) => e,
                    henyey_bucket::BucketEntry::Dead(_)
                    | henyey_bucket::BucketEntry::Metadata(_) => continue,
                };

                // Apply type filter
                if let Some(ref filter_type) = type_filter {
                    if live_entry.data.discriminant() != *filter_type {
                        continue;
                    }
                }

                // Apply last modified ledger filter
                if let Some(min_ledger) = min_last_modified {
                    if live_entry.last_modified_ledger_seq < min_ledger {
                        continue;
                    }
                }

                // Serialize to JSON
                let json = serde_json::to_string_pretty(&live_entry)?;
                writeln!(file, "{}", json)?;

                entry_count += 1;

                // Progress reporting
                if entry_count % 10000 == 0 {
                    print!(
                        "\rProcessed {} entries (level {})...",
                        entry_count, level_idx
                    );
                    std::io::stdout().flush()?;
                }
            }
        }
    }

    println!();
    println!("Dumped {} entries to {}", entry_count, output.display());

    Ok(())
}

/// Perform offline self-checks (equivalent to C++ self-check command).
///
/// This command performs comprehensive diagnostic checks:
/// 1. Header chain verification - ensures ledger headers form a valid chain
/// 2. Bucket hash verification - verifies all bucket files have correct hashes
/// 3. Crypto benchmarking - measures Ed25519 sign/verify performance
async fn cmd_self_check(config: AppConfig) -> anyhow::Result<()> {
    use std::time::Instant;
    use henyey_bucket::BucketManager;
    use henyey_crypto::SecretKey;

    let mut all_ok = true;

    // Phase 1: Header chain verification
    println!("Self-check phase 1: header chain verification");
    let db = henyey_db::Database::open(&config.database.path)?;

    let Some(latest_seq) = db.get_latest_ledger_seq()? else {
        println!("  No ledger data in database. Skipping header verification.");
        println!();
        return Ok(());
    };

    if latest_seq == 0 {
        println!("  At genesis ledger. Header chain is trivially valid.");
    } else {
        // Verify header chain going backwards
        let depth = std::cmp::min(latest_seq, 100); // Check up to 100 ledgers
        let mut current_seq = latest_seq;
        let mut verified = 0u32;

        while current_seq > 0 && verified < depth {
            let current = db
                .get_ledger_header(current_seq)?
                .ok_or_else(|| anyhow::anyhow!("Missing ledger header at {}", current_seq))?;
            let prev_seq = current_seq - 1;
            let prev = db
                .get_ledger_header(prev_seq)?
                .ok_or_else(|| anyhow::anyhow!("Missing ledger header at {}", prev_seq))?;

            let prev_hash = henyey_ledger::compute_header_hash(&prev)?;

            if current.previous_ledger_hash != prev_hash.into() {
                println!("  ERROR: Header chain broken at ledger {}", current_seq);
                println!(
                    "    Previous hash in header: {:?}",
                    current.previous_ledger_hash
                );
                println!("    Computed hash of previous: {:?}", prev_hash);
                all_ok = false;
                break;
            }

            current_seq = prev_seq;
            verified += 1;
        }

        if verified == depth {
            println!(
                "  Verified {} ledger headers (from {} to {})",
                verified,
                latest_seq,
                latest_seq - verified + 1
            );
        }
    }

    // Phase 2: Bucket hash verification
    println!();
    println!("Self-check phase 2: bucket hash verification");

    let bucket_manager = BucketManager::with_cache_size(
        config.buckets.directory.clone(),
        config.buckets.cache_size,
    )?;

    // Get the checkpoint for the current ledger
    let checkpoint =
        henyey_history::checkpoint::latest_checkpoint_before_or_at(latest_seq)
            .ok_or_else(|| anyhow::anyhow!("No checkpoint available for ledger {}", latest_seq))?;

    let levels = db
        .load_bucket_list(checkpoint)?
        .ok_or_else(|| anyhow::anyhow!("Missing bucket list snapshot at {}", checkpoint))?;

    let mut buckets_verified = 0;
    let mut buckets_failed = 0;

    // Collect all unique bucket hashes using a set for deduplication
    let hashes_to_verify: std::collections::HashSet<_> = levels
        .iter()
        .flat_map(|(curr, snap)| [*curr, *snap])
        .filter(|h| !h.is_zero())
        .collect();

    println!("  Verifying {} bucket files...", hashes_to_verify.len());

    for hash in &hashes_to_verify {
        match bucket_manager.load_bucket(hash) {
            Ok(bucket) => {
                // The bucket's hash is computed from its contents when loaded
                // If it loads successfully, the hash matches
                if bucket.hash() != *hash {
                    println!("  ERROR: Bucket hash mismatch for {}", hash);
                    println!("    Expected: {}", hash);
                    println!("    Computed: {}", bucket.hash());
                    buckets_failed += 1;
                    all_ok = false;
                } else {
                    buckets_verified += 1;
                }
            }
            Err(e) => {
                println!("  ERROR: Failed to load bucket {}: {}", hash, e);
                buckets_failed += 1;
                all_ok = false;
            }
        }
    }

    println!(
        "  Verified {} buckets, {} failures",
        buckets_verified, buckets_failed
    );

    // Phase 3: Crypto benchmarking
    println!();
    println!("Self-check phase 3: crypto benchmarking");

    const BENCHMARK_OPS: usize = 10000;
    let message = b"stellar benchmark test message for ed25519 signing";

    // Generate a keypair for benchmarking
    let secret = SecretKey::generate();
    let public = secret.public_key();

    // Benchmark signing
    let start = Instant::now();
    for _ in 0..BENCHMARK_OPS {
        let _ = secret.sign(message);
    }
    let sign_duration = start.elapsed();
    let sign_per_sec = (BENCHMARK_OPS as f64 / sign_duration.as_secs_f64()) as u64;

    // Generate signatures for verification benchmark
    let signature = secret.sign(message);

    // Benchmark verification
    let start = Instant::now();
    for _ in 0..BENCHMARK_OPS {
        let _ = public.verify(message, &signature);
    }
    let verify_duration = start.elapsed();
    let verify_per_sec = (BENCHMARK_OPS as f64 / verify_duration.as_secs_f64()) as u64;

    println!("  Benchmarked {} signatures / sec", sign_per_sec);
    println!("  Benchmarked {} verifications / sec", verify_per_sec);

    // Final result
    println!();
    if all_ok {
        println!("Self-check succeeded");
        Ok(())
    } else {
        println!("Self-check failed");
        std::process::exit(1);
    }
}

/// Write verified checkpoint ledger hashes to a file.
///
/// Downloads checkpoint headers from history archives, verifies the header
/// chain, and writes verified checkpoint hashes to a JSON file.
///
/// Equivalent to C++ stellar-core verify-checkpoints.
async fn cmd_verify_checkpoints(
    config: AppConfig,
    output: PathBuf,
    from: Option<u32>,
    to: Option<u32>,
) -> anyhow::Result<()> {
    use std::io::Write;
    use henyey_history::{checkpoint, verify, HistoryArchive};
    use henyey_ledger::compute_header_hash;

    println!("Verifying checkpoint hashes...");
    println!();

    // Create archive clients from config
    let archives: Vec<HistoryArchive> = config
        .history
        .archives
        .iter()
        .filter(|a| a.get_enabled)
        .filter_map(|a| match HistoryArchive::new(&a.url) {
            Ok(archive) => Some(archive),
            Err(e) => {
                println!("Warning: Failed to create archive {}: {}", a.url, e);
                None
            }
        })
        .collect();

    if archives.is_empty() {
        anyhow::bail!("No history archives available");
    }

    println!("Using {} archive(s)", archives.len());

    let archive = &archives[0];
    let root_has = archive.get_root_has().await?;
    let current_ledger = root_has.current_ledger;

    let start = from.unwrap_or(63); // First checkpoint
    let end = to.unwrap_or(current_ledger);

    println!("Verifying checkpoint range: {} to {}", start, end);
    println!();

    // Calculate checkpoint-aligned start and end
    let start_checkpoint = checkpoint::checkpoint_containing(start);
    let end_checkpoint = checkpoint::checkpoint_containing(end);

    // Collect verified checkpoint hashes
    let mut verified_checkpoints: Vec<serde_json::Value> = Vec::new();
    let mut prev_header: Option<stellar_xdr::curr::LedgerHeader> = None;
    let mut verified_count = 0;
    let mut error_count = 0;

    let mut current_checkpoint = start_checkpoint;
    while current_checkpoint <= end_checkpoint {
        print!("  Checkpoint {}: ", current_checkpoint);
        std::io::stdout().flush()?;

        // Download headers for this checkpoint
        match archive.get_ledger_headers(current_checkpoint).await {
            Ok(history_entries) => {
                if history_entries.is_empty() {
                    println!("FAIL (no headers)");
                    error_count += 1;
                    current_checkpoint = checkpoint::next_checkpoint(current_checkpoint);
                    continue;
                }

                // Extract headers
                let headers: Vec<stellar_xdr::curr::LedgerHeader> = history_entries
                    .iter()
                    .map(|entry| entry.header.clone())
                    .collect();

                // Verify header chain within this checkpoint
                if let Err(e) = verify::verify_header_chain(&headers) {
                    println!("FAIL (chain broken: {})", e);
                    error_count += 1;
                    current_checkpoint = checkpoint::next_checkpoint(current_checkpoint);
                    continue;
                }

                // Verify linkage to previous checkpoint
                if let Some(ref prev) = prev_header {
                    let first_header = &headers[0];
                    let prev_hash = compute_header_hash(prev)?;
                    if first_header.previous_ledger_hash != prev_hash.into() {
                        println!("FAIL (cross-checkpoint link broken)");
                        error_count += 1;
                        current_checkpoint = checkpoint::next_checkpoint(current_checkpoint);
                        continue;
                    }
                }

                // Get the checkpoint ledger header (last header in the set)
                let checkpoint_header = headers.last().unwrap();
                let checkpoint_hash = compute_header_hash(checkpoint_header)?;

                // Store for JSON output
                verified_checkpoints.push(serde_json::json!({
                    "ledger": current_checkpoint,
                    "hash": hex::encode(checkpoint_hash.as_bytes())
                }));

                println!(
                    "OK (hash: {})",
                    hex::encode(&checkpoint_hash.as_bytes()[..8])
                );
                verified_count += 1;

                // Update previous header for next iteration
                prev_header = Some(checkpoint_header.clone());
            }
            Err(e) => {
                println!("FAIL (download: {})", e);
                error_count += 1;
            }
        }

        current_checkpoint = checkpoint::next_checkpoint(current_checkpoint);
    }

    println!();
    println!(
        "Verified {} checkpoints, {} errors",
        verified_count, error_count
    );

    // Write output file
    let output_json = serde_json::json!({
        "network_passphrase": config.network.passphrase,
        "checkpoints": verified_checkpoints
    });

    let mut file = std::fs::File::create(&output)?;
    serde_json::to_writer_pretty(&file, &output_json)?;
    file.flush()?;

    println!("Wrote verified checkpoint hashes to: {}", output.display());

    if error_count > 0 {
        std::process::exit(1);
    }

    Ok(())
}

/// Send an HTTP command to a running stellar-core node.
///
/// Makes an HTTP GET request to http://127.0.0.1:{port}/{command}
/// and prints the response. Query parameters are URL-encoded.
///
/// Equivalent to C++ stellar-core http-command.
async fn cmd_http_command(command: &str, port: u16) -> anyhow::Result<()> {
    // Build the URL path, encoding special characters after the ?
    let mut path = String::from("/");
    let mut in_query = false;

    for c in command.chars() {
        if in_query {
            // URL-encode non-alphanumeric characters in query string
            if c.is_ascii_alphanumeric() || c == '&' || c == '=' {
                path.push(c);
            } else {
                path.push_str(&format!("%{:02X}", c as u8));
            }
        } else {
            path.push(c);
            if c == '?' {
                in_query = true;
            }
        }
    }

    let url = format!("http://127.0.0.1:{}{}", port, path);

    // Make the HTTP request
    let client = reqwest::Client::new();
    let response = client.get(&url).send().await;

    match response {
        Ok(resp) => {
            let status = resp.status();
            let body = resp.text().await?;

            if status.is_success() {
                println!("{}", body);
                Ok(())
            } else {
                eprintln!("HTTP {}: {}", status.as_u16(), body);
                anyhow::bail!("Request failed with status {}", status.as_u16());
            }
        }
        Err(e) => {
            if e.is_connect() {
                anyhow::bail!(
                    "Connection refused on port {}. Is stellar-core running?",
                    port
                );
            } else {
                anyhow::bail!("HTTP request failed: {}", e);
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    // =========================================================================
    // CLI Parsing Tests
    // =========================================================================

    #[test]
    fn test_cli_parsing() {
        // Test basic parsing
        let cli = Cli::parse_from(["rs-stellar-core", "info"]);
        assert!(matches!(cli.command, Commands::Info));
    }

    #[test]
    fn test_cli_run_command() {
        let cli = Cli::parse_from(["rs-stellar-core", "run", "--validator"]);
        match cli.command {
            Commands::Run {
                validator, watcher, ..
            } => {
                assert!(validator);
                assert!(!watcher);
            }
            _ => panic!("Expected Run command"),
        }
    }

    #[test]
    fn test_cli_catchup_command() {
        let cli = Cli::parse_from([
            "rs-stellar-core",
            "catchup",
            "1000000",
            "--mode",
            "complete",
        ]);
        match cli.command {
            Commands::Catchup { target, .. } => {
                assert_eq!(target, "1000000");
            }
            _ => panic!("Expected Catchup command"),
        }
    }

    #[test]
    fn test_cli_global_options() {
        let cli = Cli::parse_from(["rs-stellar-core", "--verbose", "--mainnet", "info"]);
        assert!(cli.verbose);
        assert!(cli.mainnet);
    }

    #[test]
    fn test_cli_check_quorum_intersection() {
        let cli = Cli::parse_from(["rs-stellar-core", "check-quorum-intersection", "foo.json"]);
        match cli.command {
            Commands::CheckQuorumIntersection { path } => {
                assert_eq!(path, PathBuf::from("foo.json"));
            }
            _ => panic!("Expected check-quorum-intersection command"),
        }
    }
}
